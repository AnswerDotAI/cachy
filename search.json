[
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "We often call APIs while prototyping and testing our code. A single API call (e.g.Â an Anthropic chat completion) can take 100â€™s of ms to run. This can really slow down development especially if our notebook contains many API calls ðŸ˜ž.\ncachy caches API requests. It does this by saving the result of each API call to a local cachy.jsonl file. Before calling an API (e.g.Â OpenAI) it will check if the request already exists in cachy.jsonl. If it does it will return the cached result.\nHow does it work?\nUnder the hood popular SDKâ€™s like OpenAI, Anthropic and LiteLLM use httpx.Client and httpx.AsyncClient.\ncachy patches the send method of both clients and injects a simple caching mechanism:\n\ncreate a cache key from the request\nif the key exists in cachy.jsonl return the cached response\nif not, call the API and save the response to cachy.jsonl\n\n\nimport tempfile\nfrom httpx import RequestNotRead\nfrom fastcore.test import *\n\ncachy.jsonl contains one API response per line.\nEach line has the following format {\"key\": key, \"response\": response}\n\nkey: hash of the API request\nresponse: the API response.\n\n{\n    \"key\": \"afc2be0c\", \n    \"response\": \"{\\\"id\\\":\\\"msg_xxx\\\",\\\"type\\\":\\\"message\\\",\\\"role\\\":\\\"assistant\\\",\\\"model\\\":\\\"claude-sonnet-4-20250514\\\",\\\"content\\\":[{\\\"type\\\":\\\"text\\\",\\\"text\\\":\\\"Coordination.\\\"}],\\\"stop_reason\\\":\\\"end_turn\\\",\\\"stop_sequence\\\":null,\\\"usage\\\":{\\\"input_tokens\\\":16,\\\"cache_creation_input_tokens\\\":0,\\\"cache_read_input_tokens\\\":0,\\\"cache_creation\\\":{\\\"ephemeral_5m_input_tokens\\\":0,\\\"ephemeral_1h_input_tokens\\\":0},\\\"output_tokens\\\":6,\\\"service_tier\\\":\\\"standard\\\"}}\"\n}",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#tests",
    "href": "core.html#tests",
    "title": "core",
    "section": "Tests",
    "text": "Tests\nLetâ€™s test enable_cachy on 3 SDKs (OpenAI, Anthropic, LiteLLM) for the scenarios below:\n\nsync requests with(out) streaming\nasync requests with(out) streaming\n\nAdd some helper functions.\n\nclass mods: ant=\"claude-sonnet-4-20250514\"; oai=\"gpt-4o\"; gem=\"gemini/gemini-2.5-flash\"\n\n\ndef mk_msgs(m): return [{\"role\": \"user\", \"content\": f\"write 1 word about {m}\"}]\n\n\nenable_cachy()\n\n\nOpenAI\n\nfrom openai import OpenAI\n\n\ncli = OpenAI()\n\n\nr = cli.responses.create(model=mods.oai, input=mk_msgs(\"openai sync\"))\nr\n\n\nr = cli.responses.create(model=mods.oai, input=mk_msgs(\"openai sync\"))\nr\n\nLetâ€™s test streaming.\n\nr = cli.responses.create(model=mods.oai, input=mk_msgs(\"openai sync streaming\"), stream=True)\nfor ch in r: print(str(ch)[:60])\n\n\nr = cli.responses.create(model=mods.oai, input=mk_msgs(\"openai sync streaming\"), stream=True)\nfor ch in r: print(str(ch)[:60])\n\nLetâ€™s test async.\n\nfrom openai import AsyncOpenAI\n\n\ncli = AsyncOpenAI()\n\n\nr = await cli.responses.create(model=mods.oai, input=mk_msgs(\"openai async\"))\nr\n\n\nr = await cli.responses.create(model=mods.oai, input=mk_msgs(\"openai async\"))\nr\n\nLetâ€™s test async streaming.\n\nr = await cli.responses.create(model=mods.oai, input=mk_msgs(\"openai async streaming\"), stream=True)\nasync for ch in r: print(str(ch)[:60])\n\n\nr = await cli.responses.create(model=mods.oai, input=mk_msgs(\"openai async streaming\"), stream=True)\nasync for ch in r: print(str(ch)[:60])\n\n\n\nAnthropic\n\nfrom anthropic import Anthropic\n\n\ncli = Anthropic()\n\n\nr = cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs(\"ant sync\"))\nr\n\n\nr = cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs(\"ant sync\"))\nr\n\nLetâ€™s test streaming.\n\nr = cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs(\"ant sync streaming\"), stream=True)\nfor ch in r: print(ch)\n\n\nr = cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs(\"ant sync streaming\"), stream=True)\nfor ch in r: print(ch)\n\nLetâ€™s test async.\n\nfrom anthropic import AsyncAnthropic\n\n\ncli = AsyncAnthropic()\n\n\nr = await cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs(\"ant async\"))\nr\n\n\nr = await cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs(\"ant async\"))\nr\n\nLetâ€™s test async streaming.\n\nr = await cli.messages.create(model=mods.ant,max_tokens=1024,messages=mk_msgs(\"ant async streaming\"), stream=True)\nasync for ch in r.response.aiter_bytes(): print(ch.decode())\n\n\nr = await cli.messages.create(model=mods.ant,max_tokens=1024,messages=mk_msgs(\"ant async streaming\"), stream=True)\nasync for ch in r.response.aiter_bytes(): print(ch.decode())\n\n\n\nLiteLLM\nLetâ€™s test the LiteLLM SDK by running sync/async calls with(out) streaming for OpenAI, Anthropic, & Gemini.\nWeâ€™ll also double check tool calls and citations.\n\nSync Tests\n\nfrom litellm import completion\n\nLetâ€™s define a helper method to display a streamed response.\n\ndef _stream(r): \n    for ch in r: print(ch.choices[0].delta.content or \"\")\n\n\nAnthropic\nLetâ€™s test claude-sonnet-x.\n\nr = completion(model=mods.ant, messages=mk_msgs(\"lite: ant sync...\"))\nr\n\n\nr = completion(model=mods.ant, messages=mk_msgs(\"lite: ant sync...\"))\nr\n\nNow, with streaming enabled.\n\nr = completion(model=mods.ant, messages=mk_msgs(\"lite: ant sync stream...\"), stream=True)\n_stream(r)\n\n\nr = completion(model=mods.ant, messages=mk_msgs(\"lite: ant sync stream...\"), stream=True)\n_stream(r)\n\n\n\nOpenAI\nLetâ€™s test gpt-4o.\n\nr = completion(model=mods.oai, messages=mk_msgs(\"lite: oai sync...\"))\nr\n\n\nr = completion(model=mods.oai, messages=mk_msgs(\"lite: oai sync...\"))\nr\n\nNow, with streaming enabled.\n\nr = completion(model=mods.oai, messages=mk_msgs(\"lite: oai sync stream...\"), stream=True)\n_stream(r)\n\n\nr = completion(model=mods.oai, messages=mk_msgs(\"lite: oai sync stream...\"), stream=True)\n_stream(r)\n\n\n\nGemini\nLetâ€™s test 2.5-flash.\n\nimport os\n\n\nr = completion(model=mods.gem, messages=mk_msgs(\"lite: gem sync...\"))\nr\n\n\nr = completion(model=mods.gem, messages=mk_msgs(\"lite: gem sync...\"))\nr\n\nNow, with streaming enabled.\n\nr = completion(model=mods.gem, messages=mk_msgs(\"lite: gem sync stream...\"), stream=True)\n_stream(r)\n\n\nr = completion(model=mods.gem, messages=mk_msgs(\"lite: gem sync stream...\"), stream=True)\n_stream(r)\n\n\n\n\nAsync Tests\n\nfrom litellm import acompletion\n\n\nasync def _astream(r):\n    async for chunk in r: print(chunk.choices[0].delta.content or \"\")\n\n\nAnthropic\nLetâ€™s test claude-sonnet-x.\n\nr = await acompletion(model=mods.ant, messages=mk_msgs(\"lite: ant async...\"))\nr\n\n\nr = await acompletion(model=mods.ant, messages=mk_msgs(\"lite: ant async...\"))\nr\n\nNow, with streaming enabled.\n\nr = await acompletion(model=mods.ant, messages=mk_msgs(\"lite: ant async stream...\"), stream=True)\nawait(_astream(r))\n\n\nr = await acompletion(model=mods.ant, messages=mk_msgs(\"lite: ant async stream...\"), stream=True)\nawait(_astream(r))\n\n\n\nOpenAI\nLetâ€™s test gpt-4o.\n\nr = await acompletion(model=mods.oai, messages=mk_msgs(\"lite: oai async...\"))\nr\n\n\nr = await acompletion(model=mods.oai, messages=mk_msgs(\"lite: oai async...\"))\nr\n\nNow, with streaming enabled.\n\nr = await acompletion(model=mods.oai, messages=mk_msgs(\"lite: oai async stream...\"), stream=True)\nawait(_astream(r))\n\n\nr = await acompletion(model=mods.oai, messages=mk_msgs(\"lite: oai async stream...\"), stream=True)\nawait(_astream(r))\n\n\n\nGemini\nLetâ€™s test 2.5-flash.\n\nr = await acompletion(model=mods.gem, messages=mk_msgs(\"lite: gem async...\"))\nr\n\n\nr = await acompletion(model=mods.gem, messages=mk_msgs(\"lite: gem async...\"))\nr\n\nNow, with streaming enabled.\n\nr = await acompletion(model=mods.gem, messages=mk_msgs(\"lite: gem async stream...\"), stream=True)\nawait(_astream(r))\n\n\nr = await acompletion(model=mods.gem, messages=mk_msgs(\"lite: gem async stream...\"), stream=True)\nawait(_astream(r))\n\n\n\n\nTool Calls\nAs a sanity check letâ€™s confirm that tool calls work.\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\"type\":\"string\", \"description\":\"The city e.g. Reims\"},\n                    \"unit\": {\"type\":\"string\", \"enum\":[\"celsius\", \"fahrenheit\"]},\n                },\n                \"required\": [\"location\"],\n            }\n        }\n    }\n]\n\n\nr = completion(model=mods.ant, messages=mk_msgs(\"Is it raining in Reims?\"), tools=tools)\nr\n\n\nr = completion(model=mods.ant, messages=mk_msgs(\"Is it raining in Reims?\"), tools=tools)\nr\n\n\n\n\nMultipart Request\n\ncli = Anthropic()\nr = cli.beta.files.upload(file=(\"ex.txt\", b\"hello world\", \"text/plain\"))\nr\n\n\ncli = Anthropic()\nr = cli.beta.files.upload(file=(\"ex.txt\", b\"hello world\", \"text/plain\"))\nr\n\n\n\nGemini Model Comparison\nWhen LiteLLM calls Gemini it includes the model name in the url. Letâ€™s test that we can run the same prompt with two different Gemini models.\n\nmods.gem\n\n\nr = completion(model=mods.gem, messages=mk_msgs(\"lite: gem different models...\"))\nr\n\n\nr = completion(model=\"gemini/gemini-2.5-flash\", messages=mk_msgs(\"lite: gem different models...\"))\nr\n\n\n\nGemini File Upload\nThe google-genai SDKâ€™s files.upload() relies on x-goog-upload-url and x-goog-upload-status response headers\n\nfrom google import genai\n\n\ncli = genai.Client()\n\nWhen no hdrs is provided the request fails:\n\ntfw = tempfile.NamedTemporaryFile(suffix='.txt')\n\n\nf = tfw.__enter__()\nfn = Path(f.name)\nfn.write_text(\"test content\");\n\n\ntry: gfile = cli.files.upload(file=fn)\nexcept Exception as e: print(e)\n\nRemove the failed cache entry:\n\ncfp = Path('../cachy.jsonl')\nlines = cfp.read_text().splitlines()\n_ = cfp.write_text('\\n'.join(lines[:-1]) + '\\n' if lines[:-1] else '')\n\nWhen caching Gemini file uploads, by default request content only includes mime_type and size_bytes. This means different files with the same mime type and size produce identical cache keys, causing incorrect cache hits. The fix is to pass a file content fingerprint (a hash of the file bytes) as the display_name in the upload config: cli.files.upload(file=fn, config={\"display_name\": _fingerprint(fn)}). This ensures the request body is unique per file content, generating distinct cache keys.\n\ndef _fingerprint(path): return hashlib.sha256(Path(path).read_bytes()).hexdigest()[:16]\n\n\nenable_cachy(hdrs=['x-goog-upload-url', 'x-goog-upload-status'])\n\n\ngfile = cli.files.upload(file=fn, config={\"display_name\": _fingerprint(fn)})\ngfile\n\n\ntfw.__exit__(None, None, None)",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "cachy",
    "section": "",
    "text": "We often call APIs while prototyping and testing our code. A single API call (e.g.Â an Anthropic chat completion) can take 100â€™s of ms to run. This can really slow down development especially if our notebook contains many API calls ðŸ˜ž.\ncachy caches API requests. It does this by saving the result of each call to a local cachy.jsonl file. Before calling an API (e.g.Â OpenAI) it will check if the request exists in cachy.jsonl. If it does it will return the cached result.\nHow does it work?\nUnder the hood popular SDKâ€™s like OpenAI, Anthropic and LiteLLM use httpx.Client and httpx.AsyncClient.\ncachy patches the send method of both clients and injects a simple caching mechanism:",
    "crumbs": [
      "cachy"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "cachy",
    "section": "Usage",
    "text": "Usage\nTo use cachy\n\ninstall the package: pip install pycachy\nadd the snippet below to the top of your notebook\n\nfrom cachy import enable_cachy\n\nenable_cachy()\nBy default cachy will cache requests made to OpenAI, Anthropic, Gemini and DeepSeek.\nNote: Gemini caching only works via the LiteLLM SDK.\n\n\n\n\n\n\nNoteCustom APIs\n\n\n\nIf youâ€™re using the OpenAI or LiteLLM SDK for other LLM providers like Grok, Mistral you can cache these requests as shown below.\nfrom cachy import enable_cachy, doms\nenable_cachy(doms=doms+('api.x.ai', 'api.mistral.com'))",
    "crumbs": [
      "cachy"
    ]
  },
  {
    "objectID": "index.html#docs",
    "href": "index.html#docs",
    "title": "cachy",
    "section": "Docs",
    "text": "Docs\nDocs can be found hosted on this GitHub repositoryâ€™s pages.",
    "crumbs": [
      "cachy"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "cachy",
    "section": "How to use",
    "text": "How to use\nFirst import and enable cachy\n\nfrom cachy import enable_cachy\n\n\nenable_cachy()\n\nNow run your api calls as normal.\n\nfrom openai import OpenAI\n\n\ncli = OpenAI()\n\n\nr = cli.responses.create(model=\"gpt-4.1\", input=\"Hey!\")\nr\n\nResponse(id='resp_0f917a6452ee099400697b191473c48191b8e4f6e76e0add02', created_at=1769675028.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4.1-2025-04-14', object='response', output=[ResponseOutputMessage(id='msg_0f917a6452ee099400697b1914af1081919353425f8cd2d7d7', content=[ResponseOutputText(annotations=[], text='Hey! How can I help you today? ðŸ˜Š', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, completed_at=1769675028.0, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=9, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=11, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=20), user=None, billing={'payer': 'developer'}, frequency_penalty=0.0, presence_penalty=0.0, store=True)\n\n\nIf you run the same request again it will read it from the cache.\n\nr = cli.responses.create(model=\"gpt-4.1\", input=\"Hey!\")\nr\n\nResponse(id='resp_0f917a6452ee099400697b191473c48191b8e4f6e76e0add02', created_at=1769675028.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4.1-2025-04-14', object='response', output=[ResponseOutputMessage(id='msg_0f917a6452ee099400697b1914af1081919353425f8cd2d7d7', content=[ResponseOutputText(annotations=[], text='Hey! How can I help you today? ðŸ˜Š', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, completed_at=1769675028.0, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=9, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=11, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=20), user=None, billing={'payer': 'developer'}, frequency_penalty=0.0, presence_penalty=0.0, store=True)",
    "crumbs": [
      "cachy"
    ]
  }
]