[
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "We often call APIs while prototyping and testing our code. A single API call (e.g.Â an Anthropic chat completion) can take 100â€™s of ms to run. This can really slow down development especially if our notebook contains many API calls ðŸ˜ž.\ncachy caches API requests. It does this by saving the result of each API call to a local cachy.jsonl file. Before calling an API (e.g.Â OpenAI) it will check if the request already exists in cachy.jsonl. If it does it will return the cached result.\nHow does it work?\nUnder the hood popular SDKâ€™s like OpenAI, Anthropic and LiteLLM use httpx.Client and httpx.AsyncClient.\ncachy patches the send method of both clients and injects a simple caching mechanism:\n\ncreate a cache key from the request\nif the key exists in cachy.jsonl return the cached response\nif not, call the API and save the response to cachy.jsonl\n\ncachy.jsonl contains one API response per line.\nEach line has the following format {\"key\": key, \"response\": response}\n\nkey: hash of the API request\nresponse: the API response.\n\n{\n    \"key\": \"afc2be0c\", \n    \"response\": \"{\\\"id\\\":\\\"msg_xxx\\\",\\\"type\\\":\\\"message\\\",\\\"role\\\":\\\"assistant\\\",\\\"model\\\":\\\"claude-sonnet-4-20250514\\\",\\\"content\\\":[{\\\"type\\\":\\\"text\\\",\\\"text\\\":\\\"Coordination.\\\"}],\\\"stop_reason\\\":\\\"end_turn\\\",\\\"stop_sequence\\\":null,\\\"usage\\\":{\\\"input_tokens\\\":16,\\\"cache_creation_input_tokens\\\":0,\\\"cache_read_input_tokens\\\":0,\\\"cache_creation\\\":{\\\"ephemeral_5m_input_tokens\\\":0,\\\"ephemeral_1h_input_tokens\\\":0},\\\"output_tokens\\\":6,\\\"service_tier\\\":\\\"standard\\\"}}\"\n}",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#tests",
    "href": "core.html#tests",
    "title": "core",
    "section": "Tests",
    "text": "Tests\nLetâ€™s test enable_cachy on 3 SDKs (OpenAI, Anthropic, LiteLLM) for the scenarios below:\n\nsync requests with(out) streaming\nasync requests with(out) streaming\n\nAdd some helper functions.\n\nclass mods: ant=\"claude-sonnet-4-20250514\"; oai=\"gpt-4o\"; gem=\"gemini/gemini-2.0-flash\"\n\n\ndef mk_msgs(m): return [{\"role\": \"user\", \"content\": f\"write 1 word about {m}\"}]\n\n\nenable_cachy()\n\n\nOpenAI\n\nfrom openai import OpenAI\n\n\ncli = OpenAI()\n\n\nr = cli.responses.create(model=mods.oai, input=mk_msgs(\"openai sync\"))\nr\n\nCollaboration\n\n\nid: resp_0adc61ccfb3938da0068c80baeb59c81a29e449ca5f3333fa2\ncreated_at: 1757940654.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-4o-2024-08-06\nobject: response\noutput: [ResponseOutputMessage(id=â€˜msg_0adc61ccfb3938da0068c80baf67fc81a2b8fe99f555767ec5â€™, content=[ResponseOutputText(annotations=[], text=â€˜Collaborationâ€™, type=â€˜output_textâ€™, logprobs=[])], role=â€˜assistantâ€™, status=â€˜completedâ€™, type=â€˜messageâ€™)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: None\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nreasoning: Reasoning(effort=None, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=â€˜textâ€™), verbosity=â€˜mediumâ€™)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=15, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=3, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=18)\nuser: None\nstore: True\n\n\n\n\n\nr = cli.responses.create(model=mods.oai, input=mk_msgs(\"openai sync\"))\nr\n\nCollaboration\n\n\nid: resp_0adc61ccfb3938da0068c80baeb59c81a29e449ca5f3333fa2\ncreated_at: 1757940654.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-4o-2024-08-06\nobject: response\noutput: [ResponseOutputMessage(id=â€˜msg_0adc61ccfb3938da0068c80baf67fc81a2b8fe99f555767ec5â€™, content=[ResponseOutputText(annotations=[], text=â€˜Collaborationâ€™, type=â€˜output_textâ€™, logprobs=[])], role=â€˜assistantâ€™, status=â€˜completedâ€™, type=â€˜messageâ€™)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: None\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nreasoning: Reasoning(effort=None, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=â€˜textâ€™), verbosity=â€˜mediumâ€™)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=15, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=3, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=18)\nuser: None\nstore: True\n\n\n\n\nLetâ€™s test streaming.\n\nr = cli.responses.create(model=mods.oai, input=mk_msgs(\"openai sync streaming\"), stream=True)\nfor ch in r: print(ch)\n\nResponseCreatedEvent(response=Response(id='resp_0bedad8c74657bc30068c80bb26cf4819ca6920df9fbd23916', created_at=1757940658.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=None, user=None, store=True), sequence_number=0, type='response.created')\nResponseInProgressEvent(response=Response(id='resp_0bedad8c74657bc30068c80bb26cf4819ca6920df9fbd23916', created_at=1757940658.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=None, user=None, store=True), sequence_number=1, type='response.in_progress')\nResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='msg_0bedad8c74657bc30068c80bb33eb4819c9b4ac3a1ef6111d7', content=[], role='assistant', status='in_progress', type='message'), output_index=0, sequence_number=2, type='response.output_item.added')\nResponseContentPartAddedEvent(content_index=0, item_id='msg_0bedad8c74657bc30068c80bb33eb4819c9b4ac3a1ef6111d7', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text', logprobs=[]), sequence_number=3, type='response.content_part.added')\nResponseTextDeltaEvent(content_index=0, delta='Innov', item_id='msg_0bedad8c74657bc30068c80bb33eb4819c9b4ac3a1ef6111d7', logprobs=[], output_index=0, sequence_number=4, type='response.output_text.delta', obfuscation='VCpZLMzWQSq')\nResponseTextDeltaEvent(content_index=0, delta='ative', item_id='msg_0bedad8c74657bc30068c80bb33eb4819c9b4ac3a1ef6111d7', logprobs=[], output_index=0, sequence_number=5, type='response.output_text.delta', obfuscation='xFQTnzLeSC8')\nResponseTextDoneEvent(content_index=0, item_id='msg_0bedad8c74657bc30068c80bb33eb4819c9b4ac3a1ef6111d7', logprobs=[], output_index=0, sequence_number=6, text='Innovative', type='response.output_text.done')\nResponseContentPartDoneEvent(content_index=0, item_id='msg_0bedad8c74657bc30068c80bb33eb4819c9b4ac3a1ef6111d7', output_index=0, part=ResponseOutputText(annotations=[], text='Innovative', type='output_text', logprobs=[]), sequence_number=7, type='response.content_part.done')\nResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='msg_0bedad8c74657bc30068c80bb33eb4819c9b4ac3a1ef6111d7', content=[ResponseOutputText(annotations=[], text='Innovative', type='output_text', logprobs=[])], role='assistant', status='completed', type='message'), output_index=0, sequence_number=8, type='response.output_item.done')\nResponseCompletedEvent(response=Response(id='resp_0bedad8c74657bc30068c80bb26cf4819ca6920df9fbd23916', created_at=1757940658.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[ResponseOutputMessage(id='msg_0bedad8c74657bc30068c80bb33eb4819c9b4ac3a1ef6111d7', content=[ResponseOutputText(annotations=[], text='Innovative', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=In: 16; Out: 3; Total: 19, user=None, store=True), sequence_number=9, type='response.completed')\n\n\n\nr = cli.responses.create(model=mods.oai, input=mk_msgs(\"openai sync streaming\"), stream=True)\nfor ch in r: print(ch)\n\nResponseCreatedEvent(response=Response(id='resp_0bedad8c74657bc30068c80bb26cf4819ca6920df9fbd23916', created_at=1757940658.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=None, user=None, store=True), sequence_number=0, type='response.created')\nResponseInProgressEvent(response=Response(id='resp_0bedad8c74657bc30068c80bb26cf4819ca6920df9fbd23916', created_at=1757940658.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=None, user=None, store=True), sequence_number=1, type='response.in_progress')\nResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='msg_0bedad8c74657bc30068c80bb33eb4819c9b4ac3a1ef6111d7', content=[], role='assistant', status='in_progress', type='message'), output_index=0, sequence_number=2, type='response.output_item.added')\nResponseContentPartAddedEvent(content_index=0, item_id='msg_0bedad8c74657bc30068c80bb33eb4819c9b4ac3a1ef6111d7', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text', logprobs=[]), sequence_number=3, type='response.content_part.added')\nResponseTextDeltaEvent(content_index=0, delta='Innov', item_id='msg_0bedad8c74657bc30068c80bb33eb4819c9b4ac3a1ef6111d7', logprobs=[], output_index=0, sequence_number=4, type='response.output_text.delta', obfuscation='VCpZLMzWQSq')\nResponseTextDeltaEvent(content_index=0, delta='ative', item_id='msg_0bedad8c74657bc30068c80bb33eb4819c9b4ac3a1ef6111d7', logprobs=[], output_index=0, sequence_number=5, type='response.output_text.delta', obfuscation='xFQTnzLeSC8')\nResponseTextDoneEvent(content_index=0, item_id='msg_0bedad8c74657bc30068c80bb33eb4819c9b4ac3a1ef6111d7', logprobs=[], output_index=0, sequence_number=6, text='Innovative', type='response.output_text.done')\nResponseContentPartDoneEvent(content_index=0, item_id='msg_0bedad8c74657bc30068c80bb33eb4819c9b4ac3a1ef6111d7', output_index=0, part=ResponseOutputText(annotations=[], text='Innovative', type='output_text', logprobs=[]), sequence_number=7, type='response.content_part.done')\nResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='msg_0bedad8c74657bc30068c80bb33eb4819c9b4ac3a1ef6111d7', content=[ResponseOutputText(annotations=[], text='Innovative', type='output_text', logprobs=[])], role='assistant', status='completed', type='message'), output_index=0, sequence_number=8, type='response.output_item.done')\nResponseCompletedEvent(response=Response(id='resp_0bedad8c74657bc30068c80bb26cf4819ca6920df9fbd23916', created_at=1757940658.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[ResponseOutputMessage(id='msg_0bedad8c74657bc30068c80bb33eb4819c9b4ac3a1ef6111d7', content=[ResponseOutputText(annotations=[], text='Innovative', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=In: 16; Out: 3; Total: 19, user=None, store=True), sequence_number=9, type='response.completed')\n\n\nLetâ€™s test async.\n\nfrom openai import AsyncOpenAI\n\n\ncli = AsyncOpenAI()\n\n\nr = await cli.responses.create(model=mods.oai, input=mk_msgs(\"openai async\"))\nr\n\nInnovative\n\n\nid: resp_07966b24b5f8fcde0068c80bc084cc819785b99bc0eaa06e47\ncreated_at: 1757940672.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-4o-2024-08-06\nobject: response\noutput: [ResponseOutputMessage(id=â€˜msg_07966b24b5f8fcde0068c80bc1038481978db7b032ab75dbefâ€™, content=[ResponseOutputText(annotations=[], text=â€˜Innovativeâ€™, type=â€˜output_textâ€™, logprobs=[])], role=â€˜assistantâ€™, status=â€˜completedâ€™, type=â€˜messageâ€™)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: None\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nreasoning: Reasoning(effort=None, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=â€˜textâ€™), verbosity=â€˜mediumâ€™)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=15, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=3, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=18)\nuser: None\nstore: True\n\n\n\n\n\nr = await cli.responses.create(model=mods.oai, input=mk_msgs(\"openai async\"))\nr\n\nInnovative\n\n\nid: resp_07966b24b5f8fcde0068c80bc084cc819785b99bc0eaa06e47\ncreated_at: 1757940672.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-4o-2024-08-06\nobject: response\noutput: [ResponseOutputMessage(id=â€˜msg_07966b24b5f8fcde0068c80bc1038481978db7b032ab75dbefâ€™, content=[ResponseOutputText(annotations=[], text=â€˜Innovativeâ€™, type=â€˜output_textâ€™, logprobs=[])], role=â€˜assistantâ€™, status=â€˜completedâ€™, type=â€˜messageâ€™)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: None\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nreasoning: Reasoning(effort=None, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=â€˜textâ€™), verbosity=â€˜mediumâ€™)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=15, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=3, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=18)\nuser: None\nstore: True\n\n\n\n\nLetâ€™s test async streaming.\n\nr = await cli.responses.create(model=mods.oai, input=mk_msgs(\"openai async streaming\"), stream=True)\nasync for ch in r: print(ch)\n\nResponseCreatedEvent(response=Response(id='resp_09667f14142ea18b0068c80bc16c8481a1988be17cf34e1823', created_at=1757940673.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=None, user=None, store=True), sequence_number=0, type='response.created')\nResponseInProgressEvent(response=Response(id='resp_09667f14142ea18b0068c80bc16c8481a1988be17cf34e1823', created_at=1757940673.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=None, user=None, store=True), sequence_number=1, type='response.in_progress')\nResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='msg_09667f14142ea18b0068c80bc1c07081a1913a81eae5c426d3', content=[], role='assistant', status='in_progress', type='message'), output_index=0, sequence_number=2, type='response.output_item.added')\nResponseContentPartAddedEvent(content_index=0, item_id='msg_09667f14142ea18b0068c80bc1c07081a1913a81eae5c426d3', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text', logprobs=[]), sequence_number=3, type='response.content_part.added')\nResponseTextDeltaEvent(content_index=0, delta='Eff', item_id='msg_09667f14142ea18b0068c80bc1c07081a1913a81eae5c426d3', logprobs=[], output_index=0, sequence_number=4, type='response.output_text.delta', obfuscation='bCNpIYrqdSk3w')\nResponseTextDeltaEvent(content_index=0, delta='icient', item_id='msg_09667f14142ea18b0068c80bc1c07081a1913a81eae5c426d3', logprobs=[], output_index=0, sequence_number=5, type='response.output_text.delta', obfuscation='wHhRAwEPuj')\nResponseTextDoneEvent(content_index=0, item_id='msg_09667f14142ea18b0068c80bc1c07081a1913a81eae5c426d3', logprobs=[], output_index=0, sequence_number=6, text='Efficient', type='response.output_text.done')\nResponseContentPartDoneEvent(content_index=0, item_id='msg_09667f14142ea18b0068c80bc1c07081a1913a81eae5c426d3', output_index=0, part=ResponseOutputText(annotations=[], text='Efficient', type='output_text', logprobs=[]), sequence_number=7, type='response.content_part.done')\nResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='msg_09667f14142ea18b0068c80bc1c07081a1913a81eae5c426d3', content=[ResponseOutputText(annotations=[], text='Efficient', type='output_text', logprobs=[])], role='assistant', status='completed', type='message'), output_index=0, sequence_number=8, type='response.output_item.done')\nResponseCompletedEvent(response=Response(id='resp_09667f14142ea18b0068c80bc16c8481a1988be17cf34e1823', created_at=1757940673.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[ResponseOutputMessage(id='msg_09667f14142ea18b0068c80bc1c07081a1913a81eae5c426d3', content=[ResponseOutputText(annotations=[], text='Efficient', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=In: 16; Out: 3; Total: 19, user=None, store=True), sequence_number=9, type='response.completed')\n\n\n\nr = await cli.responses.create(model=mods.oai, input=mk_msgs(\"openai async streaming\"), stream=True)\nasync for ch in r: print(ch)\n\nResponseCreatedEvent(response=Response(id='resp_09667f14142ea18b0068c80bc16c8481a1988be17cf34e1823', created_at=1757940673.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=None, user=None, store=True), sequence_number=0, type='response.created')\nResponseInProgressEvent(response=Response(id='resp_09667f14142ea18b0068c80bc16c8481a1988be17cf34e1823', created_at=1757940673.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=None, user=None, store=True), sequence_number=1, type='response.in_progress')\nResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='msg_09667f14142ea18b0068c80bc1c07081a1913a81eae5c426d3', content=[], role='assistant', status='in_progress', type='message'), output_index=0, sequence_number=2, type='response.output_item.added')\nResponseContentPartAddedEvent(content_index=0, item_id='msg_09667f14142ea18b0068c80bc1c07081a1913a81eae5c426d3', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text', logprobs=[]), sequence_number=3, type='response.content_part.added')\nResponseTextDeltaEvent(content_index=0, delta='Eff', item_id='msg_09667f14142ea18b0068c80bc1c07081a1913a81eae5c426d3', logprobs=[], output_index=0, sequence_number=4, type='response.output_text.delta', obfuscation='bCNpIYrqdSk3w')\nResponseTextDeltaEvent(content_index=0, delta='icient', item_id='msg_09667f14142ea18b0068c80bc1c07081a1913a81eae5c426d3', logprobs=[], output_index=0, sequence_number=5, type='response.output_text.delta', obfuscation='wHhRAwEPuj')\nResponseTextDoneEvent(content_index=0, item_id='msg_09667f14142ea18b0068c80bc1c07081a1913a81eae5c426d3', logprobs=[], output_index=0, sequence_number=6, text='Efficient', type='response.output_text.done')\nResponseContentPartDoneEvent(content_index=0, item_id='msg_09667f14142ea18b0068c80bc1c07081a1913a81eae5c426d3', output_index=0, part=ResponseOutputText(annotations=[], text='Efficient', type='output_text', logprobs=[]), sequence_number=7, type='response.content_part.done')\nResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='msg_09667f14142ea18b0068c80bc1c07081a1913a81eae5c426d3', content=[ResponseOutputText(annotations=[], text='Efficient', type='output_text', logprobs=[])], role='assistant', status='completed', type='message'), output_index=0, sequence_number=8, type='response.output_item.done')\nResponseCompletedEvent(response=Response(id='resp_09667f14142ea18b0068c80bc16c8481a1988be17cf34e1823', created_at=1757940673.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[ResponseOutputMessage(id='msg_09667f14142ea18b0068c80bc1c07081a1913a81eae5c426d3', content=[ResponseOutputText(annotations=[], text='Efficient', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=In: 16; Out: 3; Total: 19, user=None, store=True), sequence_number=9, type='response.completed')\n\n\n\n\nAnthropic\n\nfrom anthropic import Anthropic\n\n\ncli = Anthropic()\n\n\nr = cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs(\"ant sync\"))\nr\n\nCoordination\n\n\nid: msg_01AgKMiEZKSXzQYnSYWaducJ\ncontent: [{'citations': None, 'text': 'Coordination', 'type': 'text'}]\nmodel: claude-sonnet-4-20250514\nrole: assistant\nstop_reason: end_turn\nstop_sequence: None\ntype: message\nusage: {'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 15, 'output_tokens': 5, 'server_tool_use': None, 'service_tier': 'standard'}\n\n\n\n\n\nr = cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs(\"ant sync\"))\nr\n\nCoordination\n\n\nid: msg_01AgKMiEZKSXzQYnSYWaducJ\ncontent: [{'citations': None, 'text': 'Coordination', 'type': 'text'}]\nmodel: claude-sonnet-4-20250514\nrole: assistant\nstop_reason: end_turn\nstop_sequence: None\ntype: message\nusage: {'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 15, 'output_tokens': 5, 'server_tool_use': None, 'service_tier': 'standard'}\n\n\n\n\nLetâ€™s test streaming.\n\nr = cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs(\"ant sync streaming\"), stream=True)\nfor ch in r: print(ch)\n\nRawMessageStartEvent(message=Message(id='msg_01Jnrv7itVfTgQGFkbGnoi6k', content=[], model='claude-sonnet-4-20250514', role='assistant', stop_reason=None, stop_sequence=None, type='message', usage=In: 16; Out: 1; Cache create: 0; Cache read: 0; Total Tokens: 17; Search: 0), type='message_start')\nRawContentBlockStartEvent(content_block=TextBlock(citations=None, text='', type='text'), index=0, type='content_block_start')\nRawContentBlockDeltaEvent(delta=TextDelta(text='**', type='text_delta'), index=0, type='content_block_delta')\nRawContentBlockDeltaEvent(delta=TextDelta(text='Buffering**', type='text_delta'), index=0, type='content_block_delta')\nRawContentBlockStopEvent(index=0, type='content_block_stop')\nRawMessageDeltaEvent(delta=Delta(stop_reason='end_turn', stop_sequence=None), type='message_delta', usage=MessageDeltaUsage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=16, output_tokens=8, server_tool_use=None))\nRawMessageStopEvent(type='message_stop')\n\n\n\nr = cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs(\"ant sync streaming\"), stream=True)\nfor ch in r: print(ch)\n\nRawMessageStartEvent(message=Message(id='msg_01Jnrv7itVfTgQGFkbGnoi6k', content=[], model='claude-sonnet-4-20250514', role='assistant', stop_reason=None, stop_sequence=None, type='message', usage=In: 16; Out: 1; Cache create: 0; Cache read: 0; Total Tokens: 17; Search: 0), type='message_start')\nRawContentBlockStartEvent(content_block=TextBlock(citations=None, text='', type='text'), index=0, type='content_block_start')\nRawContentBlockDeltaEvent(delta=TextDelta(text='**', type='text_delta'), index=0, type='content_block_delta')\nRawContentBlockDeltaEvent(delta=TextDelta(text='Buffering**', type='text_delta'), index=0, type='content_block_delta')\nRawContentBlockStopEvent(index=0, type='content_block_stop')\nRawMessageDeltaEvent(delta=Delta(stop_reason='end_turn', stop_sequence=None), type='message_delta', usage=MessageDeltaUsage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=16, output_tokens=8, server_tool_use=None))\nRawMessageStopEvent(type='message_stop')\n\n\nLetâ€™s test async.\n\nfrom anthropic import AsyncAnthropic\n\n\ncli = AsyncAnthropic()\n\n\nr = await cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs(\"ant async\"))\nr\n\nconcurrent\n\n\nid: msg_01R33rKFKM5BqeJprT6A6DVM\ncontent: [{'citations': None, 'text': '**concurrent**', 'type': 'text'}]\nmodel: claude-sonnet-4-20250514\nrole: assistant\nstop_reason: end_turn\nstop_sequence: None\ntype: message\nusage: {'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 15, 'output_tokens': 6, 'server_tool_use': None, 'service_tier': 'standard'}\n\n\n\n\n\nr = await cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs(\"ant async\"))\nr\n\nconcurrent\n\n\nid: msg_01R33rKFKM5BqeJprT6A6DVM\ncontent: [{'citations': None, 'text': '**concurrent**', 'type': 'text'}]\nmodel: claude-sonnet-4-20250514\nrole: assistant\nstop_reason: end_turn\nstop_sequence: None\ntype: message\nusage: {'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 15, 'output_tokens': 6, 'server_tool_use': None, 'service_tier': 'standard'}\n\n\n\n\nLetâ€™s test async streaming.\n\nr = await cli.messages.create(model=mods.ant,max_tokens=1024,messages=mk_msgs(\"ant async streaming\"), stream=True)\nasync for ch in r.response.aiter_bytes(): print(ch.decode())\n\nevent: message_start\ndata: {\"type\":\"message_start\",\"message\":{\"id\":\"msg_019CQEuc6f7D2ewx7Co1PcTW\",\"type\":\"message\",\"role\":\"assistant\",\"model\":\"claude-sonnet-4-20250514\",\"content\":[],\"stop_reason\":null,\"stop_sequence\":null,\"usage\":{\"input_tokens\":16,\"cache_creation_input_tokens\":0,\"cache_read_input_tokens\":0,\"cache_creation\":{\"ephemeral_5m_input_tokens\":0,\"ephemeral_1h_input_tokens\":0},\"output_tokens\":1,\"service_tier\":\"standard\"}}       }\n\nevent: content_block_start\ndata: {\"type\":\"content_block_start\",\"index\":0,\"content_block\":{\"type\":\"text\",\"text\":\"\"}     }\n\nevent: content_block_delta\ndata: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"text_delta\",\"text\":\"Async\"}             }\n\nevent: content_block_delta\ndata: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"text_delta\",\"text\":\"Iterable\"}}\n\nevent: ping\ndata: {\"type\": \"ping\"}\n\nevent: content_block_stop\ndata: {\"type\":\"content_block_stop\",\"index\":0     }\n\nevent: ping\ndata: {\"type\": \"ping\"}\n\nevent: ping\ndata: {\"type\": \"ping\"}\n\nevent: message_delta\ndata: {\"type\":\"message_delta\",\"delta\":{\"stop_reason\":\"end_turn\",\"stop_sequence\":null},\"usage\":{\"input_tokens\":16,\"cache_creation_input_tokens\":0,\"cache_read_input_tokens\":0,\"output_tokens\":6}              }\n\nevent: message_stop\ndata: {\"type\":\"message_stop\"}\n\n\n\n\n\nr = await cli.messages.create(model=mods.ant,max_tokens=1024,messages=mk_msgs(\"ant async streaming\"), stream=True)\nasync for ch in r.response.aiter_bytes(): print(ch.decode())\n\nevent: message_start\ndata: {\"type\":\"message_start\",\"message\":{\"id\":\"msg_019CQEuc6f7D2ewx7Co1PcTW\",\"type\":\"message\",\"role\":\"assistant\",\"model\":\"claude-sonnet-4-20250514\",\"content\":[],\"stop_reason\":null,\"stop_sequence\":null,\"usage\":{\"input_tokens\":16,\"cache_creation_input_tokens\":0,\"cache_read_input_tokens\":0,\"cache_creation\":{\"ephemeral_5m_input_tokens\":0,\"ephemeral_1h_input_tokens\":0},\"output_tokens\":1,\"service_tier\":\"standard\"}}       }\n\nevent: content_block_start\ndata: {\"type\":\"content_block_start\",\"index\":0,\"content_block\":{\"type\":\"text\",\"text\":\"\"}     }\n\nevent: content_block_delta\ndata: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"text_delta\",\"text\":\"Async\"}             }\n\nevent: content_block_delta\ndata: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"text_delta\",\"text\":\"Iterable\"}}\n\nevent: ping\ndata: {\"type\": \"ping\"}\n\nevent: content_block_stop\ndata: {\"type\":\"content_block_stop\",\"index\":0     }\n\nevent: ping\ndata: {\"type\": \"ping\"}\n\nevent: ping\ndata: {\"type\": \"ping\"}\n\nevent: message_delta\ndata: {\"type\":\"message_delta\",\"delta\":{\"stop_reason\":\"end_turn\",\"stop_sequence\":null},\"usage\":{\"input_tokens\":16,\"cache_creation_input_tokens\":0,\"cache_read_input_tokens\":0,\"output_tokens\":6}              }\n\nevent: message_stop\ndata: {\"type\":\"message_stop\"}\n\n\n\n\n\n\nLiteLLM\nLetâ€™s test the LiteLLM SDK by running sync/async calls with(out) streaming for OpenAI, Anthropic, & Gemini.\nWeâ€™ll also double check tool calls and citations.\n\nSync Tests\n\nfrom litellm import completion\n\nLetâ€™s define a helper method to display a streamed response.\n\ndef _stream(r): \n    for ch in r: print(ch.choices[0].delta.content or \"\")\n\n\nAnthropic\nLetâ€™s test claude-sonnet-x.\n\nr = completion(model=mods.ant, messages=mk_msgs(\"lite: ant sync...\"))\nr\n\nModelResponse(id='chatcmpl-b2bea735-71f6-46bb-8645-09b96052d3e2', created=1757941046, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='**Streamlined**', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=8, prompt_tokens=18, total_tokens=26, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))\n\n\n\nr = completion(model=mods.ant, messages=mk_msgs(\"lite: ant sync...\"))\nr\n\nModelResponse(id='chatcmpl-c31df5f6-2278-4f92-9fab-5756ec4855e5', created=1757941046, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='**Streamlined**', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=8, prompt_tokens=18, total_tokens=26, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))\n\n\nNow, with streaming enabled.\n\nr = completion(model=mods.ant, messages=mk_msgs(\"lite: ant sync stream...\"), stream=True)\n_stream(r)\n\n**\nLightweight**\n\n\n\n\nr = completion(model=mods.ant, messages=mk_msgs(\"lite: ant sync stream...\"), stream=True)\n_stream(r)\n\n**\nLightweight**\n\n\n\n\n\nOpenAI\nLetâ€™s test gpt-4o.\n\nr = completion(model=mods.oai, messages=mk_msgs(\"lite: oai sync...\"))\nr\n\nModelResponse(id='chatcmpl-CG2xxEWT6Y33MdJvMnIRDuqlOQUvB', created=1757940685, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_f33640a400', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Efficiency', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=1, prompt_tokens=18, total_tokens=19, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n\n\n\nr = completion(model=mods.oai, messages=mk_msgs(\"lite: oai sync...\"))\nr\n\nModelResponse(id='chatcmpl-CG2xxEWT6Y33MdJvMnIRDuqlOQUvB', created=1757940685, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_f33640a400', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Efficiency', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=1, prompt_tokens=18, total_tokens=19, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n\n\nNow, with streaming enabled.\n\nr = completion(model=mods.oai, messages=mk_msgs(\"lite: oai sync stream...\"), stream=True)\n_stream(r)\n\nSynchronization\n\n\n\n\n\nr = completion(model=mods.oai, messages=mk_msgs(\"lite: oai sync stream...\"), stream=True)\n_stream(r)\n\nSynchronization\n\n\n\n\n\n\nGemini\nLetâ€™s test 2.0-flash.\n\nr = completion(model=mods.gem, messages=mk_msgs(\"lite: gem sync...\"))\nr\n\nModelResponse(id='zwvIaMfgK7CBvdIPsqDQqQY', created=1757941047, model='gemini-2.0-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Synchronization.\\n', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=3, prompt_tokens=10, total_tokens=13, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=10, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])\n\n\n\nr = completion(model=mods.gem, messages=mk_msgs(\"lite: gem sync...\"))\nr\n\nModelResponse(id='zwvIaMfgK7CBvdIPsqDQqQY', created=1757941047, model='gemini-2.0-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Synchronization.\\n', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=3, prompt_tokens=10, total_tokens=13, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=10, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])\n\n\nNow, with streaming enabled.\n\nr = completion(model=mods.gem, messages=mk_msgs(\"lite: gem sync stream...\"), stream=True)\n_stream(r)\n\nEff\nortless\n\n\n\n\n\nr = completion(model=mods.gem, messages=mk_msgs(\"lite: gem sync stream...\"), stream=True)\n_stream(r)\n\nEff\nortless\n\n\n\n\n\n\n\nAsync Tests\n\nfrom litellm import acompletion\n\n\nasync def _astream(r):\n    async for chunk in r: print(chunk.choices[0].delta.content or \"\")\n\n\nAnthropic\nLetâ€™s test claude-sonnet-x.\n\nr = await acompletion(model=mods.ant, messages=mk_msgs(\"lite: ant async...\"))\nr\n\nModelResponse(id='chatcmpl-66487276-199b-4045-b980-eeb2c83148e9', created=1757941047, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='**coroutine**', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=8, prompt_tokens=18, total_tokens=26, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))\n\n\n\nr = await acompletion(model=mods.ant, messages=mk_msgs(\"lite: ant async...\"))\nr\n\nModelResponse(id='chatcmpl-a1334d94-646c-4933-8b92-58178b067a88', created=1757941047, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='**coroutine**', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=8, prompt_tokens=18, total_tokens=26, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))\n\n\nNow, with streaming enabled.\n\nr = await acompletion(model=mods.ant, messages=mk_msgs(\"lite: ant async stream...\"), stream=True)\nawait(_astream(r))\n\n**\nreactive**\n\n\n\n\nr = await acompletion(model=mods.ant, messages=mk_msgs(\"lite: ant async stream...\"), stream=True)\nawait(_astream(r))\n\n**\nreactive**\n\n\n\n\n\nOpenAI\nLetâ€™s test gpt-4o.\n\nr = await acompletion(model=mods.oai, messages=mk_msgs(\"lite: oai async...\"))\nr\n\nModelResponse(id='chatcmpl-CG2y4f3zgvJK7cENDO4UDN0hHZ6fH', created=1757940692, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_f33640a400', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Fast.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=2, prompt_tokens=18, total_tokens=20, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n\n\n\nr = await acompletion(model=mods.oai, messages=mk_msgs(\"lite: oai async...\"))\nr\n\nModelResponse(id='chatcmpl-CG2y4f3zgvJK7cENDO4UDN0hHZ6fH', created=1757940692, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_f33640a400', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Fast.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=2, prompt_tokens=18, total_tokens=20, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n\n\nNow, with streaming enabled.\n\nr = await acompletion(model=mods.oai, messages=mk_msgs(\"lite: oai async stream...\"), stream=True)\nawait(_astream(r))\n\nEff\nicient\n.\n\n\n\n\n\nr = await acompletion(model=mods.oai, messages=mk_msgs(\"lite: oai async stream...\"), stream=True)\nawait(_astream(r))\n\nEff\nicient\n.\n\n\n\n\n\n\nGemini\nLetâ€™s test 2.0-flash.\n\nr = await acompletion(model=mods.gem, messages=mk_msgs(\"lite: gem async...\"))\nr\n\nModelResponse(id='1gvIaLDAGe2kvdIPsY6--Q0', created=1757941047, model='gemini-2.0-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Concurrency\\n', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=2, prompt_tokens=10, total_tokens=12, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=10, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])\n\n\n\nr = await acompletion(model=mods.gem, messages=mk_msgs(\"lite: gem async...\"))\nr\n\nModelResponse(id='1gvIaLDAGe2kvdIPsY6--Q0', created=1757941047, model='gemini-2.0-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Concurrency\\n', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=2, prompt_tokens=10, total_tokens=12, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=10, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])\n\n\nNow, with streaming enabled.\n\nr = await acompletion(model=mods.gem, messages=mk_msgs(\"lite: gem async stream...\"), stream=True)\nawait(_astream(r))\n\nConcurrency\n\n\n\n\nr = await acompletion(model=mods.gem, messages=mk_msgs(\"lite: gem async stream...\"), stream=True)\nawait(_astream(r))\n\nConcurrency\n\n\n\n\n\n\nTool Calls\nAs a sanity check letâ€™s confirm that tool calls work.\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\"type\":\"string\", \"description\":\"The city e.g. Reims\"},\n                    \"unit\": {\"type\":\"string\", \"enum\":[\"celsius\", \"fahrenheit\"]},\n                },\n                \"required\": [\"location\"],\n            }\n        }\n    }\n]\n\n\nr = completion(model=mods.ant, messages=mk_msgs(\"Is it raining in Reims?\"), tools=tools)\nr\n\nModelResponse(id='chatcmpl-78f754da-9ec9-490e-86f5-07049d0446e5', created=1757941047, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=0, function=Function(arguments='{\"location\": \"Reims\"}', name='get_current_weather'), id='toolu_0182nVBg1pTYTadKxS5qgCt4', type='function')], function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=57, prompt_tokens=427, total_tokens=484, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))\n\n\n\nr = completion(model=mods.ant, messages=mk_msgs(\"Is it raining in Reims?\"), tools=tools)\nr\n\nModelResponse(id='chatcmpl-73aac225-ac1b-42cc-80cf-61cefb999295', created=1757941047, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=0, function=Function(arguments='{\"location\": \"Reims\"}', name='get_current_weather'), id='toolu_0182nVBg1pTYTadKxS5qgCt4', type='function')], function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=57, prompt_tokens=427, total_tokens=484, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "cachy",
    "section": "",
    "text": "We often call APIs while prototyping and testing our code. A single API call (e.g.Â an Anthropic chat completion) can take 100â€™s of ms to run. This can really slow down development especially if our notebook contains many API calls ðŸ˜ž.\ncachy caches API requests. It does this by saving the result of each call to a local cachy.jsonl file. Before calling an API (e.g.Â OpenAI) it will check if the request exists in cachy.jsonl. If it does it will return the cached result.\nHow does it work?\nUnder the hood popular SDKâ€™s like OpenAI, Anthropic and LiteLLM use httpx.Client and httpx.AsyncClient.\ncachy patches the send method of both clients and injects a simple caching mechanism:",
    "crumbs": [
      "cachy"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "cachy",
    "section": "Usage",
    "text": "Usage\nTo use cachy\n\ninstall the package: pip install pycachy\nadd the snippet below to the top of your notebook\n\nfrom cachy import enable_cachy\n\nenable_cachy()\nBy default cachy will cache requests made to OpenAI, Anthropic, Gemini and DeepSeek.\nNote: Gemini caching only works via the LiteLLM SDK.\n\n\n\n\n\n\nNoteCustom APIs\n\n\n\nIf youâ€™re using the OpenAI or LiteLLM SDK for other LLM providers like Grok, Mistral you can cache these requests as shown below.\nfrom cachy import enable_cachy, doms\nenable_cachy(doms=doms+('api.x.ai', 'api.mistral.com'))",
    "crumbs": [
      "cachy"
    ]
  },
  {
    "objectID": "index.html#docs",
    "href": "index.html#docs",
    "title": "cachy",
    "section": "Docs",
    "text": "Docs\nDocs can be found hosted on this GitHub repositoryâ€™s pages.",
    "crumbs": [
      "cachy"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "cachy",
    "section": "How to use",
    "text": "How to use\nFirst import and enable cachy\n\nfrom cachy import enable_cachy\n\n\nenable_cachy()\n\nNow run your api calls as normal.\n\nfrom openai import OpenAI\n\n\ncli = OpenAI()\n\n\nr = cli.responses.create(model=\"gpt-4.1\", input=\"Hey!\")\nr\n\nHey! How can I help you today? ðŸ˜Š\n\n\nid: resp_68b9978ecec48196aa3e77b09ed41c6403f00c61bc19c097\ncreated_at: 1756993423.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-4.1-2025-04-14\nobject: response\noutput: [ResponseOutputMessage(id=â€˜msg_68b9978f9f70819684b17b0f21072a9003f00c61bc19c097â€™, content=[ResponseOutputText(annotations=[], text=â€˜Hey! How can I help you today? ðŸ˜Šâ€™, type=â€˜output_textâ€™, logprobs=[])], role=â€˜assistantâ€™, status=â€˜completedâ€™, type=â€˜messageâ€™)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: None\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nreasoning: Reasoning(effort=None, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=â€˜textâ€™), verbosity=â€˜mediumâ€™)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=9, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=11, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=20)\nuser: None\nstore: True\n\n\n\n\nIf you run the same request again it will read it from the cache.\n\nr = cli.responses.create(model=\"gpt-4.1\", input=\"Hey!\")\nr\n\nHey! How can I help you today? ðŸ˜Š\n\n\nid: resp_68b9978ecec48196aa3e77b09ed41c6403f00c61bc19c097\ncreated_at: 1756993423.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-4.1-2025-04-14\nobject: response\noutput: [ResponseOutputMessage(id=â€˜msg_68b9978f9f70819684b17b0f21072a9003f00c61bc19c097â€™, content=[ResponseOutputText(annotations=[], text=â€˜Hey! How can I help you today? ðŸ˜Šâ€™, type=â€˜output_textâ€™, logprobs=[])], role=â€˜assistantâ€™, status=â€˜completedâ€™, type=â€˜messageâ€™)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: None\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nreasoning: Reasoning(effort=None, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=â€˜textâ€™), verbosity=â€˜mediumâ€™)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=9, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=11, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=20)\nuser: None\nstore: True",
    "crumbs": [
      "cachy"
    ]
  }
]