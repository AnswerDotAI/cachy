{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f374771",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Cache your API calls with a single line of code. No mocks, no fixtures. Just faster, cleaner code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651805ab",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2477e8",
   "metadata": {},
   "source": [
    "We often call APIs while prototyping and testing our code. A single API call (e.g. an Anthropic chat completion) can take 100's of ms to run. This can really slow down development especially if our notebook contains many API calls ðŸ˜ž."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ba8e0f",
   "metadata": {},
   "source": [
    "`cachy` caches API requests. It does this by saving the result of each API call to a local `cachy.jsonl` file. Before calling an API (e.g. OpenAI) it will check if the request already exists in `cachy.jsonl`. If it does it will return the cached result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153506a8",
   "metadata": {},
   "source": [
    "**How does it work?**\n",
    "\n",
    "Under the hood popular SDK's like OpenAI, Anthropic and LiteLLM use `httpx.Client` and `httpx.AsyncClient`. \n",
    "\n",
    "`cachy` patches the `send` method of both clients and injects a simple caching mechanism:\n",
    "\n",
    "- create a cache key from the request\n",
    "- if the key exists in `cachy.jsonl` return the cached response\n",
    "- if not, call the API and save the response to `cachy.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2340645",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805144e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa68c98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import hashlib,httpx,json\n",
    "from fastcore.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290e5f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from httpx import RequestNotRead\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c538f015",
   "metadata": {},
   "source": [
    "`cachy.jsonl` contains one API response per line. \n",
    "\n",
    "Each line has the following format `{\"key\": key, \"response\": response}` \n",
    "\n",
    "- `key`: hash of the API request\n",
    "- `response`: the API response. \n",
    "\n",
    "```json\n",
    "{\n",
    "    \"key\": \"afc2be0c\", \n",
    "    \"response\": \"{\\\"id\\\":\\\"msg_xxx\\\",\\\"type\\\":\\\"message\\\",\\\"role\\\":\\\"assistant\\\",\\\"model\\\":\\\"claude-sonnet-4-20250514\\\",\\\"content\\\":[{\\\"type\\\":\\\"text\\\",\\\"text\\\":\\\"Coordination.\\\"}],\\\"stop_reason\\\":\\\"end_turn\\\",\\\"stop_sequence\\\":null,\\\"usage\\\":{\\\"input_tokens\\\":16,\\\"cache_creation_input_tokens\\\":0,\\\"cache_read_input_tokens\\\":0,\\\"cache_creation\\\":{\\\"ephemeral_5m_input_tokens\\\":0,\\\"ephemeral_1h_input_tokens\\\":0},\\\"output_tokens\\\":6,\\\"service_tier\\\":\\\"standard\\\"}}\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af73959f",
   "metadata": {},
   "source": [
    "### Patching `httpx`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc52773",
   "metadata": {},
   "source": [
    "Patching a method is very straightforward. \n",
    "\n",
    "In our case we want to patch `httpx._client.Client.send` and `httpx._client.AsyncClient.send`. \n",
    "\n",
    "These methods are called when running `httpx.get`, `httpx.post`, etc. \n",
    "\n",
    "In the example below we use `@patch` from [fastcore](https://fastcore.fast.ai/) to print `calling an API` when `httpx._client.Client.send` is run.\n",
    "\n",
    "```python\n",
    "@patch\n",
    "def send(self:httpx._client.Client, r, **kwargs):\n",
    "    print('calling an API')\n",
    "    return self._orig_send(r, **kwargs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24733a7f",
   "metadata": {},
   "source": [
    "### Cache Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5876c71",
   "metadata": {},
   "source": [
    "Now, let's build up our caching logic piece-by-piece.\n",
    "\n",
    "The first thing we need to do is ensure that our caching logic only runs on specific urls.\n",
    "\n",
    "For now, let's only cache API calls made to popular LLM providers like OpenAI, Anthropic, Google and DeepSeek. We can make this fully customizable later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527e76df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "doms = (\"api.openai.com\", \"api.anthropic.com\", \"generativelanguage.googleapis.com\", \"api.deepseek.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09571365",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def _should_cache(url, doms): return any(dom in str(url) for dom in doms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6d3122",
   "metadata": {},
   "source": [
    "We could then use `_should_cache` like this.\n",
    "\n",
    "```python\n",
    "@patch\n",
    "def send(self:httpx._client.Client, r, **kwargs):\n",
    "    if not _should_cache(r.url, doms): return self._orig_send(r, **kwargs)\n",
    "    # insert caching logic\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6308294d",
   "metadata": {},
   "source": [
    "### Cache Key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8580082d",
   "metadata": {},
   "source": [
    "The next thing we need to do is figure out if a response for the request `r` already exists in our cache. \n",
    "\n",
    "Recall that each line in `cachy.jsonl` has the following format `{\"key\": key, \"response\": response}`.\n",
    "\n",
    "Our key needs to be unique and deterministic. One way to do this is to concatenate the request URL and content, then generate a hash from the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c091e1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _key(r): return hashlib.sha256(str(r.url.copy_remove_param('key')).encode() + r.content).hexdigest()[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5d99cf",
   "metadata": {},
   "source": [
    "When LiteLLM calls Gemini it includes the API key in a query param so that's why we strip the `key` param from the url."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba8f701",
   "metadata": {},
   "source": [
    "Let's test this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a68a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = httpx.Request('POST', 'https://api.openai.com/v1/chat/completions', content=b'some content')\n",
    "r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7b3113",
   "metadata": {},
   "outputs": [],
   "source": [
    "_key(r1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cbb8f3",
   "metadata": {},
   "source": [
    "If we run it again we should get the same key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b819620d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_key(r1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e565db6",
   "metadata": {},
   "source": [
    "Let's modify the url and confirm we get a different key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007d9208",
   "metadata": {},
   "outputs": [],
   "source": [
    "_key(httpx.Request('POST', 'https://api.anthropic.com/v1/messages', content=b'some content'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcc7aa1",
   "metadata": {},
   "source": [
    "Great. Let's update our patch.\n",
    "\n",
    "```python\n",
    "@patch\n",
    "def send(self:httpx._client.Client, r, **kwargs):\n",
    "    if not _should_cache(r.url, doms): return self._orig_send(r, **kwargs)\n",
    "    key = _key(r)\n",
    "    # if cache hit return the response\n",
    "    # else run the request, write to response the cache and return it\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8107005e",
   "metadata": {},
   "source": [
    "### Cache Reads/Writes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acd1891",
   "metadata": {},
   "source": [
    "Now let's add some methods that will read from and write to `cachy.jsonl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6924d508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def _cache(key, cfp):\n",
    "    with open(cfp, \"r\") as f:\n",
    "        line = first(f, lambda l: json.loads(l)[\"key\"] == key)\n",
    "        return json.loads(line) if line else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a0213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def _write_cache(key, content, cfp, hdrs):\n",
    "    with open(cfp, \"a\") as f: f.write(json.dumps({\"key\":key, \"response\": content, \"headers\":hdrs})+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62486192",
   "metadata": {},
   "source": [
    "Let's update our `patch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d802ec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def send(self:httpx._client.Client, r, **kwargs):\n",
    "    if not _should_cache(r.url, doms): return self._orig_send(r, **kwargs)\n",
    "    key = key(r)\n",
    "    if res := _cache(key,\"cachy.jsonl\"): return httpx.Response(status_code=200, content=res, request=r)\n",
    "    res = self._orig_send(r, **kwargs)\n",
    "    content = res.read().decode()\n",
    "    _write_cache(key, content, \"cachy.jsonl\")\n",
    "    return httpx.Response(status_code=res.status_code, content=content, request=r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c2527e",
   "metadata": {},
   "source": [
    "### Multipart Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acee46f2",
   "metadata": {},
   "source": [
    "`_key` will throw the following error for multipart requests (e.g. file uploads).\n",
    "\n",
    "`RequestNotRead: Attempted to access streaming request content, without having called `read()`.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8690f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfu = httpx.Request('POST', 'https://api.openai.com/v1/chat/completions', files={\"file\": (\"test.txt\", b\"hello\")})\n",
    "rfu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df96e1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fail(lambda: _key(rfu), RequestNotRead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0512600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfu.read(); _key(rfu);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48414f2a",
   "metadata": {},
   "source": [
    "Each part of a multipart request is separated by a delimiter called a boundary with this structure `--b{RANDOM_ID}`. Here's an example for `rfu`.\n",
    "\n",
    "```txt\n",
    "b'--f9ee33966b45cc8c80952bb57cc728c4\\r\\nContent-Disposition: form-data; name=\"file\"; filename=\"test.txt\"\\r\\nContent-Type: text/plain\\r\\n\\r\\nhello\\r\\n--f9ee33966b45cc8c80952bb57cc728c4--\\r\\n'\n",
    "```\n",
    "\n",
    "As the boundary is a random id, two identical multipart requests will produce different boundaries. As the boundary is part of the request content, `_key` will generate different keys leading to cache misses ðŸ˜ž.\n",
    "\n",
    "Let's create a helper method `_content` that will extract content from any request and remove the non-deterministic boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7fca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _content(r):\n",
    "    \"Extract content from request.\"\n",
    "    if not hasattr(r, '_content'): r.read()\n",
    "    boundary = httpx._multipart.get_multipart_boundary_from_content_type(r.headers.get(\"Content-Type\", \"\").encode())\n",
    "    return r.content.replace(boundary, b\"cachy-boundary\") if boundary else r.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0533715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfu = httpx.Request('POST', 'https://api.openai.com/v1/chat/completions', files={\"file\": (\"test.txt\", b\"hello\")})\n",
    "rfu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ee5d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_content(rfu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f886766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _key(r): return hashlib.sha256(str(r.url.copy_remove_param('key')).encode() + _content(r)).hexdigest()[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd88bc2",
   "metadata": {},
   "source": [
    "Let's confirm that running `_key` multiple times on the same multipart request now returns the same key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c446f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_key(rfu), _key(rfu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f447ed40",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfae8317",
   "metadata": {},
   "source": [
    "Let's add support for streaming. \n",
    "\n",
    "First let's include an `is_stream` bool in our hash so that a non-streamed request will generate a different key to the same request when streamed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69ba22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def _key(r, is_stream=False):\n",
    "    \"Create a unique, deterministic id from the request `r`.\"\n",
    "    return hashlib.sha256(f\"{r.url.copy_remove_param('key')}{is_stream}\".encode() + _content(r)).hexdigest()[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeced1a2",
   "metadata": {},
   "source": [
    "In the `patch` we need to `consume` the entire stream before writing it to the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aa118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def send(self:httpx._client.Client, r, **kwargs):\n",
    "    is_stream = kwargs.get(\"stream\")\n",
    "    if not _should_cache(r.url, doms): return self._orig_send(r, **kwargs)\n",
    "    key = _key(r, is_stream=False)\n",
    "    if res := _cache(key,\"cachy.jsonl\"): return httpx.Response(status_code=200, content=res, request=r)\n",
    "    res = self._orig_send(r, **kwargs)\n",
    "    content = res.read().decode() if not is_stream else b''.join(list(res.iter_bytes())).decode()\n",
    "    _write_cache(key, content, \"cachy.jsonl\")\n",
    "    return httpx.Response(status_code=res.status_code, content=content, request=r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d748c8d",
   "metadata": {},
   "source": [
    "### `enable_cachy` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9013bea",
   "metadata": {},
   "source": [
    "To make `cachy` as user friendly as possible let's make it so that we can apply our patch by running a single method at the top of our notebook.\n",
    "\n",
    "```python\n",
    "from cachy import enable_cachy\n",
    "\n",
    "enable_cachy()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468e9956",
   "metadata": {},
   "source": [
    "For this to work we'll need to wrap our patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056e9c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_patch():    \n",
    "    @patch    \n",
    "    def send(self:httpx._client.Client, r, **kwargs):\n",
    "        is_stream = kwargs.get(\"stream\")\n",
    "        if not _should_cache(r.url, doms): return self._orig_send(r, **kwargs)\n",
    "        key = _key(r, is_stream=False)\n",
    "        if res := _cache(key,\"cachy.jsonl\"): return httpx.Response(status_code=200, content=res, request=r)\n",
    "        res = self._orig_send(r, **kwargs)\n",
    "        content = res.read().decode() if not is_stream else b''.join(list(res.iter_bytes())).decode()\n",
    "        _write_cache(key, content, \"cachy.jsonl\")\n",
    "        return httpx.Response(status_code=res.status_code, content=content, request=r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec81b076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enable_cachy():  \n",
    "    _apply_patch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b514b4",
   "metadata": {},
   "source": [
    "Great. Now, let's make `cachy` a little more customizable by making it possible to specify:\n",
    "\n",
    "- the APIs (or domains) to cache\n",
    "- the location of the cache file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b7f6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enable_cachy(cache_dir=None, doms=doms):\n",
    "    cfp = Path(cache_dir or find_file_parents(\"pyproject.toml\") or \".\") / \"cachy.jsonl\"\n",
    "    cfp.touch(exist_ok=True)   \n",
    "    _apply_patch(cfp, doms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a2bd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_patch(cfp, doms):    \n",
    "    @patch\n",
    "    def send(self:httpx._client.Client, r, **kwargs):\n",
    "        is_stream = kwargs.get(\"stream\")\n",
    "        if not _should_cache(r.url, doms): return self._orig_send(r, **kwargs)\n",
    "        key = _key(r, is_stream=False)\n",
    "        if res := _cache(key,cfp): return httpx.Response(status_code=200, content=res, request=r)\n",
    "        res = self._orig_send(r, **kwargs)\n",
    "        content = res.read().decode() if not is_stream else b''.join(list(res.iter_bytes())).decode()\n",
    "        _write_cache(key,content,cfp)\n",
    "        return httpx.Response(status_code=res.status_code, content=content, request=r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707f28fb",
   "metadata": {},
   "source": [
    "### Async"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195e5cbf",
   "metadata": {},
   "source": [
    "Some APIs such as Gemini Files API rely on response headers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ced1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _res_hdrs(res, hdrs=None): return {k: v for k, v in res.headers.items() if k.lower() in hdrs} if hdrs else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71262b62",
   "metadata": {},
   "source": [
    "Now let's add support for `async` requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78203d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def _apply_async_patch(cfp, doms, hdrs):    \n",
    "    @patch\n",
    "    async def send(self:httpx._client.AsyncClient, r, **kwargs):\n",
    "        is_stream = kwargs.get(\"stream\")\n",
    "        if not _should_cache(r.url, doms): return await self._orig_send(r, **kwargs)\n",
    "        key = _key(r, is_stream=False)\n",
    "        if res := _cache(key,cfp): return httpx.Response(status_code=200, content=res['response'], headers=res.get('headers'), request=r)\n",
    "        res = await self._orig_send(r, **kwargs)\n",
    "        content = res.read().decode() if not is_stream else b''.join([c async for c in res.aiter_bytes()]).decode()\n",
    "        headers = _res_hdrs(res, hdrs)\n",
    "        _write_cache(key,content,cfp,headers)\n",
    "        return httpx.Response(status_code=res.status_code, content=content, headers=headers, request=r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f67f865",
   "metadata": {},
   "source": [
    "Let's rename our original patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ee9a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def _apply_sync_patch(cfp, doms, hdrs):    \n",
    "    @patch\n",
    "    def send(self:httpx._client.Client, r, **kwargs):\n",
    "        is_stream = kwargs.get(\"stream\")\n",
    "        if not _should_cache(r.url, doms): return self._orig_send(r, **kwargs)\n",
    "        key = _key(r, is_stream=False)\n",
    "        if res := _cache(key,cfp): return httpx.Response(status_code=200, content=res['response'], headers=res.get('headers'), request=r)\n",
    "        res = self._orig_send(r, **kwargs)\n",
    "        content = res.read().decode() if not is_stream else b''.join(list(res.iter_bytes())).decode()\n",
    "        headers = _res_hdrs(res, hdrs)\n",
    "        _write_cache(key,content,cfp,headers)\n",
    "        return httpx.Response(status_code=res.status_code, content=content, headers=headers, request=r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a642c528",
   "metadata": {},
   "source": [
    "Finally, let's update `enable_cachy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05ad1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def enable_cachy(cache_dir=None, doms=doms, hdrs=None):\n",
    "    if hdrs is None: hdrs=[]\n",
    "    cfp = Path(cache_dir or find_file_parents(\"pyproject.toml\") or \".\") / \"cachy.jsonl\"\n",
    "    cfp.touch(exist_ok=True)   \n",
    "    _apply_sync_patch(cfp, doms, hdrs)\n",
    "    _apply_async_patch(cfp, doms, hdrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed290a0",
   "metadata": {},
   "source": [
    "And a way to turn if off:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c960325",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def disable_cachy():\n",
    "    httpx._client.AsyncClient.send = httpx._client.AsyncClient._orig_send\n",
    "    httpx._client.Client.send      = httpx._client.Client._orig_send"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4bced9",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331c8690",
   "metadata": {},
   "source": [
    "Let's test `enable_cachy` on 3 SDKs (OpenAI, Anthropic, LiteLLM) for the scenarios below:\n",
    "\n",
    "- sync requests with(out) streaming\n",
    "- async requests with(out) streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bf1d9a",
   "metadata": {},
   "source": [
    "Add some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6bda2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mods: ant=\"claude-sonnet-4-20250514\"; oai=\"gpt-4o\"; gem=\"gemini/gemini-2.5-flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2624539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_msgs(m): return [{\"role\": \"user\", \"content\": f\"write 1 word about {m}\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb679457",
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_cachy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d85bd2",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ed1055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1bf759",
   "metadata": {},
   "outputs": [],
   "source": [
    "cli = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d922cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = cli.responses.create(model=mods.oai, input=mk_msgs(\"openai sync\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d29424",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = cli.responses.create(model=mods.oai, input=mk_msgs(\"openai sync\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f362dd",
   "metadata": {},
   "source": [
    "Let's test streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da61876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = cli.responses.create(model=mods.oai, input=mk_msgs(\"openai sync streaming\"), stream=True)\n",
    "for ch in r: print(str(ch)[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db516546",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = cli.responses.create(model=mods.oai, input=mk_msgs(\"openai sync streaming\"), stream=True)\n",
    "for ch in r: print(str(ch)[:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b9ecf9",
   "metadata": {},
   "source": [
    "Let's test async."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595c658e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359b75e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cli = AsyncOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4814a1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = await cli.responses.create(model=mods.oai, input=mk_msgs(\"openai async\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8119cb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = await cli.responses.create(model=mods.oai, input=mk_msgs(\"openai async\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493117f7",
   "metadata": {},
   "source": [
    "Let's test async streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152c9702",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = await cli.responses.create(model=mods.oai, input=mk_msgs(\"openai async streaming\"), stream=True)\n",
    "async for ch in r: print(str(ch)[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33c9631",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = await cli.responses.create(model=mods.oai, input=mk_msgs(\"openai async streaming\"), stream=True)\n",
    "async for ch in r: print(str(ch)[:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd629f87",
   "metadata": {},
   "source": [
    "### Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdea6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c0bd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "cli = Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411ff629",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs(\"ant sync\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a245d059",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs(\"ant sync\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fbc541",
   "metadata": {},
   "source": [
    "Let's test streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba0ad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs(\"ant sync streaming\"), stream=True)\n",
    "for ch in r: print(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0e2ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs(\"ant sync streaming\"), stream=True)\n",
    "for ch in r: print(ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8564692",
   "metadata": {},
   "source": [
    "Let's test async."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cfd94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import AsyncAnthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591e9426",
   "metadata": {},
   "outputs": [],
   "source": [
    "cli = AsyncAnthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657bc916",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = await cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs(\"ant async\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0e58d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = await cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs(\"ant async\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4495b439",
   "metadata": {},
   "source": [
    "Let's test async streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b42c9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = await cli.messages.create(model=mods.ant,max_tokens=1024,messages=mk_msgs(\"ant async streaming\"), stream=True)\n",
    "async for ch in r.response.aiter_bytes(): print(ch.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a4b1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = await cli.messages.create(model=mods.ant,max_tokens=1024,messages=mk_msgs(\"ant async streaming\"), stream=True)\n",
    "async for ch in r.response.aiter_bytes(): print(ch.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9005138e",
   "metadata": {},
   "source": [
    "### LiteLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e918f9",
   "metadata": {},
   "source": [
    "Let's test the LiteLLM SDK by running sync/async calls with(out) streaming for OpenAI, Anthropic, & Gemini.\n",
    "\n",
    "We'll also double check tool calls and citations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84de3502",
   "metadata": {},
   "source": [
    "#### Sync Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d410d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac22f2de",
   "metadata": {},
   "source": [
    "Let's define a helper method to display a streamed response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3b6a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _stream(r): \n",
    "    for ch in r: print(ch.choices[0].delta.content or \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af15cf6a",
   "metadata": {},
   "source": [
    "##### Anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aecb14",
   "metadata": {},
   "source": [
    "Let's test `claude-sonnet-x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a1dd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = completion(model=mods.ant, messages=mk_msgs(\"lite: ant sync...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a962a63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = completion(model=mods.ant, messages=mk_msgs(\"lite: ant sync...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af92953",
   "metadata": {},
   "source": [
    "Now, with streaming enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd195108",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = completion(model=mods.ant, messages=mk_msgs(\"lite: ant sync stream...\"), stream=True)\n",
    "_stream(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43acec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = completion(model=mods.ant, messages=mk_msgs(\"lite: ant sync stream...\"), stream=True)\n",
    "_stream(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d755d14d",
   "metadata": {},
   "source": [
    "##### OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69376bcf",
   "metadata": {},
   "source": [
    "Let's test `gpt-4o`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affb84d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = completion(model=mods.oai, messages=mk_msgs(\"lite: oai sync...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed17001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = completion(model=mods.oai, messages=mk_msgs(\"lite: oai sync...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63010c88",
   "metadata": {},
   "source": [
    "Now, with streaming enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe98d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = completion(model=mods.oai, messages=mk_msgs(\"lite: oai sync stream...\"), stream=True)\n",
    "_stream(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd0c6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = completion(model=mods.oai, messages=mk_msgs(\"lite: oai sync stream...\"), stream=True)\n",
    "_stream(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaa5d5d",
   "metadata": {},
   "source": [
    "##### Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee787b41",
   "metadata": {},
   "source": [
    "Let's test `2.5-flash`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b6b685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205eeb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = completion(model=mods.gem, messages=mk_msgs(\"lite: gem sync...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f5b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = completion(model=mods.gem, messages=mk_msgs(\"lite: gem sync...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b9cb44",
   "metadata": {},
   "source": [
    "Now, with streaming enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a16022",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = completion(model=mods.gem, messages=mk_msgs(\"lite: gem sync stream...\"), stream=True)\n",
    "_stream(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd1db86",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = completion(model=mods.gem, messages=mk_msgs(\"lite: gem sync stream...\"), stream=True)\n",
    "_stream(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a974e039",
   "metadata": {},
   "source": [
    "#### Async Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9536a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import acompletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d399cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _astream(r):\n",
    "    async for chunk in r: print(chunk.choices[0].delta.content or \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d2b5ce",
   "metadata": {},
   "source": [
    "##### Anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e974c34c",
   "metadata": {},
   "source": [
    "Let's test `claude-sonnet-x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e7f30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = await acompletion(model=mods.ant, messages=mk_msgs(\"lite: ant async...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a80696",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = await acompletion(model=mods.ant, messages=mk_msgs(\"lite: ant async...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f35af59",
   "metadata": {},
   "source": [
    "Now, with streaming enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0b902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = await acompletion(model=mods.ant, messages=mk_msgs(\"lite: ant async stream...\"), stream=True)\n",
    "await(_astream(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f402f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = await acompletion(model=mods.ant, messages=mk_msgs(\"lite: ant async stream...\"), stream=True)\n",
    "await(_astream(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1f6b1a",
   "metadata": {},
   "source": [
    "##### OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5c220e",
   "metadata": {},
   "source": [
    "Let's test `gpt-4o`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0c12ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = await acompletion(model=mods.oai, messages=mk_msgs(\"lite: oai async...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d56291",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = await acompletion(model=mods.oai, messages=mk_msgs(\"lite: oai async...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5e0831",
   "metadata": {},
   "source": [
    "Now, with streaming enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75887c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = await acompletion(model=mods.oai, messages=mk_msgs(\"lite: oai async stream...\"), stream=True)\n",
    "await(_astream(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afd4a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = await acompletion(model=mods.oai, messages=mk_msgs(\"lite: oai async stream...\"), stream=True)\n",
    "await(_astream(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e397a46e",
   "metadata": {},
   "source": [
    "##### Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3136f3",
   "metadata": {},
   "source": [
    "Let's test `2.5-flash`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e703de6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = await acompletion(model=mods.gem, messages=mk_msgs(\"lite: gem async...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aee1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = await acompletion(model=mods.gem, messages=mk_msgs(\"lite: gem async...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf609ca",
   "metadata": {},
   "source": [
    "Now, with streaming enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7958e93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = await acompletion(model=mods.gem, messages=mk_msgs(\"lite: gem async stream...\"), stream=True)\n",
    "await(_astream(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f63db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = await acompletion(model=mods.gem, messages=mk_msgs(\"lite: gem async stream...\"), stream=True)\n",
    "await(_astream(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99d5ad4",
   "metadata": {},
   "source": [
    "#### Tool Calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691dc4c7",
   "metadata": {},
   "source": [
    "As a sanity check let's confirm that tool calls work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c0a42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\"type\":\"string\", \"description\":\"The city e.g. Reims\"},\n",
    "                    \"unit\": {\"type\":\"string\", \"enum\":[\"celsius\", \"fahrenheit\"]},\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80460dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = completion(model=mods.ant, messages=mk_msgs(\"Is it raining in Reims?\"), tools=tools)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94a5792",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = completion(model=mods.ant, messages=mk_msgs(\"Is it raining in Reims?\"), tools=tools)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629995d9",
   "metadata": {},
   "source": [
    "### Multipart Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a718ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cli = Anthropic()\n",
    "r = cli.beta.files.upload(file=(\"ex.txt\", b\"hello world\", \"text/plain\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bea763",
   "metadata": {},
   "outputs": [],
   "source": [
    "cli = Anthropic()\n",
    "r = cli.beta.files.upload(file=(\"ex.txt\", b\"hello world\", \"text/plain\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9503b81",
   "metadata": {},
   "source": [
    "### Gemini Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fbd97c",
   "metadata": {},
   "source": [
    "When LiteLLM calls Gemini it includes the model name in the url. Let's test that we can run the same prompt with two different Gemini models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839019b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mods.gem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbea562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = completion(model=mods.gem, messages=mk_msgs(\"lite: gem different models...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e2593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = completion(model=\"gemini/gemini-2.5-flash\", messages=mk_msgs(\"lite: gem different models...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1338588",
   "metadata": {},
   "source": [
    "### Gemini File Upload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914f9c0d",
   "metadata": {},
   "source": [
    "The `google-genai` SDK's `files.upload()` relies on `x-goog-upload-url` and `x-goog-upload-status` response headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde63027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b5f4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cli = genai.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bf0c3e",
   "metadata": {},
   "source": [
    "When no `hdrs` is provided the request fails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd0393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfw = tempfile.NamedTemporaryFile(suffix='.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4c629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = tfw.__enter__()\n",
    "fn = Path(f.name)\n",
    "fn.write_text(\"test content\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6384e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: gfile = cli.files.upload(file=fn)\n",
    "except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d943eb1",
   "metadata": {},
   "source": [
    "Remove the failed cache entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b7c2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfp = Path('../cachy.jsonl')\n",
    "lines = cfp.read_text().splitlines()\n",
    "_ = cfp.write_text('\\n'.join(lines[:-1]) + '\\n' if lines[:-1] else '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebbc164",
   "metadata": {},
   "source": [
    "When caching Gemini file uploads, by default request content only includes `mime_type` and `size_bytes`. This means different files with the same mime type and size produce identical cache keys, causing incorrect cache hits. The fix is to pass a file content fingerprint (a hash of the file bytes) as the `display_name` in the upload config: `cli.files.upload(file=fn, config={\"display_name\": _fingerprint(fn)})`. This ensures the request body is unique per file content, generating distinct cache keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630aaa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fingerprint(path): return hashlib.sha256(Path(path).read_bytes()).hexdigest()[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b475da38",
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_cachy(hdrs=['x-goog-upload-url', 'x-goog-upload-status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae63ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gfile = cli.files.upload(file=fn, config={\"display_name\": _fingerprint(fn)})\n",
    "gfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6698bda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfw.__exit__(None, None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2cb334",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044debe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
