{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Cache your API calls and make your notebooks fast again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often call APIs while prototyping and testing our code. A single API call (e.g. an Anthropic chat completion) can take 100's of ms to run. This can really slow down development especially if our notebook contains many API calls ðŸ˜ž."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cachy` caches API requests. It does this by saving the result of each API call to a local `cachy.jsonl` file. Before calling an API (e.g. OpenAI) it will check if the request already exists in `cachy.jsonl`. If it does it will return the cached result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does it work?**\n",
    "\n",
    "Under the hood popular SDK's like OpenAI, Anthropic and LiteLLM use `httpx.Client` and `httpx.AsyncClient`. \n",
    "\n",
    "`cachy` patches the `send` method of both clients and injects a simple caching mechanism:\n",
    "\n",
    "- create a cache key from the request\n",
    "- if the key exists in `cachy.jsonl` return the cached response\n",
    "- if not, call the API and save the response to `cachy.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import hashlib,httpx,json\n",
    "from fastcore.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cachy.jsonl` contains one API response per line. \n",
    "\n",
    "Each line has the following format `{\"key\": key, \"response\": response}` \n",
    "\n",
    "- `key`: hash of the API request\n",
    "- `response`: the API response. \n",
    "\n",
    "```json\n",
    "{\n",
    "    \"key\": \"afc2be0c\", \n",
    "    \"response\": \"{\\\"id\\\":\\\"msg_xxx\\\",\\\"type\\\":\\\"message\\\",\\\"role\\\":\\\"assistant\\\",\\\"model\\\":\\\"claude-sonnet-4-20250514\\\",\\\"content\\\":[{\\\"type\\\":\\\"text\\\",\\\"text\\\":\\\"Coordination.\\\"}],\\\"stop_reason\\\":\\\"end_turn\\\",\\\"stop_sequence\\\":null,\\\"usage\\\":{\\\"input_tokens\\\":16,\\\"cache_creation_input_tokens\\\":0,\\\"cache_read_input_tokens\\\":0,\\\"cache_creation\\\":{\\\"ephemeral_5m_input_tokens\\\":0,\\\"ephemeral_1h_input_tokens\\\":0},\\\"output_tokens\\\":6,\\\"service_tier\\\":\\\"standard\\\"}}\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patching `httpx`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patching a method is very straightforward. \n",
    "\n",
    "In our case we want to patch `httpx._client.Client.send` and `httpx._client.AsyncClient.send`. \n",
    "\n",
    "These methods are called when running `httpx.get`, `httpx.post`, etc. \n",
    "\n",
    "In the example below we use `@patch` from [fastcore](https://fastcore.fast.ai/) to print `calling an API` when `httpx._client.Client.send` is run.\n",
    "\n",
    "```python\n",
    "@patch\n",
    "def send(self:httpx._client.Client, r, **kwargs):\n",
    "    print('calling an API')\n",
    "    return self._orig_send(r, **kwargs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build up our caching logic piece-by-piece.\n",
    "\n",
    "The first thing we need to do is ensure that our caching logic only runs on specific urls.\n",
    "\n",
    "For now, let's only cache API calls made to popular LLM providers like OpenAI, Anthropic, Google and DeepSeek. We can make this fully customizable later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "doms = (\"api.openai.com\", \"api.anthropic.com\", \"generativelanguage.googleapis.com\", \"api.deepseek.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def _should_cache(url, doms): return any(dom in str(url) for dom in doms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could then use `_should_cache` like this.\n",
    "\n",
    "```python\n",
    "@patch\n",
    "def send(self:httpx._client.Client, r, **kwargs):\n",
    "    if not _should_cache(r.url, doms): return self._orig_send(r, **kwargs)\n",
    "    # insert caching logic\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we need to do is figure out if a response for the request `r` already exists in our cache. \n",
    "\n",
    "Recall that each line in `cachy.jsonl` has the following format `{\"key\": key, \"response\": response}`.\n",
    "\n",
    "Our key needs to be unique and deterministic. One way to do this is to concatenate the request URL and content, then generate a hash from the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _key(r): return hashlib.sha256(str(r.url.host).encode() + r.content).hexdigest()[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `r.url.host` instead of `r.url` because when LiteLLM calls Gemini it includes the API key in a query param. See [#1](https://github.com/AnswerDotAI/cachy/issues/1).\n",
    "\n",
    "If we used `r.url` we wouldn't be able to use dummy api keys when running our notebooks in a CI pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Request('POST', 'https://api.openai.com/v1/chat/completions')>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = httpx.Request('POST', 'https://api.openai.com/v1/chat/completions', content=b'some content')\n",
    "r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'80acdde4'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_key(r1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run it again we should get the same key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'80acdde4'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_key(r1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify the url and confirm we get a different key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2707fa05'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_key(httpx.Request('POST', 'https://api.anthropic.com/v1/messages', content=b'some content'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Let's update our patch.\n",
    "\n",
    "```python\n",
    "@patch\n",
    "def send(self:httpx._client.Client, r, **kwargs):\n",
    "    if not _should_cache(r.url, doms): return self._orig_send(r, **kwargs)\n",
    "    key = _key(r)\n",
    "    # if cache hit return the response\n",
    "    # else run the request, write to response the cache and return it\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache Reads/Writes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add some methods that will read from and write to `cachy.jsonl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def _cache(key, cfp):\n",
    "    with open(cfp, \"r\") as f:\n",
    "        line = first(f, lambda l: json.loads(l)[\"key\"] == key)\n",
    "        return json.loads(line)[\"response\"] if line else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def _write_cache(key, content, cfp):\n",
    "    with open(cfp, \"a\") as f: f.write(json.dumps({\"key\":key, \"response\": content})+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's update our `patch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def send(self:httpx._client.Client, r, **kwargs):\n",
    "    if not _should_cache(r.url, doms): return self._orig_send(r, **kwargs)\n",
    "    key = key(r)\n",
    "    if res := _cache(key,\"cachy.jsonl\"): return httpx.Response(status_code=200, content=res, request=r)\n",
    "    res = self._orig_send(r, **kwargs)\n",
    "    content = res.read().decode()\n",
    "    _write_cache(key, content, \"cachy.jsonl\")\n",
    "    return httpx.Response(status_code=res.status_code, content=content, request=r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add support for streaming. \n",
    "\n",
    "First let's include an `is_stream` bool in our hash so that a non-streamed request will generate a different key to the same request when streamed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def _key(r, is_stream=False):\n",
    "    \"Create a unique, deterministic id from the request `r`.\"\n",
    "    return hashlib.sha256(f\"{r.url.host}{is_stream}\".encode() + r.content).hexdigest()[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `patch` we need to `consume` the entire stream before writing it to the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def send(self:httpx._client.Client, r, **kwargs):\n",
    "    is_stream = kwargs.get(\"stream\")\n",
    "    if not _should_cache(r.url, doms): return self._orig_send(r, **kwargs)\n",
    "    key = _key(r, is_stream=False)\n",
    "    if res := _cache(key,\"cachy.jsonl\"): return httpx.Response(status_code=200, content=res, request=r)\n",
    "    res = self._orig_send(r, **kwargs)\n",
    "    content = res.read().decode() if not is_stream else b''.join(list(res.iter_bytes())).decode()\n",
    "    _write_cache(key, content, \"cachy.jsonl\")\n",
    "    return httpx.Response(status_code=res.status_code, content=content, request=r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `enable_cachy` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make `cachy` as user friendly as possible let's make it so that we can apply our patch by running a single method at the top of our notebook.\n",
    "\n",
    "```python\n",
    "from cachy import enable_cachy\n",
    "\n",
    "enable_cachy()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this to work we'll need to wrap our patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_patch():    \n",
    "    @patch    \n",
    "    def send(self:httpx._client.Client, r, **kwargs):\n",
    "        is_stream = kwargs.get(\"stream\")\n",
    "        if not _should_cache(r.url, doms): return self._orig_send(r, **kwargs)\n",
    "        key = _key(r, is_stream=False)\n",
    "        if res := _cache(key,\"cachy.jsonl\"): return httpx.Response(status_code=200, content=res, request=r)\n",
    "        res = self._orig_send(r, **kwargs)\n",
    "        content = res.read().decode() if not is_stream else b''.join(list(res.iter_bytes())).decode()\n",
    "        _write_cache(key, content, \"cachy.jsonl\")\n",
    "        return httpx.Response(status_code=res.status_code, content=content, request=r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enable_cachy():  \n",
    "    _apply_patch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Now, let's make `cachy` a little more customizable by making it possible to specify:\n",
    "\n",
    "- the APIs (or domains) to cache\n",
    "- the location of the cache file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enable_cachy(cache_dir=None, doms=doms):\n",
    "    cfp = Path(cache_dir or getattr(Config.find(\"settings.ini\"), \"config_path\", \".\")) / \"cachy.jsonl\"\n",
    "    cfp.touch(exist_ok=True)   \n",
    "    _apply_patch(cfp, doms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: If our notebook is running in an nbdev project `Config.find(\"settings.ini\").config_path` automatically finds the base dir.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_patch(cfp, doms):    \n",
    "    @patch\n",
    "    def send(self:httpx._client.Client, r, **kwargs):\n",
    "        is_stream = kwargs.get(\"stream\")\n",
    "        if not _should_cache(r.url, doms): return self._orig_send(r, **kwargs)\n",
    "        key = _key(r, is_stream=False)\n",
    "        if res := _cache(key,cfp): return httpx.Response(status_code=200, content=res, request=r)\n",
    "        res = self._orig_send(r, **kwargs)\n",
    "        content = res.read().decode() if not is_stream else b''.join(list(res.iter_bytes())).decode()\n",
    "        _write_cache(key,content,cfp)\n",
    "        return httpx.Response(status_code=res.status_code, content=content, request=r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add support for `async` requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def _apply_async_patch(cfp, doms):    \n",
    "    @patch\n",
    "    async def send(self:httpx._client.AsyncClient, r, **kwargs):\n",
    "        is_stream = kwargs.get(\"stream\")\n",
    "        if not _should_cache(r.url, doms): return await self._orig_send(r, **kwargs)\n",
    "        key = _key(r, is_stream=False)\n",
    "        if res := _cache(key, cfp): return httpx.Response(status_code=200, content=res, request=r)\n",
    "        res = await self._orig_send(r, **kwargs)\n",
    "        content = res.read().decode() if not is_stream else b''.join([c async for c in res.aiter_bytes()]).decode()\n",
    "        _write_cache(key, content, cfp)\n",
    "        return httpx.Response(status_code=res.status_code, content=content, request=r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rename our original patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def _apply_sync_patch(cfp, doms):    \n",
    "    @patch\n",
    "    def send(self:httpx._client.Client, r, **kwargs):\n",
    "        is_stream = kwargs.get(\"stream\")\n",
    "        if not _should_cache(r.url, doms): return self._orig_send(r, **kwargs)\n",
    "        key = _key(r, is_stream=False)\n",
    "        if res := _cache(key,cfp): return httpx.Response(status_code=200, content=res, request=r)\n",
    "        res = self._orig_send(r, **kwargs)\n",
    "        content = res.read().decode() if not is_stream else b''.join(list(res.iter_bytes())).decode()\n",
    "        _write_cache(key,content,cfp)\n",
    "        return httpx.Response(status_code=res.status_code, content=content, request=r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's update `enable_cachy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enable_cachy(cache_dir=None, doms=doms):\n",
    "    cfp = Path(cache_dir or getattr(Config.find(\"settings.ini\"), \"config_path\", \".\")) / \"cachy.jsonl\"\n",
    "    cfp.touch(exist_ok=True)   \n",
    "    _apply_sync_patch(cfp, doms)\n",
    "    _apply_async_patch(cfp, doms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patching `litellm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A LiteLLM `ModelResponse(Stream)` has a random `id` and a `created` timestamp. These are not provided by the http response but generated client side. As a result we have to find a different way of making these identical on consecutive requests.\n",
    "\n",
    "For the `ModelResponesStream` we can patch the `__init__` function. However, for the `ModelResponse` this is not enough, because in the `(a)completion` function litellm overwrites `model_response.created` after the object has been initialized. To resolve this we wrap the `(a)completion` and overwrite the response as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def _apply_litellm_patch():\n",
    "    try: import litellm\n",
    "    except ImportError: return  \n",
    "    \n",
    "    @patch\n",
    "    def __init__(self:litellm.ModelResponseStream, *args, **kwargs):\n",
    "        result = self._orig___init__(*args, **kwargs)\n",
    "        self.id,self.created  = 'cachy-dummy-id',1\n",
    "        return result\n",
    "    \n",
    "    @patch\n",
    "    def __init__(self:litellm.ModelResponse, *args, **kwargs):\n",
    "        result = self._orig___init__(*args, **kwargs)\n",
    "        self.id,self.created  = 'cachy-dummy-id',1\n",
    "        return result\n",
    "    \n",
    "    original_completion = litellm.completion\n",
    "    def wrapped_completion(*args, **kwargs):\n",
    "        res = original_completion(*args, **kwargs)\n",
    "        if hasattr(res, 'id'): res.id = 'cachy-dummy-id'\n",
    "        if hasattr(res, 'created'): res.created = 1\n",
    "        return res\n",
    "    litellm.completion = wrapped_completion\n",
    "    \n",
    "    original_acompletion = litellm.acompletion\n",
    "    async def wrapped_acompletion(*args, **kwargs):\n",
    "        res = await original_acompletion(*args, **kwargs)\n",
    "        if hasattr(res, 'id'): res.id = 'cachy-dummy-id'\n",
    "        if hasattr(res, 'created'): res.created = 1\n",
    "        return res\n",
    "    litellm.acompletion = wrapped_acompletion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we export our `enable_cachy` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def enable_cachy(cache_dir=None, doms=doms):\n",
    "    cfp = Path(cache_dir or getattr(Config.find(\"settings.ini\"), \"config_path\", \".\")) / \"cachy.jsonl\"\n",
    "    cfp.touch(exist_ok=True)   \n",
    "    _apply_sync_patch(cfp, doms)\n",
    "    _apply_async_patch(cfp, doms)\n",
    "    _apply_litellm_patch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test `enable_cachy` on 3 SDKs (OpenAI, Anthropic, LiteLLM) for the scenarios below:\n",
    "\n",
    "- sync requests with(out) streaming\n",
    "- async requests with(out) streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mods: ant=\"claude-sonnet-4-20250514\"; oai=\"gpt-4o\"; gem=\"gemini/gemini-2.0-flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_msgs(m): return [{\"role\": \"user\", \"content\": f\"write 1 word about {m}\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_cachy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Collaboration\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_0adc61ccfb3938da0068c80baeb59c81a29e449ca5f3333fa2\n",
       "- created_at: 1757940654.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: None\n",
       "- metadata: {}\n",
       "- model: gpt-4o-2024-08-06\n",
       "- object: response\n",
       "- output: [ResponseOutputMessage(id='msg_0adc61ccfb3938da0068c80baf67fc81a2b8fe99f555767ec5', content=[ResponseOutputText(annotations=[], text='Collaboration', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: []\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- conversation: None\n",
       "- max_output_tokens: None\n",
       "- max_tool_calls: None\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- prompt_cache_key: None\n",
       "- reasoning: Reasoning(effort=None, generate_summary=None, summary=None)\n",
       "- safety_identifier: None\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium')\n",
       "- top_logprobs: 0\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=15, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=3, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=18)\n",
       "- user: None\n",
       "- store: True\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_0adc61ccfb3938da0068c80baeb59c81a29e449ca5f3333fa2', created_at=1757940654.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[ResponseOutputMessage(id='msg_0adc61ccfb3938da0068c80baf67fc81a2b8fe99f555767ec5', content=[ResponseOutputText(annotations=[], text='Collaboration', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=In: 15; Out: 3; Total: 18, user=None, store=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = cli.responses.create(model=mods.oai, input=mk_msgs(\"openai sync\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Collaboration\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_0adc61ccfb3938da0068c80baeb59c81a29e449ca5f3333fa2\n",
       "- created_at: 1757940654.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: None\n",
       "- metadata: {}\n",
       "- model: gpt-4o-2024-08-06\n",
       "- object: response\n",
       "- output: [ResponseOutputMessage(id='msg_0adc61ccfb3938da0068c80baf67fc81a2b8fe99f555767ec5', content=[ResponseOutputText(annotations=[], text='Collaboration', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: []\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- conversation: None\n",
       "- max_output_tokens: None\n",
       "- max_tool_calls: None\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- prompt_cache_key: None\n",
       "- reasoning: Reasoning(effort=None, generate_summary=None, summary=None)\n",
       "- safety_identifier: None\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium')\n",
       "- top_logprobs: 0\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=15, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=3, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=18)\n",
       "- user: None\n",
       "- store: True\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_0adc61ccfb3938da0068c80baeb59c81a29e449ca5f3333fa2', created_at=1757940654.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[ResponseOutputMessage(id='msg_0adc61ccfb3938da0068c80baf67fc81a2b8fe99f555767ec5', content=[ResponseOutputText(annotations=[], text='Collaboration', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=In: 15; Out: 3; Total: 18, user=None, store=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = cli.responses.create(model=mods.oai, input=mk_msgs(\"openai sync\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResponseCreatedEvent(response=Response(id='resp_0bedad8c74657bc30068c80bb26cf4819ca6920df9fbd23916', created_at=1757940658.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=None, user=None, store=True), sequence_number=0, type='response.created')\n",
      "ResponseInProgressEvent(response=Response(id='resp_0bedad8c74657bc30068c80bb26cf4819ca6920df9fbd23916', created_at=1757940658.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=None, user=None, store=True), sequence_number=1, type='response.in_progress')\n",
      "ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='msg_0bedad8c74657bc30068c80bb33eb4819c9b4ac3a1ef6111d7', content=[], role='assistant', status='in_progress', type='message'), output_index=0, sequence_number=2, type='response.output_item.added')\n",
      "ResponseContentPartAddedEvent(content_index=0, item_id='msg_0bedad8c74657bc30068c80bb33eb4819c9b4ac3a1ef6111d7', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text', logprobs=[]), sequence_number=3, type='response.content_part.added')\n",
      "ResponseTextDeltaEvent(content_index=0, delta='Innov', item_id='msg_0bedad8c74657bc30068c80bb33eb4819c9b4ac3a1ef6111d7', logprobs=[], output_index=0, sequence_number=4, type='response.output_text.delta', obfuscation='VCpZLMzWQSq')\n",
      "ResponseTextDeltaEvent(content_index=0, delta='ative', item_id='msg_0bedad8c74657bc30068c80bb33eb4819c9b4ac3a1ef6111d7', logprobs=[], output_index=0, sequence_number=5, type='response.output_text.delta', obfuscation='xFQTnzLeSC8')\n",
      "ResponseTextDoneEvent(content_index=0, item_id='msg_0bedad8c74657bc30068c80bb33eb4819c9b4ac3a1ef6111d7', logprobs=[], output_index=0, sequence_number=6, text='Innovative', type='response.output_text.done')\n",
      "ResponseContentPartDoneEvent(content_index=0, item_id='msg_0bedad8c74657bc30068c80bb33eb4819c9b4ac3a1ef6111d7', output_index=0, part=ResponseOutputText(annotations=[], text='Innovative', type='output_text', logprobs=[]), sequence_number=7, type='response.content_part.done')\n",
      "ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='msg_0bedad8c74657bc30068c80bb33eb4819c9b4ac3a1ef6111d7', content=[ResponseOutputText(annotations=[], text='Innovative', type='output_text', logprobs=[])], role='assistant', status='completed', type='message'), output_index=0, sequence_number=8, type='response.output_item.done')\n",
      "ResponseCompletedEvent(response=Response(id='resp_0bedad8c74657bc30068c80bb26cf4819ca6920df9fbd23916', created_at=1757940658.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[ResponseOutputMessage(id='msg_0bedad8c74657bc30068c80bb33eb4819c9b4ac3a1ef6111d7', content=[ResponseOutputText(annotations=[], text='Innovative', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=In: 16; Out: 3; Total: 19, user=None, store=True), sequence_number=9, type='response.completed')\n"
     ]
    }
   ],
   "source": [
    "r = cli.responses.create(model=mods.oai, input=mk_msgs(\"openai sync streaming\"), stream=True)\n",
    "for ch in r: print(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResponseCreatedEvent(response=Response(id='resp_0bedad8c74657bc30068c80bb26cf4819ca6920df9fbd23916', created_at=1757940658.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=None, user=None, store=True), sequence_number=0, type='response.created')\n",
      "ResponseInProgressEvent(response=Response(id='resp_0bedad8c74657bc30068c80bb26cf4819ca6920df9fbd23916', created_at=1757940658.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=None, user=None, store=True), sequence_number=1, type='response.in_progress')\n",
      "ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='msg_0bedad8c74657bc30068c80bb33eb4819c9b4ac3a1ef6111d7', content=[], role='assistant', status='in_progress', type='message'), output_index=0, sequence_number=2, type='response.output_item.added')\n",
      "ResponseContentPartAddedEvent(content_index=0, item_id='msg_0bedad8c74657bc30068c80bb33eb4819c9b4ac3a1ef6111d7', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text', logprobs=[]), sequence_number=3, type='response.content_part.added')\n",
      "ResponseTextDeltaEvent(content_index=0, delta='Innov', item_id='msg_0bedad8c74657bc30068c80bb33eb4819c9b4ac3a1ef6111d7', logprobs=[], output_index=0, sequence_number=4, type='response.output_text.delta', obfuscation='VCpZLMzWQSq')\n",
      "ResponseTextDeltaEvent(content_index=0, delta='ative', item_id='msg_0bedad8c74657bc30068c80bb33eb4819c9b4ac3a1ef6111d7', logprobs=[], output_index=0, sequence_number=5, type='response.output_text.delta', obfuscation='xFQTnzLeSC8')\n",
      "ResponseTextDoneEvent(content_index=0, item_id='msg_0bedad8c74657bc30068c80bb33eb4819c9b4ac3a1ef6111d7', logprobs=[], output_index=0, sequence_number=6, text='Innovative', type='response.output_text.done')\n",
      "ResponseContentPartDoneEvent(content_index=0, item_id='msg_0bedad8c74657bc30068c80bb33eb4819c9b4ac3a1ef6111d7', output_index=0, part=ResponseOutputText(annotations=[], text='Innovative', type='output_text', logprobs=[]), sequence_number=7, type='response.content_part.done')\n",
      "ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='msg_0bedad8c74657bc30068c80bb33eb4819c9b4ac3a1ef6111d7', content=[ResponseOutputText(annotations=[], text='Innovative', type='output_text', logprobs=[])], role='assistant', status='completed', type='message'), output_index=0, sequence_number=8, type='response.output_item.done')\n",
      "ResponseCompletedEvent(response=Response(id='resp_0bedad8c74657bc30068c80bb26cf4819ca6920df9fbd23916', created_at=1757940658.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[ResponseOutputMessage(id='msg_0bedad8c74657bc30068c80bb33eb4819c9b4ac3a1ef6111d7', content=[ResponseOutputText(annotations=[], text='Innovative', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=In: 16; Out: 3; Total: 19, user=None, store=True), sequence_number=9, type='response.completed')\n"
     ]
    }
   ],
   "source": [
    "r = cli.responses.create(model=mods.oai, input=mk_msgs(\"openai sync streaming\"), stream=True)\n",
    "for ch in r: print(ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test async."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli = AsyncOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Innovative\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_07966b24b5f8fcde0068c80bc084cc819785b99bc0eaa06e47\n",
       "- created_at: 1757940672.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: None\n",
       "- metadata: {}\n",
       "- model: gpt-4o-2024-08-06\n",
       "- object: response\n",
       "- output: [ResponseOutputMessage(id='msg_07966b24b5f8fcde0068c80bc1038481978db7b032ab75dbef', content=[ResponseOutputText(annotations=[], text='Innovative', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: []\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- conversation: None\n",
       "- max_output_tokens: None\n",
       "- max_tool_calls: None\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- prompt_cache_key: None\n",
       "- reasoning: Reasoning(effort=None, generate_summary=None, summary=None)\n",
       "- safety_identifier: None\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium')\n",
       "- top_logprobs: 0\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=15, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=3, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=18)\n",
       "- user: None\n",
       "- store: True\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_07966b24b5f8fcde0068c80bc084cc819785b99bc0eaa06e47', created_at=1757940672.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[ResponseOutputMessage(id='msg_07966b24b5f8fcde0068c80bc1038481978db7b032ab75dbef', content=[ResponseOutputText(annotations=[], text='Innovative', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=In: 15; Out: 3; Total: 18, user=None, store=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await cli.responses.create(model=mods.oai, input=mk_msgs(\"openai async\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Innovative\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_07966b24b5f8fcde0068c80bc084cc819785b99bc0eaa06e47\n",
       "- created_at: 1757940672.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: None\n",
       "- metadata: {}\n",
       "- model: gpt-4o-2024-08-06\n",
       "- object: response\n",
       "- output: [ResponseOutputMessage(id='msg_07966b24b5f8fcde0068c80bc1038481978db7b032ab75dbef', content=[ResponseOutputText(annotations=[], text='Innovative', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: []\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- conversation: None\n",
       "- max_output_tokens: None\n",
       "- max_tool_calls: None\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- prompt_cache_key: None\n",
       "- reasoning: Reasoning(effort=None, generate_summary=None, summary=None)\n",
       "- safety_identifier: None\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium')\n",
       "- top_logprobs: 0\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=15, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=3, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=18)\n",
       "- user: None\n",
       "- store: True\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_07966b24b5f8fcde0068c80bc084cc819785b99bc0eaa06e47', created_at=1757940672.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[ResponseOutputMessage(id='msg_07966b24b5f8fcde0068c80bc1038481978db7b032ab75dbef', content=[ResponseOutputText(annotations=[], text='Innovative', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=In: 15; Out: 3; Total: 18, user=None, store=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await cli.responses.create(model=mods.oai, input=mk_msgs(\"openai async\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test async streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResponseCreatedEvent(response=Response(id='resp_09667f14142ea18b0068c80bc16c8481a1988be17cf34e1823', created_at=1757940673.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=None, user=None, store=True), sequence_number=0, type='response.created')\n",
      "ResponseInProgressEvent(response=Response(id='resp_09667f14142ea18b0068c80bc16c8481a1988be17cf34e1823', created_at=1757940673.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=None, user=None, store=True), sequence_number=1, type='response.in_progress')\n",
      "ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='msg_09667f14142ea18b0068c80bc1c07081a1913a81eae5c426d3', content=[], role='assistant', status='in_progress', type='message'), output_index=0, sequence_number=2, type='response.output_item.added')\n",
      "ResponseContentPartAddedEvent(content_index=0, item_id='msg_09667f14142ea18b0068c80bc1c07081a1913a81eae5c426d3', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text', logprobs=[]), sequence_number=3, type='response.content_part.added')\n",
      "ResponseTextDeltaEvent(content_index=0, delta='Eff', item_id='msg_09667f14142ea18b0068c80bc1c07081a1913a81eae5c426d3', logprobs=[], output_index=0, sequence_number=4, type='response.output_text.delta', obfuscation='bCNpIYrqdSk3w')\n",
      "ResponseTextDeltaEvent(content_index=0, delta='icient', item_id='msg_09667f14142ea18b0068c80bc1c07081a1913a81eae5c426d3', logprobs=[], output_index=0, sequence_number=5, type='response.output_text.delta', obfuscation='wHhRAwEPuj')\n",
      "ResponseTextDoneEvent(content_index=0, item_id='msg_09667f14142ea18b0068c80bc1c07081a1913a81eae5c426d3', logprobs=[], output_index=0, sequence_number=6, text='Efficient', type='response.output_text.done')\n",
      "ResponseContentPartDoneEvent(content_index=0, item_id='msg_09667f14142ea18b0068c80bc1c07081a1913a81eae5c426d3', output_index=0, part=ResponseOutputText(annotations=[], text='Efficient', type='output_text', logprobs=[]), sequence_number=7, type='response.content_part.done')\n",
      "ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='msg_09667f14142ea18b0068c80bc1c07081a1913a81eae5c426d3', content=[ResponseOutputText(annotations=[], text='Efficient', type='output_text', logprobs=[])], role='assistant', status='completed', type='message'), output_index=0, sequence_number=8, type='response.output_item.done')\n",
      "ResponseCompletedEvent(response=Response(id='resp_09667f14142ea18b0068c80bc16c8481a1988be17cf34e1823', created_at=1757940673.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[ResponseOutputMessage(id='msg_09667f14142ea18b0068c80bc1c07081a1913a81eae5c426d3', content=[ResponseOutputText(annotations=[], text='Efficient', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=In: 16; Out: 3; Total: 19, user=None, store=True), sequence_number=9, type='response.completed')\n"
     ]
    }
   ],
   "source": [
    "r = await cli.responses.create(model=mods.oai, input=mk_msgs(\"openai async streaming\"), stream=True)\n",
    "async for ch in r: print(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResponseCreatedEvent(response=Response(id='resp_09667f14142ea18b0068c80bc16c8481a1988be17cf34e1823', created_at=1757940673.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=None, user=None, store=True), sequence_number=0, type='response.created')\n",
      "ResponseInProgressEvent(response=Response(id='resp_09667f14142ea18b0068c80bc16c8481a1988be17cf34e1823', created_at=1757940673.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=None, user=None, store=True), sequence_number=1, type='response.in_progress')\n",
      "ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='msg_09667f14142ea18b0068c80bc1c07081a1913a81eae5c426d3', content=[], role='assistant', status='in_progress', type='message'), output_index=0, sequence_number=2, type='response.output_item.added')\n",
      "ResponseContentPartAddedEvent(content_index=0, item_id='msg_09667f14142ea18b0068c80bc1c07081a1913a81eae5c426d3', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text', logprobs=[]), sequence_number=3, type='response.content_part.added')\n",
      "ResponseTextDeltaEvent(content_index=0, delta='Eff', item_id='msg_09667f14142ea18b0068c80bc1c07081a1913a81eae5c426d3', logprobs=[], output_index=0, sequence_number=4, type='response.output_text.delta', obfuscation='bCNpIYrqdSk3w')\n",
      "ResponseTextDeltaEvent(content_index=0, delta='icient', item_id='msg_09667f14142ea18b0068c80bc1c07081a1913a81eae5c426d3', logprobs=[], output_index=0, sequence_number=5, type='response.output_text.delta', obfuscation='wHhRAwEPuj')\n",
      "ResponseTextDoneEvent(content_index=0, item_id='msg_09667f14142ea18b0068c80bc1c07081a1913a81eae5c426d3', logprobs=[], output_index=0, sequence_number=6, text='Efficient', type='response.output_text.done')\n",
      "ResponseContentPartDoneEvent(content_index=0, item_id='msg_09667f14142ea18b0068c80bc1c07081a1913a81eae5c426d3', output_index=0, part=ResponseOutputText(annotations=[], text='Efficient', type='output_text', logprobs=[]), sequence_number=7, type='response.content_part.done')\n",
      "ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='msg_09667f14142ea18b0068c80bc1c07081a1913a81eae5c426d3', content=[ResponseOutputText(annotations=[], text='Efficient', type='output_text', logprobs=[])], role='assistant', status='completed', type='message'), output_index=0, sequence_number=8, type='response.output_item.done')\n",
      "ResponseCompletedEvent(response=Response(id='resp_09667f14142ea18b0068c80bc16c8481a1988be17cf34e1823', created_at=1757940673.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[ResponseOutputMessage(id='msg_09667f14142ea18b0068c80bc1c07081a1913a81eae5c426d3', content=[ResponseOutputText(annotations=[], text='Efficient', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=In: 16; Out: 3; Total: 19, user=None, store=True), sequence_number=9, type='response.completed')\n"
     ]
    }
   ],
   "source": [
    "r = await cli.responses.create(model=mods.oai, input=mk_msgs(\"openai async streaming\"), stream=True)\n",
    "async for ch in r: print(ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli = Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Coordination\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `msg_01AgKMiEZKSXzQYnSYWaducJ`\n",
       "- content: `[{'citations': None, 'text': 'Coordination', 'type': 'text'}]`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- role: `assistant`\n",
       "- stop_reason: `end_turn`\n",
       "- stop_sequence: `None`\n",
       "- type: `message`\n",
       "- usage: `{'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 15, 'output_tokens': 5, 'server_tool_use': None, 'service_tier': 'standard'}`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Message(id='msg_01AgKMiEZKSXzQYnSYWaducJ', content=[TextBlock(citations=None, text='Coordination', type='text')], model='claude-sonnet-4-20250514', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=In: 15; Out: 5; Cache create: 0; Cache read: 0; Total Tokens: 20; Search: 0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs(\"ant sync\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Coordination\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `msg_01AgKMiEZKSXzQYnSYWaducJ`\n",
       "- content: `[{'citations': None, 'text': 'Coordination', 'type': 'text'}]`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- role: `assistant`\n",
       "- stop_reason: `end_turn`\n",
       "- stop_sequence: `None`\n",
       "- type: `message`\n",
       "- usage: `{'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 15, 'output_tokens': 5, 'server_tool_use': None, 'service_tier': 'standard'}`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Message(id='msg_01AgKMiEZKSXzQYnSYWaducJ', content=[TextBlock(citations=None, text='Coordination', type='text')], model='claude-sonnet-4-20250514', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=In: 15; Out: 5; Cache create: 0; Cache read: 0; Total Tokens: 20; Search: 0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs(\"ant sync\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RawMessageStartEvent(message=Message(id='msg_01Jnrv7itVfTgQGFkbGnoi6k', content=[], model='claude-sonnet-4-20250514', role='assistant', stop_reason=None, stop_sequence=None, type='message', usage=In: 16; Out: 1; Cache create: 0; Cache read: 0; Total Tokens: 17; Search: 0), type='message_start')\n",
      "RawContentBlockStartEvent(content_block=TextBlock(citations=None, text='', type='text'), index=0, type='content_block_start')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text='**', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text='Buffering**', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockStopEvent(index=0, type='content_block_stop')\n",
      "RawMessageDeltaEvent(delta=Delta(stop_reason='end_turn', stop_sequence=None), type='message_delta', usage=MessageDeltaUsage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=16, output_tokens=8, server_tool_use=None))\n",
      "RawMessageStopEvent(type='message_stop')\n"
     ]
    }
   ],
   "source": [
    "r = cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs(\"ant sync streaming\"), stream=True)\n",
    "for ch in r: print(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RawMessageStartEvent(message=Message(id='msg_01Jnrv7itVfTgQGFkbGnoi6k', content=[], model='claude-sonnet-4-20250514', role='assistant', stop_reason=None, stop_sequence=None, type='message', usage=In: 16; Out: 1; Cache create: 0; Cache read: 0; Total Tokens: 17; Search: 0), type='message_start')\n",
      "RawContentBlockStartEvent(content_block=TextBlock(citations=None, text='', type='text'), index=0, type='content_block_start')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text='**', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text='Buffering**', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockStopEvent(index=0, type='content_block_stop')\n",
      "RawMessageDeltaEvent(delta=Delta(stop_reason='end_turn', stop_sequence=None), type='message_delta', usage=MessageDeltaUsage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=16, output_tokens=8, server_tool_use=None))\n",
      "RawMessageStopEvent(type='message_stop')\n"
     ]
    }
   ],
   "source": [
    "r = cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs(\"ant sync streaming\"), stream=True)\n",
    "for ch in r: print(ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test async."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import AsyncAnthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli = AsyncAnthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**concurrent**\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `msg_01R33rKFKM5BqeJprT6A6DVM`\n",
       "- content: `[{'citations': None, 'text': '**concurrent**', 'type': 'text'}]`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- role: `assistant`\n",
       "- stop_reason: `end_turn`\n",
       "- stop_sequence: `None`\n",
       "- type: `message`\n",
       "- usage: `{'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 15, 'output_tokens': 6, 'server_tool_use': None, 'service_tier': 'standard'}`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Message(id='msg_01R33rKFKM5BqeJprT6A6DVM', content=[TextBlock(citations=None, text='**concurrent**', type='text')], model='claude-sonnet-4-20250514', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=In: 15; Out: 6; Cache create: 0; Cache read: 0; Total Tokens: 21; Search: 0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs(\"ant async\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**concurrent**\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `msg_01R33rKFKM5BqeJprT6A6DVM`\n",
       "- content: `[{'citations': None, 'text': '**concurrent**', 'type': 'text'}]`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- role: `assistant`\n",
       "- stop_reason: `end_turn`\n",
       "- stop_sequence: `None`\n",
       "- type: `message`\n",
       "- usage: `{'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 15, 'output_tokens': 6, 'server_tool_use': None, 'service_tier': 'standard'}`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Message(id='msg_01R33rKFKM5BqeJprT6A6DVM', content=[TextBlock(citations=None, text='**concurrent**', type='text')], model='claude-sonnet-4-20250514', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=In: 15; Out: 6; Cache create: 0; Cache read: 0; Total Tokens: 21; Search: 0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs(\"ant async\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test async streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event: message_start\n",
      "data: {\"type\":\"message_start\",\"message\":{\"id\":\"msg_019CQEuc6f7D2ewx7Co1PcTW\",\"type\":\"message\",\"role\":\"assistant\",\"model\":\"claude-sonnet-4-20250514\",\"content\":[],\"stop_reason\":null,\"stop_sequence\":null,\"usage\":{\"input_tokens\":16,\"cache_creation_input_tokens\":0,\"cache_read_input_tokens\":0,\"cache_creation\":{\"ephemeral_5m_input_tokens\":0,\"ephemeral_1h_input_tokens\":0},\"output_tokens\":1,\"service_tier\":\"standard\"}}       }\n",
      "\n",
      "event: content_block_start\n",
      "data: {\"type\":\"content_block_start\",\"index\":0,\"content_block\":{\"type\":\"text\",\"text\":\"\"}     }\n",
      "\n",
      "event: content_block_delta\n",
      "data: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"text_delta\",\"text\":\"Async\"}             }\n",
      "\n",
      "event: content_block_delta\n",
      "data: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"text_delta\",\"text\":\"Iterable\"}}\n",
      "\n",
      "event: ping\n",
      "data: {\"type\": \"ping\"}\n",
      "\n",
      "event: content_block_stop\n",
      "data: {\"type\":\"content_block_stop\",\"index\":0     }\n",
      "\n",
      "event: ping\n",
      "data: {\"type\": \"ping\"}\n",
      "\n",
      "event: ping\n",
      "data: {\"type\": \"ping\"}\n",
      "\n",
      "event: message_delta\n",
      "data: {\"type\":\"message_delta\",\"delta\":{\"stop_reason\":\"end_turn\",\"stop_sequence\":null},\"usage\":{\"input_tokens\":16,\"cache_creation_input_tokens\":0,\"cache_read_input_tokens\":0,\"output_tokens\":6}              }\n",
      "\n",
      "event: message_stop\n",
      "data: {\"type\":\"message_stop\"}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = await cli.messages.create(model=mods.ant,max_tokens=1024,messages=mk_msgs(\"ant async streaming\"), stream=True)\n",
    "async for ch in r.response.aiter_bytes(): print(ch.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event: message_start\n",
      "data: {\"type\":\"message_start\",\"message\":{\"id\":\"msg_019CQEuc6f7D2ewx7Co1PcTW\",\"type\":\"message\",\"role\":\"assistant\",\"model\":\"claude-sonnet-4-20250514\",\"content\":[],\"stop_reason\":null,\"stop_sequence\":null,\"usage\":{\"input_tokens\":16,\"cache_creation_input_tokens\":0,\"cache_read_input_tokens\":0,\"cache_creation\":{\"ephemeral_5m_input_tokens\":0,\"ephemeral_1h_input_tokens\":0},\"output_tokens\":1,\"service_tier\":\"standard\"}}       }\n",
      "\n",
      "event: content_block_start\n",
      "data: {\"type\":\"content_block_start\",\"index\":0,\"content_block\":{\"type\":\"text\",\"text\":\"\"}     }\n",
      "\n",
      "event: content_block_delta\n",
      "data: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"text_delta\",\"text\":\"Async\"}             }\n",
      "\n",
      "event: content_block_delta\n",
      "data: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"text_delta\",\"text\":\"Iterable\"}}\n",
      "\n",
      "event: ping\n",
      "data: {\"type\": \"ping\"}\n",
      "\n",
      "event: content_block_stop\n",
      "data: {\"type\":\"content_block_stop\",\"index\":0     }\n",
      "\n",
      "event: ping\n",
      "data: {\"type\": \"ping\"}\n",
      "\n",
      "event: ping\n",
      "data: {\"type\": \"ping\"}\n",
      "\n",
      "event: message_delta\n",
      "data: {\"type\":\"message_delta\",\"delta\":{\"stop_reason\":\"end_turn\",\"stop_sequence\":null},\"usage\":{\"input_tokens\":16,\"cache_creation_input_tokens\":0,\"cache_read_input_tokens\":0,\"output_tokens\":6}              }\n",
      "\n",
      "event: message_stop\n",
      "data: {\"type\":\"message_stop\"}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = await cli.messages.create(model=mods.ant,max_tokens=1024,messages=mk_msgs(\"ant async streaming\"), stream=True)\n",
    "async for ch in r.response.aiter_bytes(): print(ch.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LiteLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the LiteLLM SDK by running sync/async calls with(out) streaming for OpenAI, Anthropic, & Gemini.\n",
    "\n",
    "We'll also double check tool calls and citations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sync Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a helper method to display a streamed response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _stream(r): \n",
    "    for ch in r: print(ch.choices[0].delta.content or \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test `claude-sonnet-x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='cachy-dummy-id', created=1, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='**Streamlined**', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=8, prompt_tokens=18, total_tokens=26, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = completion(model=mods.ant, messages=mk_msgs(\"lite: ant sync...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='cachy-dummy-id', created=1, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='**Streamlined**', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=8, prompt_tokens=18, total_tokens=26, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = completion(model=mods.ant, messages=mk_msgs(\"lite: ant sync...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with streaming enabled.\n",
    "\n",
    "First lets verify the `id` and `created` patching is working here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ModelResponseStream(id='cachy-dummy-id', created=1, model='claude-sonnet-4-20250514', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='**', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, citations=None),\n",
       " ModelResponseStream(id='cachy-dummy-id', created=1, model='claude-sonnet-4-20250514', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='Lightweight**', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, citations=None),\n",
       " ModelResponseStream(id='cachy-dummy-id', created=1, model='claude-sonnet-4-20250514', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = completion(model=mods.ant, messages=mk_msgs(\"lite: ant sync stream...\"), stream=True)\n",
    "os = [o for o in r]\n",
    "os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's verify it's compatible with litelllm's stream_chunk_builder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import stream_chunk_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='cachy-dummy-id', created=1, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='**Lightweight**', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=7, prompt_tokens=19, total_tokens=26, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_chunk_builder(os)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success: the `id` and `created` fields are still patched!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test `gpt-4o`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='cachy-dummy-id', created=1, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_f33640a400', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Efficiency', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=1, prompt_tokens=18, total_tokens=19, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = completion(model=mods.oai, messages=mk_msgs(\"lite: oai sync...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='cachy-dummy-id', created=1, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_f33640a400', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Efficiency', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=1, prompt_tokens=18, total_tokens=19, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = completion(model=mods.oai, messages=mk_msgs(\"lite: oai sync...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with streaming enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synchronization\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = completion(model=mods.oai, messages=mk_msgs(\"lite: oai sync stream...\"), stream=True)\n",
    "_stream(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synchronization\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = completion(model=mods.oai, messages=mk_msgs(\"lite: oai sync stream...\"), stream=True)\n",
    "_stream(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test `2.0-flash`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='cachy-dummy-id', created=1, model='gemini-2.0-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Synchronization.\\n', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=3, prompt_tokens=10, total_tokens=13, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=10, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = completion(model=mods.gem, messages=mk_msgs(\"lite: gem sync...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='cachy-dummy-id', created=1, model='gemini-2.0-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Synchronization.\\n', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=3, prompt_tokens=10, total_tokens=13, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=10, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = completion(model=mods.gem, messages=mk_msgs(\"lite: gem sync...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with streaming enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eff\n",
      "ortless\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = completion(model=mods.gem, messages=mk_msgs(\"lite: gem sync stream...\"), stream=True)\n",
    "_stream(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eff\n",
      "ortless\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = completion(model=mods.gem, messages=mk_msgs(\"lite: gem sync stream...\"), stream=True)\n",
    "_stream(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Async Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import acompletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _astream(r):\n",
    "    async for chunk in r: print(chunk.choices[0].delta.content or \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test `claude-sonnet-x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='cachy-dummy-id', created=1, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='**coroutine**', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=8, prompt_tokens=18, total_tokens=26, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await acompletion(model=mods.ant, messages=mk_msgs(\"lite: ant async...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='cachy-dummy-id', created=1, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='**coroutine**', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=8, prompt_tokens=18, total_tokens=26, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await acompletion(model=mods.ant, messages=mk_msgs(\"lite: ant async...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with streaming enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "reactive**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = await acompletion(model=mods.ant, messages=mk_msgs(\"lite: ant async stream...\"), stream=True)\n",
    "await(_astream(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "reactive**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = await acompletion(model=mods.ant, messages=mk_msgs(\"lite: ant async stream...\"), stream=True)\n",
    "await(_astream(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test `gpt-4o`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='cachy-dummy-id', created=1, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_f33640a400', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Fast.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=2, prompt_tokens=18, total_tokens=20, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await acompletion(model=mods.oai, messages=mk_msgs(\"lite: oai async...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='cachy-dummy-id', created=1, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_f33640a400', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Fast.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=2, prompt_tokens=18, total_tokens=20, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await acompletion(model=mods.oai, messages=mk_msgs(\"lite: oai async...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with streaming enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eff\n",
      "icient\n",
      ".\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = await acompletion(model=mods.oai, messages=mk_msgs(\"lite: oai async stream...\"), stream=True)\n",
    "await(_astream(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eff\n",
      "icient\n",
      ".\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = await acompletion(model=mods.oai, messages=mk_msgs(\"lite: oai async stream...\"), stream=True)\n",
    "await(_astream(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test `2.0-flash`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='cachy-dummy-id', created=1, model='gemini-2.0-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Concurrency\\n', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=2, prompt_tokens=10, total_tokens=12, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=10, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await acompletion(model=mods.gem, messages=mk_msgs(\"lite: gem async...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='cachy-dummy-id', created=1, model='gemini-2.0-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Concurrency\\n', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=2, prompt_tokens=10, total_tokens=12, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=10, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await acompletion(model=mods.gem, messages=mk_msgs(\"lite: gem async...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with streaming enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concurrency\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = await acompletion(model=mods.gem, messages=mk_msgs(\"lite: gem async stream...\"), stream=True)\n",
    "await(_astream(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concurrency\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = await acompletion(model=mods.gem, messages=mk_msgs(\"lite: gem async stream...\"), stream=True)\n",
    "await(_astream(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tool Calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check let's confirm that tool calls work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\"type\":\"string\", \"description\":\"The city e.g. Reims\"},\n",
    "                    \"unit\": {\"type\":\"string\", \"enum\":[\"celsius\", \"fahrenheit\"]},\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='cachy-dummy-id', created=1, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=0, function=Function(arguments='{\"location\": \"Reims\"}', name='get_current_weather'), id='toolu_0182nVBg1pTYTadKxS5qgCt4', type='function')], function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=57, prompt_tokens=427, total_tokens=484, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = completion(model=mods.ant, messages=mk_msgs(\"Is it raining in Reims?\"), tools=tools)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='cachy-dummy-id', created=1, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=0, function=Function(arguments='{\"location\": \"Reims\"}', name='get_current_weather'), id='toolu_0182nVBg1pTYTadKxS5qgCt4', type='function')], function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=57, prompt_tokens=427, total_tokens=484, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = completion(model=mods.ant, messages=mk_msgs(\"Is it raining in Reims?\"), tools=tools)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
