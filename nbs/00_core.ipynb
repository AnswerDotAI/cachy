{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f374771",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Cache your API calls with a single line of code. No mocks, no fixtures. Just faster, cleaner code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651805ab",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2477e8",
   "metadata": {},
   "source": [
    "We often call APIs while prototyping and testing our code. A single API call (e.g. an Anthropic chat completion) can take 100's of ms to run. This can really slow down development especially if our notebook contains many API calls ðŸ˜ž."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ba8e0f",
   "metadata": {},
   "source": [
    "`cachy` caches API requests. It does this by saving the result of each API call to a local `cachy.jsonl` file. Before calling an API (e.g. OpenAI) it will check if the request already exists in `cachy.jsonl`. If it does it will return the cached result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153506a8",
   "metadata": {},
   "source": [
    "**How does it work?**\n",
    "\n",
    "Under the hood popular SDK's like OpenAI, Anthropic and LiteLLM use `httpx.Client` and `httpx.AsyncClient`. \n",
    "\n",
    "`cachy` patches the `send` method of both clients and injects a simple caching mechanism:\n",
    "\n",
    "- create a cache key from the request\n",
    "- if the key exists in `cachy.jsonl` return the cached response\n",
    "- if not, call the API and save the response to `cachy.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2340645",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805144e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa68c98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import hashlib,httpx,json\n",
    "from fastcore.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290e5f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from httpx import RequestNotRead\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c538f015",
   "metadata": {},
   "source": [
    "`cachy.jsonl` contains one API response per line. \n",
    "\n",
    "Each line has the following format `{\"key\": key, \"response\": response}` \n",
    "\n",
    "- `key`: hash of the API request\n",
    "- `response`: the API response. \n",
    "\n",
    "```json\n",
    "{\n",
    "    \"key\": \"afc2be0c\", \n",
    "    \"response\": \"{\\\"id\\\":\\\"msg_xxx\\\",\\\"type\\\":\\\"message\\\",\\\"role\\\":\\\"assistant\\\",\\\"model\\\":\\\"claude-sonnet-4-20250514\\\",\\\"content\\\":[{\\\"type\\\":\\\"text\\\",\\\"text\\\":\\\"Coordination.\\\"}],\\\"stop_reason\\\":\\\"end_turn\\\",\\\"stop_sequence\\\":null,\\\"usage\\\":{\\\"input_tokens\\\":16,\\\"cache_creation_input_tokens\\\":0,\\\"cache_read_input_tokens\\\":0,\\\"cache_creation\\\":{\\\"ephemeral_5m_input_tokens\\\":0,\\\"ephemeral_1h_input_tokens\\\":0},\\\"output_tokens\\\":6,\\\"service_tier\\\":\\\"standard\\\"}}\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af73959f",
   "metadata": {},
   "source": [
    "### Patching `httpx`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc52773",
   "metadata": {},
   "source": [
    "Patching a method is very straightforward. \n",
    "\n",
    "In our case we want to patch `httpx._client.Client.send` and `httpx._client.AsyncClient.send`. \n",
    "\n",
    "These methods are called when running `httpx.get`, `httpx.post`, etc. \n",
    "\n",
    "In the example below we use `@patch` from [fastcore](https://fastcore.fast.ai/) to print `calling an API` when `httpx._client.Client.send` is run.\n",
    "\n",
    "```python\n",
    "@patch\n",
    "def send(self:httpx._client.Client, r, **kwargs):\n",
    "    print('calling an API')\n",
    "    return self._orig_send(r, **kwargs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24733a7f",
   "metadata": {},
   "source": [
    "### Cache Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5876c71",
   "metadata": {},
   "source": [
    "Now, let's build up our caching logic piece-by-piece.\n",
    "\n",
    "The first thing we need to do is ensure that our caching logic only runs on specific urls.\n",
    "\n",
    "For now, let's only cache API calls made to popular LLM providers like OpenAI, Anthropic, Google and DeepSeek. We can make this fully customizable later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527e76df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "doms = (\"api.openai.com\", \"api.anthropic.com\", \"generativelanguage.googleapis.com\", \"api.deepseek.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09571365",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def _should_cache(url, doms): return any(dom in str(url) for dom in doms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6d3122",
   "metadata": {},
   "source": [
    "We could then use `_should_cache` like this.\n",
    "\n",
    "```python\n",
    "@patch\n",
    "def send(self:httpx._client.Client, r, **kwargs):\n",
    "    if not _should_cache(r.url, doms): return self._orig_send(r, **kwargs)\n",
    "    # insert caching logic\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6308294d",
   "metadata": {},
   "source": [
    "### Cache Key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8580082d",
   "metadata": {},
   "source": [
    "The next thing we need to do is figure out if a response for the request `r` already exists in our cache. \n",
    "\n",
    "Recall that each line in `cachy.jsonl` has the following format `{\"key\": key, \"response\": response}`.\n",
    "\n",
    "Our key needs to be unique and deterministic. One way to do this is to concatenate the request URL and content, then generate a hash from the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c091e1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _key(r): return hashlib.sha256(str(r.url.copy_remove_param('key')).encode() + r.content).hexdigest()[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5d99cf",
   "metadata": {},
   "source": [
    "When LiteLLM calls Gemini it includes the API key in a query param so that's why we strip the `key` param from the url."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba8f701",
   "metadata": {},
   "source": [
    "Let's test this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a68a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Request('POST', 'https://api.openai.com/v1/chat/completions')>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = httpx.Request('POST', 'https://api.openai.com/v1/chat/completions', content=b'some content')\n",
    "r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7b3113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2d135d43'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_key(r1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cbb8f3",
   "metadata": {},
   "source": [
    "If we run it again we should get the same key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b819620d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2d135d43'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_key(r1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e565db6",
   "metadata": {},
   "source": [
    "Let's modify the url and confirm we get a different key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007d9208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8a99b0a9'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_key(httpx.Request('POST', 'https://api.anthropic.com/v1/messages', content=b'some content'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcc7aa1",
   "metadata": {},
   "source": [
    "Great. Let's update our patch.\n",
    "\n",
    "```python\n",
    "@patch\n",
    "def send(self:httpx._client.Client, r, **kwargs):\n",
    "    if not _should_cache(r.url, doms): return self._orig_send(r, **kwargs)\n",
    "    key = _key(r)\n",
    "    # if cache hit return the response\n",
    "    # else run the request, write to response the cache and return it\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8107005e",
   "metadata": {},
   "source": [
    "### Cache Reads/Writes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acd1891",
   "metadata": {},
   "source": [
    "Now let's add some methods that will read from and write to `cachy.jsonl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6924d508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def _cache(key, cfp):\n",
    "    with open(cfp, \"r\") as f:\n",
    "        line = first(f, lambda l: json.loads(l)[\"key\"] == key)\n",
    "        return json.loads(line) if line else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a0213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def _write_cache(key, content, cfp, hdrs):\n",
    "    with open(cfp, \"a\") as f: f.write(json.dumps({\"key\":key, \"response\": content, \"headers\":hdrs})+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62486192",
   "metadata": {},
   "source": [
    "Let's update our `patch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d802ec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def send(self:httpx._client.Client, r, **kwargs):\n",
    "    if not _should_cache(r.url, doms): return self._orig_send(r, **kwargs)\n",
    "    key = key(r)\n",
    "    if res := _cache(key,\"cachy.jsonl\"): return httpx.Response(status_code=200, content=res, request=r)\n",
    "    res = self._orig_send(r, **kwargs)\n",
    "    content = res.read().decode()\n",
    "    _write_cache(key, content, \"cachy.jsonl\")\n",
    "    return httpx.Response(status_code=res.status_code, content=content, request=r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c2527e",
   "metadata": {},
   "source": [
    "### Multipart Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acee46f2",
   "metadata": {},
   "source": [
    "`_key` will throw the following error for multipart requests (e.g. file uploads).\n",
    "\n",
    "`RequestNotRead: Attempted to access streaming request content, without having called `read()`.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8690f59d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Request('POST', 'https://api.openai.com/v1/chat/completions')>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfu = httpx.Request('POST', 'https://api.openai.com/v1/chat/completions', files={\"file\": (\"test.txt\", b\"hello\")})\n",
    "rfu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df96e1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fail(lambda: _key(rfu), RequestNotRead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0512600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfu.read(); _key(rfu);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48414f2a",
   "metadata": {},
   "source": [
    "Each part of a multipart request is separated by a delimiter called a boundary with this structure `--b{RANDOM_ID}`. Here's an example for `rfu`.\n",
    "\n",
    "```txt\n",
    "b'--f9ee33966b45cc8c80952bb57cc728c4\\r\\nContent-Disposition: form-data; name=\"file\"; filename=\"test.txt\"\\r\\nContent-Type: text/plain\\r\\n\\r\\nhello\\r\\n--f9ee33966b45cc8c80952bb57cc728c4--\\r\\n'\n",
    "```\n",
    "\n",
    "As the boundary is a random id, two identical multipart requests will produce different boundaries. As the boundary is part of the request content, `_key` will generate different keys leading to cache misses ðŸ˜ž.\n",
    "\n",
    "Let's create a helper method `_content` that will extract content from any request and remove the non-deterministic boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7fca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _content(r):\n",
    "    \"Extract content from request.\"\n",
    "    if not hasattr(r, '_content'): r.read()\n",
    "    boundary = httpx._multipart.get_multipart_boundary_from_content_type(r.headers.get(\"Content-Type\", \"\").encode())\n",
    "    return r.content.replace(boundary, b\"cachy-boundary\") if boundary else r.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0533715e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Request('POST', 'https://api.openai.com/v1/chat/completions')>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfu = httpx.Request('POST', 'https://api.openai.com/v1/chat/completions', files={\"file\": (\"test.txt\", b\"hello\")})\n",
    "rfu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ee5d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'--cachy-boundary\\r\\nContent-Disposition: form-data; name=\"file\"; filename=\"test.txt\"\\r\\nContent-Type: text/plain\\r\\n\\r\\nhello\\r\\n--cachy-boundary--\\r\\n'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_content(rfu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f886766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _key(r): return hashlib.sha256(str(r.url.copy_remove_param('key')).encode() + _content(r)).hexdigest()[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd88bc2",
   "metadata": {},
   "source": [
    "Let's confirm that running `_key` multiple times on the same multipart request now returns the same key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c446f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('9ae79ac5', '9ae79ac5')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_key(rfu), _key(rfu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f447ed40",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfae8317",
   "metadata": {},
   "source": [
    "Let's add support for streaming. \n",
    "\n",
    "First let's include an `is_stream` bool in our hash so that a non-streamed request will generate a different key to the same request when streamed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69ba22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def _key(r, is_stream=False):\n",
    "    \"Create a unique, deterministic id from the request `r`.\"\n",
    "    return hashlib.sha256(f\"{r.url.copy_remove_param('key')}{is_stream}\".encode() + _content(r)).hexdigest()[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeced1a2",
   "metadata": {},
   "source": [
    "In the `patch` we need to `consume` the entire stream before writing it to the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aa118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def send(self:httpx._client.Client, r, **kwargs):\n",
    "    is_stream = kwargs.get(\"stream\")\n",
    "    if not _should_cache(r.url, doms): return self._orig_send(r, **kwargs)\n",
    "    key = _key(r, is_stream=False)\n",
    "    if res := _cache(key,\"cachy.jsonl\"): return httpx.Response(status_code=200, content=res, request=r)\n",
    "    res = self._orig_send(r, **kwargs)\n",
    "    content = res.read().decode() if not is_stream else b''.join(list(res.iter_bytes())).decode()\n",
    "    _write_cache(key, content, \"cachy.jsonl\")\n",
    "    return httpx.Response(status_code=res.status_code, content=content, request=r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d748c8d",
   "metadata": {},
   "source": [
    "### `enable_cachy` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9013bea",
   "metadata": {},
   "source": [
    "To make `cachy` as user friendly as possible let's make it so that we can apply our patch by running a single method at the top of our notebook.\n",
    "\n",
    "```python\n",
    "from cachy import enable_cachy\n",
    "\n",
    "enable_cachy()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468e9956",
   "metadata": {},
   "source": [
    "For this to work we'll need to wrap our patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056e9c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_patch():    \n",
    "    @patch    \n",
    "    def send(self:httpx._client.Client, r, **kwargs):\n",
    "        is_stream = kwargs.get(\"stream\")\n",
    "        if not _should_cache(r.url, doms): return self._orig_send(r, **kwargs)\n",
    "        key = _key(r, is_stream=False)\n",
    "        if res := _cache(key,\"cachy.jsonl\"): return httpx.Response(status_code=200, content=res, request=r)\n",
    "        res = self._orig_send(r, **kwargs)\n",
    "        content = res.read().decode() if not is_stream else b''.join(list(res.iter_bytes())).decode()\n",
    "        _write_cache(key, content, \"cachy.jsonl\")\n",
    "        return httpx.Response(status_code=res.status_code, content=content, request=r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec81b076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enable_cachy():  \n",
    "    _apply_patch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b514b4",
   "metadata": {},
   "source": [
    "Great. Now, let's make `cachy` a little more customizable by making it possible to specify:\n",
    "\n",
    "- the APIs (or domains) to cache\n",
    "- the location of the cache file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b7f6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enable_cachy(cache_dir=None, doms=doms):\n",
    "    cfp = Path(cache_dir or find_file_parents('pyproject.toml')) / \"cachy.jsonl\"\n",
    "    cfp.touch(exist_ok=True)   \n",
    "    _apply_patch(cfp, doms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a2bd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_patch(cfp, doms):    \n",
    "    @patch\n",
    "    def send(self:httpx._client.Client, r, **kwargs):\n",
    "        is_stream = kwargs.get(\"stream\")\n",
    "        if not _should_cache(r.url, doms): return self._orig_send(r, **kwargs)\n",
    "        key = _key(r, is_stream=False)\n",
    "        if res := _cache(key,cfp): return httpx.Response(status_code=200, content=res, request=r)\n",
    "        res = self._orig_send(r, **kwargs)\n",
    "        content = res.read().decode() if not is_stream else b''.join(list(res.iter_bytes())).decode()\n",
    "        _write_cache(key,content,cfp)\n",
    "        return httpx.Response(status_code=res.status_code, content=content, request=r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707f28fb",
   "metadata": {},
   "source": [
    "### Async"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195e5cbf",
   "metadata": {},
   "source": [
    "Some APIs such as Gemini Files API rely on response headers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ced1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _res_hdrs(res, hdrs=None): return {k: v for k, v in res.headers.items() if k.lower() in hdrs} if hdrs else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71262b62",
   "metadata": {},
   "source": [
    "Now let's add support for `async` requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78203d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def _apply_async_patch(cfp, doms, hdrs):    \n",
    "    @patch\n",
    "    async def send(self:httpx._client.AsyncClient, r, **kwargs):\n",
    "        is_stream = kwargs.get(\"stream\")\n",
    "        if not _should_cache(r.url, doms): return await self._orig_send(r, **kwargs)\n",
    "        key = _key(r, is_stream=False)\n",
    "        if res := _cache(key,cfp): return httpx.Response(status_code=200, content=res['response'], headers=res.get('headers'), request=r)\n",
    "        res = await self._orig_send(r, **kwargs)\n",
    "        content = res.read().decode() if not is_stream else b''.join([c async for c in res.aiter_bytes()]).decode()\n",
    "        headers = _res_hdrs(res, hdrs)\n",
    "        _write_cache(key,content,cfp,headers)\n",
    "        return httpx.Response(status_code=res.status_code, content=content, headers=headers, request=r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f67f865",
   "metadata": {},
   "source": [
    "Let's rename our original patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ee9a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def _apply_sync_patch(cfp, doms, hdrs):    \n",
    "    @patch\n",
    "    def send(self:httpx._client.Client, r, **kwargs):\n",
    "        is_stream = kwargs.get(\"stream\")\n",
    "        if not _should_cache(r.url, doms): return self._orig_send(r, **kwargs)\n",
    "        key = _key(r, is_stream=False)\n",
    "        if res := _cache(key,cfp): return httpx.Response(status_code=200, content=res['response'], headers=res.get('headers'), request=r)\n",
    "        res = self._orig_send(r, **kwargs)\n",
    "        content = res.read().decode() if not is_stream else b''.join(list(res.iter_bytes())).decode()\n",
    "        headers = _res_hdrs(res, hdrs)\n",
    "        _write_cache(key,content,cfp,headers)\n",
    "        return httpx.Response(status_code=res.status_code, content=content, headers=headers, request=r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a642c528",
   "metadata": {},
   "source": [
    "Finally, let's update `enable_cachy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05ad1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def enable_cachy(cache_dir=None, doms=doms, hdrs=None):\n",
    "    if hdrs is None: hdrs=[]\n",
    "    cfp = Path(cache_dir or find_file_parents('pyproject.toml')) / \"cachy.jsonl\"\n",
    "    cfp.touch(exist_ok=True)   \n",
    "    _apply_sync_patch(cfp, doms, hdrs)\n",
    "    _apply_async_patch(cfp, doms, hdrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed290a0",
   "metadata": {},
   "source": [
    "And a way to turn if off:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c960325",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def disable_cachy():\n",
    "    httpx._client.AsyncClient.send = httpx._client.AsyncClient._orig_send\n",
    "    httpx._client.Client.send      = httpx._client.Client._orig_send"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4bced9",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331c8690",
   "metadata": {},
   "source": [
    "Let's test `enable_cachy` on 3 SDKs (OpenAI, Anthropic, LiteLLM) for the scenarios below:\n",
    "\n",
    "- sync requests with(out) streaming\n",
    "- async requests with(out) streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bf1d9a",
   "metadata": {},
   "source": [
    "Add some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6bda2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mods: ant=\"claude-sonnet-4-20250514\"; oai=\"gpt-4o\"; gem=\"gemini/gemini-2.0-flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2624539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_msgs(m): return [{\"role\": \"user\", \"content\": f\"write 1 word about {m}\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb679457",
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_cachy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d85bd2",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ed1055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1bf759",
   "metadata": {},
   "outputs": [],
   "source": [
    "cli = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d922cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(id='resp_017850b8b871e44100692ede6ef6c081a085e7e17b2c19943b', created_at=1764679278.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[ResponseOutputMessage(id='msg_017850b8b871e44100692ede6f9b7081a0a6e61d0ebd2c78da', content=[ResponseOutputText(annotations=[], text='Collaboration', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, completed_at=None, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=15, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=3, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=18), user=None, billing={'payer': 'developer'}, store=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = cli.responses.create(model=mods.oai, input=mk_msgs(\"openai sync\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d29424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(id='resp_017850b8b871e44100692ede6ef6c081a085e7e17b2c19943b', created_at=1764679278.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[ResponseOutputMessage(id='msg_017850b8b871e44100692ede6f9b7081a0a6e61d0ebd2c78da', content=[ResponseOutputText(annotations=[], text='Collaboration', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, completed_at=None, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=15, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=3, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=18), user=None, billing={'payer': 'developer'}, store=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = cli.responses.create(model=mods.oai, input=mk_msgs(\"openai sync\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f362dd",
   "metadata": {},
   "source": [
    "Let's test streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da61876b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResponseCreatedEvent(response=Response(id='resp_05244a0e69c5\n",
      "ResponseInProgressEvent(response=Response(id='resp_05244a0e6\n",
      "ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='\n",
      "ResponseContentPartAddedEvent(content_index=0, item_id='msg_\n",
      "ResponseTextDeltaEvent(content_index=0, delta='Innov', item_\n",
      "ResponseTextDeltaEvent(content_index=0, delta='ative', item_\n",
      "ResponseTextDoneEvent(content_index=0, item_id='msg_05244a0e\n",
      "ResponseContentPartDoneEvent(content_index=0, item_id='msg_0\n",
      "ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='m\n",
      "ResponseCompletedEvent(response=Response(id='resp_05244a0e69\n"
     ]
    }
   ],
   "source": [
    "r = cli.responses.create(model=mods.oai, input=mk_msgs(\"openai sync streaming\"), stream=True)\n",
    "for ch in r: print(str(ch)[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db516546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResponseCreatedEvent(response=Response(id='resp_05244a0e69c5\n",
      "ResponseInProgressEvent(response=Response(id='resp_05244a0e6\n",
      "ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='\n",
      "ResponseContentPartAddedEvent(content_index=0, item_id='msg_\n",
      "ResponseTextDeltaEvent(content_index=0, delta='Innov', item_\n",
      "ResponseTextDeltaEvent(content_index=0, delta='ative', item_\n",
      "ResponseTextDoneEvent(content_index=0, item_id='msg_05244a0e\n",
      "ResponseContentPartDoneEvent(content_index=0, item_id='msg_0\n",
      "ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='m\n",
      "ResponseCompletedEvent(response=Response(id='resp_05244a0e69\n"
     ]
    }
   ],
   "source": [
    "r = cli.responses.create(model=mods.oai, input=mk_msgs(\"openai sync streaming\"), stream=True)\n",
    "for ch in r: print(str(ch)[:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b9ecf9",
   "metadata": {},
   "source": [
    "Let's test async."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595c658e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359b75e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cli = AsyncOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4814a1bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(id='resp_0efa725f1445a96c00692ede73db0481a094ddddaab4135e1f', created_at=1764679283.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[ResponseOutputMessage(id='msg_0efa725f1445a96c00692ede74206881a0bd7959dfa7047a88', content=[ResponseOutputText(annotations=[], text='Innovative', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, completed_at=None, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=15, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=3, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=18), user=None, billing={'payer': 'developer'}, store=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await cli.responses.create(model=mods.oai, input=mk_msgs(\"openai async\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8119cb80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(id='resp_0efa725f1445a96c00692ede73db0481a094ddddaab4135e1f', created_at=1764679283.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[ResponseOutputMessage(id='msg_0efa725f1445a96c00692ede74206881a0bd7959dfa7047a88', content=[ResponseOutputText(annotations=[], text='Innovative', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, completed_at=None, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=15, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=3, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=18), user=None, billing={'payer': 'developer'}, store=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await cli.responses.create(model=mods.oai, input=mk_msgs(\"openai async\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493117f7",
   "metadata": {},
   "source": [
    "Let's test async streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152c9702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResponseCreatedEvent(response=Response(id='resp_05d129295d21\n",
      "ResponseInProgressEvent(response=Response(id='resp_05d129295\n",
      "ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='\n",
      "ResponseContentPartAddedEvent(content_index=0, item_id='msg_\n",
      "ResponseTextDeltaEvent(content_index=0, delta='Eff', item_id\n",
      "ResponseTextDeltaEvent(content_index=0, delta='icient', item\n",
      "ResponseTextDoneEvent(content_index=0, item_id='msg_05d12929\n",
      "ResponseContentPartDoneEvent(content_index=0, item_id='msg_0\n",
      "ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='m\n",
      "ResponseCompletedEvent(response=Response(id='resp_05d129295d\n"
     ]
    }
   ],
   "source": [
    "r = await cli.responses.create(model=mods.oai, input=mk_msgs(\"openai async streaming\"), stream=True)\n",
    "async for ch in r: print(str(ch)[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33c9631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResponseCreatedEvent(response=Response(id='resp_05d129295d21\n",
      "ResponseInProgressEvent(response=Response(id='resp_05d129295\n",
      "ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='\n",
      "ResponseContentPartAddedEvent(content_index=0, item_id='msg_\n",
      "ResponseTextDeltaEvent(content_index=0, delta='Eff', item_id\n",
      "ResponseTextDeltaEvent(content_index=0, delta='icient', item\n",
      "ResponseTextDoneEvent(content_index=0, item_id='msg_05d12929\n",
      "ResponseContentPartDoneEvent(content_index=0, item_id='msg_0\n",
      "ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='m\n",
      "ResponseCompletedEvent(response=Response(id='resp_05d129295d\n"
     ]
    }
   ],
   "source": [
    "r = await cli.responses.create(model=mods.oai, input=mk_msgs(\"openai async streaming\"), stream=True)\n",
    "async for ch in r: print(str(ch)[:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd629f87",
   "metadata": {},
   "source": [
    "### Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdea6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c0bd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "cli = Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411ff629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Coordination\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `msg_01CfjNMMiotXXVesJp8bhZy2`\n",
       "- content: `[{'citations': None, 'text': 'Coordination', 'type': 'text'}]`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- role: `assistant`\n",
       "- stop_reason: `end_turn`\n",
       "- stop_sequence: `None`\n",
       "- type: `message`\n",
       "- usage: `{'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 15, 'output_tokens': 5, 'server_tool_use': None, 'service_tier': 'standard'}`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Message(id='msg_01CfjNMMiotXXVesJp8bhZy2', content=[TextBlock(citations=None, text='Coordination', type='text')], model='claude-sonnet-4-20250514', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=In: 15; Out: 5; Cache create: 0; Cache read: 0; Total Tokens: 20; Search: 0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs(\"ant sync\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a245d059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Coordination\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `msg_01CfjNMMiotXXVesJp8bhZy2`\n",
       "- content: `[{'citations': None, 'text': 'Coordination', 'type': 'text'}]`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- role: `assistant`\n",
       "- stop_reason: `end_turn`\n",
       "- stop_sequence: `None`\n",
       "- type: `message`\n",
       "- usage: `{'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 15, 'output_tokens': 5, 'server_tool_use': None, 'service_tier': 'standard'}`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Message(id='msg_01CfjNMMiotXXVesJp8bhZy2', content=[TextBlock(citations=None, text='Coordination', type='text')], model='claude-sonnet-4-20250514', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=In: 15; Out: 5; Cache create: 0; Cache read: 0; Total Tokens: 20; Search: 0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs(\"ant sync\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fbc541",
   "metadata": {},
   "source": [
    "Let's test streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba0ad68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RawMessageStartEvent(message=Message(id='msg_015x4UT4k9GhMN47kCy1ctrt', content=[], model='claude-sonnet-4-20250514', role='assistant', stop_reason=None, stop_sequence=None, type='message', usage=In: 16; Out: 2; Cache create: 0; Cache read: 0; Total Tokens: 18; Search: 0), type='message_start')\n",
      "RawContentBlockStartEvent(content_block=TextBlock(citations=None, text='', type='text'), index=0, type='content_block_start')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text='Buff', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text='ering', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockStopEvent(index=0, type='content_block_stop')\n",
      "RawMessageDeltaEvent(delta=Delta(stop_reason='end_turn', stop_sequence=None), type='message_delta', usage=MessageDeltaUsage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=16, output_tokens=6, server_tool_use=None))\n",
      "RawMessageStopEvent(type='message_stop')\n"
     ]
    }
   ],
   "source": [
    "r = cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs(\"ant sync streaming\"), stream=True)\n",
    "for ch in r: print(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0e2ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RawMessageStartEvent(message=Message(id='msg_015x4UT4k9GhMN47kCy1ctrt', content=[], model='claude-sonnet-4-20250514', role='assistant', stop_reason=None, stop_sequence=None, type='message', usage=In: 16; Out: 2; Cache create: 0; Cache read: 0; Total Tokens: 18; Search: 0), type='message_start')\n",
      "RawContentBlockStartEvent(content_block=TextBlock(citations=None, text='', type='text'), index=0, type='content_block_start')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text='Buff', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text='ering', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockStopEvent(index=0, type='content_block_stop')\n",
      "RawMessageDeltaEvent(delta=Delta(stop_reason='end_turn', stop_sequence=None), type='message_delta', usage=MessageDeltaUsage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=16, output_tokens=6, server_tool_use=None))\n",
      "RawMessageStopEvent(type='message_stop')\n"
     ]
    }
   ],
   "source": [
    "r = cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs(\"ant sync streaming\"), stream=True)\n",
    "for ch in r: print(ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8564692",
   "metadata": {},
   "source": [
    "Let's test async."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cfd94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import AsyncAnthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591e9426",
   "metadata": {},
   "outputs": [],
   "source": [
    "cli = AsyncAnthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657bc916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Concurrency**\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `msg_01NMWNEQCiGWepH1g4eB2yDb`\n",
       "- content: `[{'citations': None, 'text': '**Concurrency**', 'type': 'text'}]`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- role: `assistant`\n",
       "- stop_reason: `end_turn`\n",
       "- stop_sequence: `None`\n",
       "- type: `message`\n",
       "- usage: `{'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 15, 'output_tokens': 8, 'server_tool_use': None, 'service_tier': 'standard'}`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Message(id='msg_01NMWNEQCiGWepH1g4eB2yDb', content=[TextBlock(citations=None, text='**Concurrency**', type='text')], model='claude-sonnet-4-20250514', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=In: 15; Out: 8; Cache create: 0; Cache read: 0; Total Tokens: 23; Search: 0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs(\"ant async\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0e58d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Concurrency**\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `msg_01NMWNEQCiGWepH1g4eB2yDb`\n",
       "- content: `[{'citations': None, 'text': '**Concurrency**', 'type': 'text'}]`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- role: `assistant`\n",
       "- stop_reason: `end_turn`\n",
       "- stop_sequence: `None`\n",
       "- type: `message`\n",
       "- usage: `{'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 15, 'output_tokens': 8, 'server_tool_use': None, 'service_tier': 'standard'}`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Message(id='msg_01NMWNEQCiGWepH1g4eB2yDb', content=[TextBlock(citations=None, text='**Concurrency**', type='text')], model='claude-sonnet-4-20250514', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=In: 15; Out: 8; Cache create: 0; Cache read: 0; Total Tokens: 23; Search: 0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs(\"ant async\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4495b439",
   "metadata": {},
   "source": [
    "Let's test async streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b42c9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event: message_start\n",
      "data: {\"type\":\"message_start\",\"message\":{\"model\":\"claude-sonnet-4-20250514\",\"id\":\"msg_01HSogveNre4UeiteLGqiZkt\",\"type\":\"message\",\"role\":\"assistant\",\"content\":[],\"stop_reason\":null,\"stop_sequence\":null,\"usage\":{\"input_tokens\":16,\"cache_creation_input_tokens\":0,\"cache_read_input_tokens\":0,\"cache_creation\":{\"ephemeral_5m_input_tokens\":0,\"ephemeral_1h_input_tokens\":0},\"output_tokens\":1,\"service_tier\":\"standard\"}}            }\n",
      "\n",
      "event: content_block_start\n",
      "data: {\"type\":\"content_block_start\",\"index\":0,\"content_block\":{\"type\":\"text\",\"text\":\"\"}         }\n",
      "\n",
      "event: content_block_delta\n",
      "data: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"text_delta\",\"text\":\"**\"}               }\n",
      "\n",
      "event: content_block_delta\n",
      "data: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"text_delta\",\"text\":\"Concurrent\"}             }\n",
      "\n",
      "event: ping\n",
      "data: {\"type\": \"ping\"}\n",
      "\n",
      "event: content_block_delta\n",
      "data: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"text_delta\",\"text\":\"**\"}    }\n",
      "\n",
      "event: content_block_stop\n",
      "data: {\"type\":\"content_block_stop\",\"index\":0   }\n",
      "\n",
      "event: message_delta\n",
      "data: {\"type\":\"message_delta\",\"delta\":{\"stop_reason\":\"end_turn\",\"stop_sequence\":null},\"usage\":{\"input_tokens\":16,\"cache_creation_input_tokens\":0,\"cache_read_input_tokens\":0,\"output_tokens\":7}  }\n",
      "\n",
      "event: message_stop\n",
      "data: {\"type\":\"message_stop\"     }\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = await cli.messages.create(model=mods.ant,max_tokens=1024,messages=mk_msgs(\"ant async streaming\"), stream=True)\n",
    "async for ch in r.response.aiter_bytes(): print(ch.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a4b1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event: message_start\n",
      "data: {\"type\":\"message_start\",\"message\":{\"model\":\"claude-sonnet-4-20250514\",\"id\":\"msg_01HSogveNre4UeiteLGqiZkt\",\"type\":\"message\",\"role\":\"assistant\",\"content\":[],\"stop_reason\":null,\"stop_sequence\":null,\"usage\":{\"input_tokens\":16,\"cache_creation_input_tokens\":0,\"cache_read_input_tokens\":0,\"cache_creation\":{\"ephemeral_5m_input_tokens\":0,\"ephemeral_1h_input_tokens\":0},\"output_tokens\":1,\"service_tier\":\"standard\"}}            }\n",
      "\n",
      "event: content_block_start\n",
      "data: {\"type\":\"content_block_start\",\"index\":0,\"content_block\":{\"type\":\"text\",\"text\":\"\"}         }\n",
      "\n",
      "event: content_block_delta\n",
      "data: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"text_delta\",\"text\":\"**\"}               }\n",
      "\n",
      "event: content_block_delta\n",
      "data: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"text_delta\",\"text\":\"Concurrent\"}             }\n",
      "\n",
      "event: ping\n",
      "data: {\"type\": \"ping\"}\n",
      "\n",
      "event: content_block_delta\n",
      "data: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"text_delta\",\"text\":\"**\"}    }\n",
      "\n",
      "event: content_block_stop\n",
      "data: {\"type\":\"content_block_stop\",\"index\":0   }\n",
      "\n",
      "event: message_delta\n",
      "data: {\"type\":\"message_delta\",\"delta\":{\"stop_reason\":\"end_turn\",\"stop_sequence\":null},\"usage\":{\"input_tokens\":16,\"cache_creation_input_tokens\":0,\"cache_read_input_tokens\":0,\"output_tokens\":7}  }\n",
      "\n",
      "event: message_stop\n",
      "data: {\"type\":\"message_stop\"     }\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = await cli.messages.create(model=mods.ant,max_tokens=1024,messages=mk_msgs(\"ant async streaming\"), stream=True)\n",
    "async for ch in r.response.aiter_bytes(): print(ch.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9005138e",
   "metadata": {},
   "source": [
    "### LiteLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e918f9",
   "metadata": {},
   "source": [
    "Let's test the LiteLLM SDK by running sync/async calls with(out) streaming for OpenAI, Anthropic, & Gemini.\n",
    "\n",
    "We'll also double check tool calls and citations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84de3502",
   "metadata": {},
   "source": [
    "#### Sync Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d410d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac22f2de",
   "metadata": {},
   "source": [
    "Let's define a helper method to display a streamed response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3b6a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _stream(r): \n",
    "    for ch in r: print(ch.choices[0].delta.content or \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af15cf6a",
   "metadata": {},
   "source": [
    "##### Anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aecb14",
   "metadata": {},
   "source": [
    "Let's test `claude-sonnet-x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a1dd76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-59ae9055-a8d9-494b-b2a9-4f2642fac172', created=1766866806, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='**partial**', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=6, prompt_tokens=18, total_tokens=24, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = completion(model=mods.ant, messages=mk_msgs(\"lite: ant sync...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a962a63a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-2c056aa5-c452-4cbf-859f-f9082f36ede9', created=1766866806, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='**partial**', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=6, prompt_tokens=18, total_tokens=24, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = completion(model=mods.ant, messages=mk_msgs(\"lite: ant sync...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af92953",
   "metadata": {},
   "source": [
    "Now, with streaming enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd195108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "Efficient\n",
      "**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = completion(model=mods.ant, messages=mk_msgs(\"lite: ant sync stream...\"), stream=True)\n",
    "_stream(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43acec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "Efficient\n",
      "**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = completion(model=mods.ant, messages=mk_msgs(\"lite: ant sync stream...\"), stream=True)\n",
    "_stream(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d755d14d",
   "metadata": {},
   "source": [
    "##### OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69376bcf",
   "metadata": {},
   "source": [
    "Let's test `gpt-4o`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affb84d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-CiJzMl6JYB7siXcKdtd6IzFhjOSSU', created=1764679304, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_e819e3438b', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Connectivity', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=1, prompt_tokens=18, total_tokens=19, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = completion(model=mods.oai, messages=mk_msgs(\"lite: oai sync...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed17001d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-CiJzMl6JYB7siXcKdtd6IzFhjOSSU', created=1764679304, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_e819e3438b', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Connectivity', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=1, prompt_tokens=18, total_tokens=19, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = completion(model=mods.oai, messages=mk_msgs(\"lite: oai sync...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63010c88",
   "metadata": {},
   "source": [
    "Now, with streaming enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe98d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integration\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = completion(model=mods.oai, messages=mk_msgs(\"lite: oai sync stream...\"), stream=True)\n",
    "_stream(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd0c6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integration\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = completion(model=mods.oai, messages=mk_msgs(\"lite: oai sync stream...\"), stream=True)\n",
    "_stream(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaa5d5d",
   "metadata": {},
   "source": [
    "##### Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee787b41",
   "metadata": {},
   "source": [
    "Let's test `2.0-flash`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205eeb18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='it4uaaeWJNKlkdUPv-fwiQg', created=1766866806, model='gemini-2.0-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Efficient.\\n', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=3, prompt_tokens=10, total_tokens=13, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=3, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=10, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = completion(model=mods.gem, messages=mk_msgs(\"lite: gem sync...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f5b245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='it4uaaeWJNKlkdUPv-fwiQg', created=1766866806, model='gemini-2.0-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Efficient.\\n', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=3, prompt_tokens=10, total_tokens=13, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=3, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=10, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = completion(model=mods.gem, messages=mk_msgs(\"lite: gem sync...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b9cb44",
   "metadata": {},
   "source": [
    "Now, with streaming enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a16022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = completion(model=mods.gem, messages=mk_msgs(\"lite: gem sync stream...\"), stream=True)\n",
    "_stream(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd1db86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = completion(model=mods.gem, messages=mk_msgs(\"lite: gem sync stream...\"), stream=True)\n",
    "_stream(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a974e039",
   "metadata": {},
   "source": [
    "#### Async Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9536a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import acompletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d399cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _astream(r):\n",
    "    async for chunk in r: print(chunk.choices[0].delta.content or \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d2b5ce",
   "metadata": {},
   "source": [
    "##### Anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e974c34c",
   "metadata": {},
   "source": [
    "Let's test `claude-sonnet-x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e7f30b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-7e4e7298-4f73-4ef3-968d-fed2e4e498e2', created=1766866807, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='**coroutines**', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=8, prompt_tokens=18, total_tokens=26, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await acompletion(model=mods.ant, messages=mk_msgs(\"lite: ant async...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a80696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-ebf4dc2c-e6e5-4f98-9fbc-0db98cbdd4ce', created=1766866807, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='**coroutines**', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=8, prompt_tokens=18, total_tokens=26, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await acompletion(model=mods.ant, messages=mk_msgs(\"lite: ant async...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f35af59",
   "metadata": {},
   "source": [
    "Now, with streaming enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0b902a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "concurrent\n",
      "**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = await acompletion(model=mods.ant, messages=mk_msgs(\"lite: ant async stream...\"), stream=True)\n",
    "await(_astream(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f402f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "concurrent\n",
      "**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = await acompletion(model=mods.ant, messages=mk_msgs(\"lite: ant async stream...\"), stream=True)\n",
    "await(_astream(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1f6b1a",
   "metadata": {},
   "source": [
    "##### OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5c220e",
   "metadata": {},
   "source": [
    "Let's test `gpt-4o`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0c12ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-CiJzVWXtqcySUdl7b9G7E80vaWjwW', created=1764679313, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_e819e3438b', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Efficient.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=3, prompt_tokens=18, total_tokens=21, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await acompletion(model=mods.oai, messages=mk_msgs(\"lite: oai async...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d56291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-CiJzVWXtqcySUdl7b9G7E80vaWjwW', created=1764679313, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_e819e3438b', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Efficient.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=3, prompt_tokens=18, total_tokens=21, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await acompletion(model=mods.oai, messages=mk_msgs(\"lite: oai async...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5e0831",
   "metadata": {},
   "source": [
    "Now, with streaming enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75887c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Illuminate\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = await acompletion(model=mods.oai, messages=mk_msgs(\"lite: oai async stream...\"), stream=True)\n",
    "await(_astream(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afd4a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Illuminate\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = await acompletion(model=mods.oai, messages=mk_msgs(\"lite: oai async stream...\"), stream=True)\n",
    "await(_astream(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e397a46e",
   "metadata": {},
   "source": [
    "##### Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3136f3",
   "metadata": {},
   "source": [
    "Let's test `2.0-flash`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e703de6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='kt4uaenhMJ66xN8Pgpv6gAE', created=1766866808, model='gemini-2.0-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Concurrency\\n', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=2, prompt_tokens=10, total_tokens=12, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=2, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=10, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await acompletion(model=mods.gem, messages=mk_msgs(\"lite: gem async...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aee1e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='kt4uaenhMJ66xN8Pgpv6gAE', created=1766866808, model='gemini-2.0-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Concurrency\\n', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=2, prompt_tokens=10, total_tokens=12, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=2, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=10, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await acompletion(model=mods.gem, messages=mk_msgs(\"lite: gem async...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf609ca",
   "metadata": {},
   "source": [
    "Now, with streaming enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7958e93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = await acompletion(model=mods.gem, messages=mk_msgs(\"lite: gem async stream...\"), stream=True)\n",
    "await(_astream(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f63db23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = await acompletion(model=mods.gem, messages=mk_msgs(\"lite: gem async stream...\"), stream=True)\n",
    "await(_astream(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99d5ad4",
   "metadata": {},
   "source": [
    "#### Tool Calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691dc4c7",
   "metadata": {},
   "source": [
    "As a sanity check let's confirm that tool calls work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c0a42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\"type\":\"string\", \"description\":\"The city e.g. Reims\"},\n",
    "                    \"unit\": {\"type\":\"string\", \"enum\":[\"celsius\", \"fahrenheit\"]},\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80460dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-ff28c002-46b3-4f69-a569-c67c3a898070', created=1766866808, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=0, function=Function(arguments='{\"location\": \"Reims\"}', name='get_current_weather'), id='toolu_017sHr4VFzg6Nh7jt8wSescj', type='function')], function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=57, prompt_tokens=427, total_tokens=484, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = completion(model=mods.ant, messages=mk_msgs(\"Is it raining in Reims?\"), tools=tools)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94a5792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-dbd216b5-0df7-4b58-abe0-cc524b4fbb92', created=1766866808, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=0, function=Function(arguments='{\"location\": \"Reims\"}', name='get_current_weather'), id='toolu_017sHr4VFzg6Nh7jt8wSescj', type='function')], function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=57, prompt_tokens=427, total_tokens=484, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = completion(model=mods.ant, messages=mk_msgs(\"Is it raining in Reims?\"), tools=tools)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629995d9",
   "metadata": {},
   "source": [
    "### Multipart Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a718ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileMetadata(id='file_011CVhstprQYGfRzyf5RpTWu', created_at=datetime.datetime(2025, 12, 2, 12, 41, 59, 74000, tzinfo=datetime.timezone.utc), filename='ex.txt', mime_type='text/plain', size_bytes=11, type='file', downloadable=False)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cli = Anthropic()\n",
    "r = cli.beta.files.upload(file=(\"ex.txt\", b\"hello world\", \"text/plain\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bea763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileMetadata(id='file_011CVhstprQYGfRzyf5RpTWu', created_at=datetime.datetime(2025, 12, 2, 12, 41, 59, 74000, tzinfo=datetime.timezone.utc), filename='ex.txt', mime_type='text/plain', size_bytes=11, type='file', downloadable=False)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cli = Anthropic()\n",
    "r = cli.beta.files.upload(file=(\"ex.txt\", b\"hello world\", \"text/plain\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9503b81",
   "metadata": {},
   "source": [
    "### Gemini Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fbd97c",
   "metadata": {},
   "source": [
    "When LiteLLM calls Gemini it includes the model name in the url. Let's test that we can run the same prompt with two different Gemini models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839019b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemini/gemini-2.0-flash'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mods.gem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbea562e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='l94uabyuKYKlkdUPh6DpkQg', created=1766866808, model='gemini-2.0-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Streamlined\\n', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=3, prompt_tokens=11, total_tokens=14, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=3, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=11, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = completion(model=mods.gem, messages=mk_msgs(\"lite: gem different models...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e2593d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='nN4uafGuNYGekdUP0cTEkQM', created=1766866808, model='gemini-2.5-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Versatile', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=867, prompt_tokens=12, total_tokens=879, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=865, rejected_prediction_tokens=None, text_tokens=2, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=12, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = completion(model=\"gemini/gemini-2.5-flash\", messages=mk_msgs(\"lite: gem different models...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1338588",
   "metadata": {},
   "source": [
    "### Gemini File Upload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914f9c0d",
   "metadata": {},
   "source": [
    "The `google-genai` SDK's `files.upload()` relies on `x-goog-upload-url` and `x-goog-upload-status` response headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde63027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b5f4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cli = genai.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bf0c3e",
   "metadata": {},
   "source": [
    "When no `hdrs` is provided the request fails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6384e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/keremturgutlu/aai_git/aai_repos_extra/transcriptions/examples/multi_track_audio/Jeremy.mp3 is not a valid file path.\n"
     ]
    }
   ],
   "source": [
    "fn = '/Users/keremturgutlu/aai_git/aai_repos_extra/transcriptions/examples/multi_track_audio/Jeremy.mp3'\n",
    "try: gfile = cli.files.upload(file=fn)\n",
    "except Exception as e: print(e) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d943eb1",
   "metadata": {},
   "source": [
    "Remove the failed cache entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b7c2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfp = Path('../cachy.jsonl')\n",
    "lines = cfp.read_text().splitlines()\n",
    "_ = cfp.write_text('\\n'.join(lines[:-1]) + '\\n' if lines[:-1] else '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebbc164",
   "metadata": {},
   "source": [
    "When caching Gemini file uploads, by default request content only includes `mime_type` and `size_bytes`. This means different files with the same mime type and size produce identical cache keys, causing incorrect cache hits. The fix is to pass a file content fingerprint (a hash of the file bytes) as the `display_name` in the upload config: `cli.files.upload(file=fn, config={\"display_name\": _fingerprint(fn)})`. This ensures the request body is unique per file content, generating distinct cache keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630aaa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fingerprint(path): return hashlib.sha256(Path(path).read_bytes()).hexdigest()[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b475da38",
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_cachy(hdrs=['x-goog-upload-url', 'x-goog-upload-status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae63ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gfile = cli.files.upload(file=fn, config={\"display_name\": _fingerprint(fn)})\n",
    "# gfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2cb334",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044debe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
