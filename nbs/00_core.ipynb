{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Cache your API calls and make your notebooks fast again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often call APIs while prototyping and testing our code. A single API call (e.g. an Anthropic chat completion) can take 100's of ms to run. This can really slow down development especially if our notebook contains many API calls ðŸ˜ž."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cachy` caches API requests. It does this by saving the result of each API call to a local `cachy.jsonl` file. Before calling an API (e.g. OpenAI) it will check if the request already exists in `cachy.jsonl`. If it does it will return the cached result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does it work?**\n",
    "\n",
    "Under the hood popular SDK's like OpenAI, Anthropic and LiteLLM use `httpx.Client` and `httpx.AsyncClient`. \n",
    "\n",
    "`cachy` patches the `send` method of both clients and injects a simple caching mechanism:\n",
    "\n",
    "- create a cache key from the request\n",
    "- if the key exists in `cachy.jsonl` return the cached response\n",
    "- if not, call the API and save the response to `cachy.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import hashlib,httpx,json\n",
    "from fastcore.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cachy.jsonl` contains one API response per line. \n",
    "\n",
    "Each line has the following format `{\"key\": key, \"response\": response}` \n",
    "\n",
    "- `key`: hash of the API request\n",
    "- `response`: the API response. \n",
    "\n",
    "```json\n",
    "{\n",
    "    \"key\": \"afc2be0c\", \n",
    "    \"response\": \"{\\\"id\\\":\\\"msg_xxx\\\",\\\"type\\\":\\\"message\\\",\\\"role\\\":\\\"assistant\\\",\\\"model\\\":\\\"claude-sonnet-4-20250514\\\",\\\"content\\\":[{\\\"type\\\":\\\"text\\\",\\\"text\\\":\\\"Coordination.\\\"}],\\\"stop_reason\\\":\\\"end_turn\\\",\\\"stop_sequence\\\":null,\\\"usage\\\":{\\\"input_tokens\\\":16,\\\"cache_creation_input_tokens\\\":0,\\\"cache_read_input_tokens\\\":0,\\\"cache_creation\\\":{\\\"ephemeral_5m_input_tokens\\\":0,\\\"ephemeral_1h_input_tokens\\\":0},\\\"output_tokens\\\":6,\\\"service_tier\\\":\\\"standard\\\"}}\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patching `httpx`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patching a method is very straightforward. \n",
    "\n",
    "In our case we want to patch `httpx._client.Client.send` and `httpx._client.AsyncClient.send`. \n",
    "\n",
    "These methods are called when running `httpx.get`, `httpx.post`, etc. \n",
    "\n",
    "In the example below we use `@patch` from [fastcore](https://fastcore.fast.ai/) to print `calling an API` when `httpx._client.Client.send` is run.\n",
    "\n",
    "```python\n",
    "@patch\n",
    "def send(self:httpx._client.Client, r, **kwargs):\n",
    "    print('calling an API')\n",
    "    return self._orig_send(r, **kwargs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build up our caching logic piece-by-piece.\n",
    "\n",
    "The first thing we need to do is ensure that our caching logic only runs on specific urls.\n",
    "\n",
    "For now, let's only cache API calls made to popular LLM providers like OpenAI, Anthropic, Google and DeepSeek. We can make this fully customizable later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "doms = (\"api.openai.com\", \"api.anthropic.com\", \"generativelanguage.googleapis.com\", \"api.deepseek.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def _should_cache(url, doms): return any(dom in str(url) for dom in doms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could then use `_should_cache` like this.\n",
    "\n",
    "```python\n",
    "@patch\n",
    "def send(self:httpx._client.Client, r, **kwargs):\n",
    "    if not _should_cache(r.url, doms): return self._orig_send(r, **kwargs)\n",
    "    # insert caching logic\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we need to do is figure out if a response for the request `r` already exists in our cache. \n",
    "\n",
    "Recall that each line in `cachy.jsonl` has the following format `{\"key\": key, \"response\": response}`.\n",
    "\n",
    "Our key needs to be unique and deterministic. One way to do this is to concatenate the request URL and content, then generate a hash from the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _key(r): return hashlib.sha256(str(r.url).encode() + r.content).hexdigest()[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Request('POST', 'https://answer.ai')>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = httpx.Request('POST', 'https://answer.ai', content=b'some content')\n",
    "r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3baa91c4'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_key(r1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run it again we should get the same key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3baa91c4'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_key(r1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify the url and confirm we get a different key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'afee66b2'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_key(httpx.Request('POST', 'https://fast.ai/', content=b'some content'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Let's update our patch.\n",
    "\n",
    "```python\n",
    "@patch\n",
    "def send(self:httpx._client.Client, r, **kwargs):\n",
    "    if not _should_cache(r.url, doms): return self._orig_send(r, **kwargs)\n",
    "    key = _key(r)\n",
    "    # if cache hit return the response\n",
    "    # else run the request, write to response the cache and return it\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache Reads/Writes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add some methods that will read from and write to `cachy.jsonl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def _cache(key, cfp):\n",
    "    with open(cfp, \"r\") as f:\n",
    "        line = first(f, lambda l: json.loads(l)[\"key\"] == key)\n",
    "        return json.loads(line)[\"response\"] if line else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def _write_cache(key, content, cfp):\n",
    "    with open(cfp, \"a\") as f: f.write(json.dumps({\"key\":key, \"response\": content})+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's update our `patch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def send(self:httpx._client.Client, r, **kwargs):\n",
    "    if not _should_cache(r.url, doms): return self._orig_send(r, **kwargs)\n",
    "    key = key(r)\n",
    "    if res := _cache(key,\"cachy.jsonl\"): return httpx.Response(status_code=200, content=res, request=r)\n",
    "    res = self._orig_send(r, **kwargs)\n",
    "    content = res.read().decode()\n",
    "    _write_cache(key, content, \"cachy.jsonl\")\n",
    "    return httpx.Response(status_code=res.status_code, content=content, request=r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add support for streaming. \n",
    "\n",
    "First let's include an `is_stream` bool in our hash so that a non-streamed request will generate a different key to the same request when streamed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def _key(r, is_stream=False):\n",
    "    \"Create a unique, deterministic id from the request `r`.\"\n",
    "    return hashlib.sha256(f\"{r.url}{is_stream}\".encode() + r.content).hexdigest()[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `patch` we need to `consume` the entire stream before writing it to the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def send(self:httpx._client.Client, r, **kwargs):\n",
    "    is_stream = kwargs.get(\"stream\")\n",
    "    if not _should_cache(r.url, doms): return self._orig_send(r, **kwargs)\n",
    "    key = _key(r, is_stream=False)\n",
    "    if res := _cache(key,\"cachy.jsonl\"): return httpx.Response(status_code=200, content=res, request=r)\n",
    "    res = self._orig_send(r, **kwargs)\n",
    "    content = res.read().decode() if not is_stream else b''.join(list(res.iter_bytes())).decode()\n",
    "    _write_cache(key, content, \"cachy.jsonl\")\n",
    "    return httpx.Response(status_code=res.status_code, content=content, request=r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `enable_cachy` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make `cachy` as user friendly as possible let's make it so that we can apply our patch by running a single method at the top of our notebook.\n",
    "\n",
    "```python\n",
    "from cachy import enable_cachy\n",
    "\n",
    "enable_cachy()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this to work we'll need to wrap our patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_patch():    \n",
    "    @patch    \n",
    "    def send(self:httpx._client.Client, r, **kwargs):\n",
    "        is_stream = kwargs.get(\"stream\")\n",
    "        if not _should_cache(r.url, doms): return self._orig_send(r, **kwargs)\n",
    "        key = _key(r, is_stream=False)\n",
    "        if res := _cache(key,\"cachy.jsonl\"): return httpx.Response(status_code=200, content=res, request=r)\n",
    "        res = self._orig_send(r, **kwargs)\n",
    "        content = res.read().decode() if not is_stream else b''.join(list(res.iter_bytes())).decode()\n",
    "        _write_cache(key, content, \"cachy.jsonl\")\n",
    "        return httpx.Response(status_code=res.status_code, content=content, request=r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enable_cachy():  \n",
    "    _apply_patch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Now, let's make `cachy` a little more customizable by making it possible to specify:\n",
    "\n",
    "- the APIs (or domains) to cache\n",
    "- the location of the cache file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enable_cachy(cache_dir=None, doms=doms):\n",
    "    cfp = Path(cache_dir or Config.find(\"settings.ini\").config_path or \".\") / \"cachy.jsonl\"\n",
    "    cfp.touch(exist_ok=True)   \n",
    "    _apply_patch(cfp, doms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: If our notebook is running in an nbdev project `Config.find(\"settings.ini\").config_path` automatically finds the base dir.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_patch(cfp, doms):    \n",
    "    @patch\n",
    "    def send(self:httpx._client.Client, r, **kwargs):\n",
    "        is_stream = kwargs.get(\"stream\")\n",
    "        if not _should_cache(r.url, doms): return self._orig_send(r, **kwargs)\n",
    "        key = _key(r, is_stream=False)\n",
    "        if res := _cache(key,cfp): return httpx.Response(status_code=200, content=res, request=r)\n",
    "        res = self._orig_send(r, **kwargs)\n",
    "        content = res.read().decode() if not is_stream else b''.join(list(res.iter_bytes())).decode()\n",
    "        _write_cache(key,content,cfp)\n",
    "        return httpx.Response(status_code=res.status_code, content=content, request=r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add support for `async` requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def _apply_async_patch(cfp, doms):    \n",
    "    @patch\n",
    "    async def send(self:httpx._client.AsyncClient, r, **kwargs):\n",
    "        is_stream = kwargs.get(\"stream\")\n",
    "        if not _should_cache(r.url, doms): return await self._orig_send(r, **kwargs)\n",
    "        key = _key(r, is_stream=False)\n",
    "        if res := _cache(key, cfp): return httpx.Response(status_code=200, content=res, request=r)\n",
    "        res = await self._orig_send(r, **kwargs)\n",
    "        content = res.read().decode() if not is_stream else b''.join([c async for c in res.aiter_bytes()]).decode()\n",
    "        _write_cache(key, content, cfp)\n",
    "        return httpx.Response(status_code=res.status_code, content=content, request=r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rename our original patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def _apply_sync_patch(cfp, doms):    \n",
    "    @patch\n",
    "    def send(self:httpx._client.Client, r, **kwargs):\n",
    "        is_stream = kwargs.get(\"stream\")\n",
    "        if not _should_cache(r.url, doms): return self._orig_send(r, **kwargs)\n",
    "        key = _key(r, is_stream=False)\n",
    "        if res := _cache(key,cfp): return httpx.Response(status_code=200, content=res, request=r)\n",
    "        res = self._orig_send(r, **kwargs)\n",
    "        content = res.read().decode() if not is_stream else b''.join(list(res.iter_bytes())).decode()\n",
    "        _write_cache(key,content,cfp)\n",
    "        return httpx.Response(status_code=res.status_code, content=content, request=r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's update `enable_cachy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def enable_cachy(cache_dir=None, doms=doms):\n",
    "    cfp = Path(cache_dir or Config.find(\"settings.ini\").config_path or \".\") / \"cachy.jsonl\"\n",
    "    cfp.touch(exist_ok=True)   \n",
    "    _apply_sync_patch(cfp, doms)\n",
    "    _apply_async_patch(cfp, doms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test `enable_cachy` on 3 SDKs (OpenAI, Anthropic, LiteLLM) for the scenarios below:\n",
    "\n",
    "- sync requests with(out) streaming\n",
    "- async requests with(out) streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mods: ant=\"claude-sonnet-4-20250514\"; oai=\"gpt-4o\"; gem=\"gemini/gemini-2.0-flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_msgs(m): return [{\"role\": \"user\", \"content\": f\"write 1 word about {m}\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_cachy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Collaboration\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_68b98c0b9cac819f95bbf6ad7ee7cabd0ae9bf0ceab25340\n",
       "- created_at: 1756990475.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: None\n",
       "- metadata: {}\n",
       "- model: gpt-4o-2024-08-06\n",
       "- object: response\n",
       "- output: [ResponseOutputMessage(id='msg_68b98c0c6fdc819fa02a8a8152719d670ae9bf0ceab25340', content=[ResponseOutputText(annotations=[], text='Collaboration', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: []\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- conversation: None\n",
       "- max_output_tokens: None\n",
       "- max_tool_calls: None\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- prompt_cache_key: None\n",
       "- reasoning: Reasoning(effort=None, generate_summary=None, summary=None)\n",
       "- safety_identifier: None\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium')\n",
       "- top_logprobs: 0\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=15, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=3, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=18)\n",
       "- user: None\n",
       "- store: True\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_68b98c0b9cac819f95bbf6ad7ee7cabd0ae9bf0ceab25340', created_at=1756990475.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[ResponseOutputMessage(id='msg_68b98c0c6fdc819fa02a8a8152719d670ae9bf0ceab25340', content=[ResponseOutputText(annotations=[], text='Collaboration', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=In: 15; Out: 3; Total: 18, user=None, store=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = cli.responses.create(model=mods.oai, input=mk_msgs(\"openai sync\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Collaboration\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_68b98c0b9cac819f95bbf6ad7ee7cabd0ae9bf0ceab25340\n",
       "- created_at: 1756990475.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: None\n",
       "- metadata: {}\n",
       "- model: gpt-4o-2024-08-06\n",
       "- object: response\n",
       "- output: [ResponseOutputMessage(id='msg_68b98c0c6fdc819fa02a8a8152719d670ae9bf0ceab25340', content=[ResponseOutputText(annotations=[], text='Collaboration', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: []\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- conversation: None\n",
       "- max_output_tokens: None\n",
       "- max_tool_calls: None\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- prompt_cache_key: None\n",
       "- reasoning: Reasoning(effort=None, generate_summary=None, summary=None)\n",
       "- safety_identifier: None\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium')\n",
       "- top_logprobs: 0\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=15, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=3, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=18)\n",
       "- user: None\n",
       "- store: True\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_68b98c0b9cac819f95bbf6ad7ee7cabd0ae9bf0ceab25340', created_at=1756990475.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[ResponseOutputMessage(id='msg_68b98c0c6fdc819fa02a8a8152719d670ae9bf0ceab25340', content=[ResponseOutputText(annotations=[], text='Collaboration', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=In: 15; Out: 3; Total: 18, user=None, store=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = cli.responses.create(model=mods.oai, input=mk_msgs(\"openai sync\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResponseCreatedEvent(response=Response(id='resp_68b98c1027808194a1740c52e02008b00b23cab13df8b32a', created_at=1756990480.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=None, user=None, store=True), sequence_number=0, type='response.created')\n",
      "ResponseInProgressEvent(response=Response(id='resp_68b98c1027808194a1740c52e02008b00b23cab13df8b32a', created_at=1756990480.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=None, user=None, store=True), sequence_number=1, type='response.in_progress')\n",
      "ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='msg_68b98c10fa888194ac64d16f139032d20b23cab13df8b32a', content=[], role='assistant', status='in_progress', type='message'), output_index=0, sequence_number=2, type='response.output_item.added')\n",
      "ResponseContentPartAddedEvent(content_index=0, item_id='msg_68b98c10fa888194ac64d16f139032d20b23cab13df8b32a', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text', logprobs=[]), sequence_number=3, type='response.content_part.added')\n",
      "ResponseTextDeltaEvent(content_index=0, delta='Se', item_id='msg_68b98c10fa888194ac64d16f139032d20b23cab13df8b32a', logprobs=[], output_index=0, sequence_number=4, type='response.output_text.delta', obfuscation='gWEwdz1qiFUeEK')\n",
      "ResponseTextDeltaEvent(content_index=0, delta='am', item_id='msg_68b98c10fa888194ac64d16f139032d20b23cab13df8b32a', logprobs=[], output_index=0, sequence_number=5, type='response.output_text.delta', obfuscation='LNxNwezTMNE6J0')\n",
      "ResponseTextDeltaEvent(content_index=0, delta='less', item_id='msg_68b98c10fa888194ac64d16f139032d20b23cab13df8b32a', logprobs=[], output_index=0, sequence_number=6, type='response.output_text.delta', obfuscation='oqZZ8WAnjYJ8')\n",
      "ResponseTextDoneEvent(content_index=0, item_id='msg_68b98c10fa888194ac64d16f139032d20b23cab13df8b32a', logprobs=[], output_index=0, sequence_number=7, text='Seamless', type='response.output_text.done')\n",
      "ResponseContentPartDoneEvent(content_index=0, item_id='msg_68b98c10fa888194ac64d16f139032d20b23cab13df8b32a', output_index=0, part=ResponseOutputText(annotations=[], text='Seamless', type='output_text', logprobs=[]), sequence_number=8, type='response.content_part.done')\n",
      "ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='msg_68b98c10fa888194ac64d16f139032d20b23cab13df8b32a', content=[ResponseOutputText(annotations=[], text='Seamless', type='output_text', logprobs=[])], role='assistant', status='completed', type='message'), output_index=0, sequence_number=9, type='response.output_item.done')\n",
      "ResponseCompletedEvent(response=Response(id='resp_68b98c1027808194a1740c52e02008b00b23cab13df8b32a', created_at=1756990480.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[ResponseOutputMessage(id='msg_68b98c10fa888194ac64d16f139032d20b23cab13df8b32a', content=[ResponseOutputText(annotations=[], text='Seamless', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=In: 16; Out: 4; Total: 20, user=None, store=True), sequence_number=10, type='response.completed')\n"
     ]
    }
   ],
   "source": [
    "r = cli.responses.create(model=mods.oai, input=mk_msgs(\"openai sync streaming\"), stream=True)\n",
    "for ch in r: print(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResponseCreatedEvent(response=Response(id='resp_68b98c1027808194a1740c52e02008b00b23cab13df8b32a', created_at=1756990480.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=None, user=None, store=True), sequence_number=0, type='response.created')\n",
      "ResponseInProgressEvent(response=Response(id='resp_68b98c1027808194a1740c52e02008b00b23cab13df8b32a', created_at=1756990480.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=None, user=None, store=True), sequence_number=1, type='response.in_progress')\n",
      "ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='msg_68b98c10fa888194ac64d16f139032d20b23cab13df8b32a', content=[], role='assistant', status='in_progress', type='message'), output_index=0, sequence_number=2, type='response.output_item.added')\n",
      "ResponseContentPartAddedEvent(content_index=0, item_id='msg_68b98c10fa888194ac64d16f139032d20b23cab13df8b32a', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text', logprobs=[]), sequence_number=3, type='response.content_part.added')\n",
      "ResponseTextDeltaEvent(content_index=0, delta='Se', item_id='msg_68b98c10fa888194ac64d16f139032d20b23cab13df8b32a', logprobs=[], output_index=0, sequence_number=4, type='response.output_text.delta', obfuscation='gWEwdz1qiFUeEK')\n",
      "ResponseTextDeltaEvent(content_index=0, delta='am', item_id='msg_68b98c10fa888194ac64d16f139032d20b23cab13df8b32a', logprobs=[], output_index=0, sequence_number=5, type='response.output_text.delta', obfuscation='LNxNwezTMNE6J0')\n",
      "ResponseTextDeltaEvent(content_index=0, delta='less', item_id='msg_68b98c10fa888194ac64d16f139032d20b23cab13df8b32a', logprobs=[], output_index=0, sequence_number=6, type='response.output_text.delta', obfuscation='oqZZ8WAnjYJ8')\n",
      "ResponseTextDoneEvent(content_index=0, item_id='msg_68b98c10fa888194ac64d16f139032d20b23cab13df8b32a', logprobs=[], output_index=0, sequence_number=7, text='Seamless', type='response.output_text.done')\n",
      "ResponseContentPartDoneEvent(content_index=0, item_id='msg_68b98c10fa888194ac64d16f139032d20b23cab13df8b32a', output_index=0, part=ResponseOutputText(annotations=[], text='Seamless', type='output_text', logprobs=[]), sequence_number=8, type='response.content_part.done')\n",
      "ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='msg_68b98c10fa888194ac64d16f139032d20b23cab13df8b32a', content=[ResponseOutputText(annotations=[], text='Seamless', type='output_text', logprobs=[])], role='assistant', status='completed', type='message'), output_index=0, sequence_number=9, type='response.output_item.done')\n",
      "ResponseCompletedEvent(response=Response(id='resp_68b98c1027808194a1740c52e02008b00b23cab13df8b32a', created_at=1756990480.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[ResponseOutputMessage(id='msg_68b98c10fa888194ac64d16f139032d20b23cab13df8b32a', content=[ResponseOutputText(annotations=[], text='Seamless', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=In: 16; Out: 4; Total: 20, user=None, store=True), sequence_number=10, type='response.completed')\n"
     ]
    }
   ],
   "source": [
    "r = cli.responses.create(model=mods.oai, input=mk_msgs(\"openai sync streaming\"), stream=True)\n",
    "for ch in r: print(ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test async."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli = AsyncOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Innovative\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_68b98c1d2fa881a2af14282ad1943b5b01fd7389a58824ea\n",
       "- created_at: 1756990493.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: None\n",
       "- metadata: {}\n",
       "- model: gpt-4o-2024-08-06\n",
       "- object: response\n",
       "- output: [ResponseOutputMessage(id='msg_68b98c1debfc81a298eb9da76ec1c55401fd7389a58824ea', content=[ResponseOutputText(annotations=[], text='Innovative', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: []\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- conversation: None\n",
       "- max_output_tokens: None\n",
       "- max_tool_calls: None\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- prompt_cache_key: None\n",
       "- reasoning: Reasoning(effort=None, generate_summary=None, summary=None)\n",
       "- safety_identifier: None\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium')\n",
       "- top_logprobs: 0\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=15, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=3, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=18)\n",
       "- user: None\n",
       "- store: True\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_68b98c1d2fa881a2af14282ad1943b5b01fd7389a58824ea', created_at=1756990493.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[ResponseOutputMessage(id='msg_68b98c1debfc81a298eb9da76ec1c55401fd7389a58824ea', content=[ResponseOutputText(annotations=[], text='Innovative', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=In: 15; Out: 3; Total: 18, user=None, store=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await cli.responses.create(model=mods.oai, input=mk_msgs(\"openai async\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Innovative\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_68b98c1d2fa881a2af14282ad1943b5b01fd7389a58824ea\n",
       "- created_at: 1756990493.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: None\n",
       "- metadata: {}\n",
       "- model: gpt-4o-2024-08-06\n",
       "- object: response\n",
       "- output: [ResponseOutputMessage(id='msg_68b98c1debfc81a298eb9da76ec1c55401fd7389a58824ea', content=[ResponseOutputText(annotations=[], text='Innovative', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: []\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- conversation: None\n",
       "- max_output_tokens: None\n",
       "- max_tool_calls: None\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- prompt_cache_key: None\n",
       "- reasoning: Reasoning(effort=None, generate_summary=None, summary=None)\n",
       "- safety_identifier: None\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium')\n",
       "- top_logprobs: 0\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=15, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=3, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=18)\n",
       "- user: None\n",
       "- store: True\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_68b98c1d2fa881a2af14282ad1943b5b01fd7389a58824ea', created_at=1756990493.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[ResponseOutputMessage(id='msg_68b98c1debfc81a298eb9da76ec1c55401fd7389a58824ea', content=[ResponseOutputText(annotations=[], text='Innovative', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=In: 15; Out: 3; Total: 18, user=None, store=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await cli.responses.create(model=mods.oai, input=mk_msgs(\"openai async\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test async streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResponseCreatedEvent(response=Response(id='resp_68b98c217c14819e9206cffe640fb615057b4550d4f1f4e1', created_at=1756990497.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=None, user=None, store=True), sequence_number=0, type='response.created')\n",
      "ResponseInProgressEvent(response=Response(id='resp_68b98c217c14819e9206cffe640fb615057b4550d4f1f4e1', created_at=1756990497.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=None, user=None, store=True), sequence_number=1, type='response.in_progress')\n",
      "ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='msg_68b98c223130819eb09c898a79f3fce5057b4550d4f1f4e1', content=[], role='assistant', status='in_progress', type='message'), output_index=0, sequence_number=2, type='response.output_item.added')\n",
      "ResponseContentPartAddedEvent(content_index=0, item_id='msg_68b98c223130819eb09c898a79f3fce5057b4550d4f1f4e1', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text', logprobs=[]), sequence_number=3, type='response.content_part.added')\n",
      "ResponseTextDeltaEvent(content_index=0, delta='Eff', item_id='msg_68b98c223130819eb09c898a79f3fce5057b4550d4f1f4e1', logprobs=[], output_index=0, sequence_number=4, type='response.output_text.delta', obfuscation='OqZoX2uC6hvN6')\n",
      "ResponseTextDeltaEvent(content_index=0, delta='icient', item_id='msg_68b98c223130819eb09c898a79f3fce5057b4550d4f1f4e1', logprobs=[], output_index=0, sequence_number=5, type='response.output_text.delta', obfuscation='Gv3vy9bTO6')\n",
      "ResponseTextDoneEvent(content_index=0, item_id='msg_68b98c223130819eb09c898a79f3fce5057b4550d4f1f4e1', logprobs=[], output_index=0, sequence_number=6, text='Efficient', type='response.output_text.done')\n",
      "ResponseContentPartDoneEvent(content_index=0, item_id='msg_68b98c223130819eb09c898a79f3fce5057b4550d4f1f4e1', output_index=0, part=ResponseOutputText(annotations=[], text='Efficient', type='output_text', logprobs=[]), sequence_number=7, type='response.content_part.done')\n",
      "ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='msg_68b98c223130819eb09c898a79f3fce5057b4550d4f1f4e1', content=[ResponseOutputText(annotations=[], text='Efficient', type='output_text', logprobs=[])], role='assistant', status='completed', type='message'), output_index=0, sequence_number=8, type='response.output_item.done')\n",
      "ResponseCompletedEvent(response=Response(id='resp_68b98c217c14819e9206cffe640fb615057b4550d4f1f4e1', created_at=1756990497.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[ResponseOutputMessage(id='msg_68b98c223130819eb09c898a79f3fce5057b4550d4f1f4e1', content=[ResponseOutputText(annotations=[], text='Efficient', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=In: 16; Out: 3; Total: 19, user=None, store=True), sequence_number=9, type='response.completed')\n"
     ]
    }
   ],
   "source": [
    "r = await cli.responses.create(model=mods.oai, input=mk_msgs(\"openai async streaming\"), stream=True)\n",
    "async for ch in r: print(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResponseCreatedEvent(response=Response(id='resp_68b98c217c14819e9206cffe640fb615057b4550d4f1f4e1', created_at=1756990497.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=None, user=None, store=True), sequence_number=0, type='response.created')\n",
      "ResponseInProgressEvent(response=Response(id='resp_68b98c217c14819e9206cffe640fb615057b4550d4f1f4e1', created_at=1756990497.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=None, user=None, store=True), sequence_number=1, type='response.in_progress')\n",
      "ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='msg_68b98c223130819eb09c898a79f3fce5057b4550d4f1f4e1', content=[], role='assistant', status='in_progress', type='message'), output_index=0, sequence_number=2, type='response.output_item.added')\n",
      "ResponseContentPartAddedEvent(content_index=0, item_id='msg_68b98c223130819eb09c898a79f3fce5057b4550d4f1f4e1', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text', logprobs=[]), sequence_number=3, type='response.content_part.added')\n",
      "ResponseTextDeltaEvent(content_index=0, delta='Eff', item_id='msg_68b98c223130819eb09c898a79f3fce5057b4550d4f1f4e1', logprobs=[], output_index=0, sequence_number=4, type='response.output_text.delta', obfuscation='OqZoX2uC6hvN6')\n",
      "ResponseTextDeltaEvent(content_index=0, delta='icient', item_id='msg_68b98c223130819eb09c898a79f3fce5057b4550d4f1f4e1', logprobs=[], output_index=0, sequence_number=5, type='response.output_text.delta', obfuscation='Gv3vy9bTO6')\n",
      "ResponseTextDoneEvent(content_index=0, item_id='msg_68b98c223130819eb09c898a79f3fce5057b4550d4f1f4e1', logprobs=[], output_index=0, sequence_number=6, text='Efficient', type='response.output_text.done')\n",
      "ResponseContentPartDoneEvent(content_index=0, item_id='msg_68b98c223130819eb09c898a79f3fce5057b4550d4f1f4e1', output_index=0, part=ResponseOutputText(annotations=[], text='Efficient', type='output_text', logprobs=[]), sequence_number=7, type='response.content_part.done')\n",
      "ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='msg_68b98c223130819eb09c898a79f3fce5057b4550d4f1f4e1', content=[ResponseOutputText(annotations=[], text='Efficient', type='output_text', logprobs=[])], role='assistant', status='completed', type='message'), output_index=0, sequence_number=8, type='response.output_item.done')\n",
      "ResponseCompletedEvent(response=Response(id='resp_68b98c217c14819e9206cffe640fb615057b4550d4f1f4e1', created_at=1756990497.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-2024-08-06', object='response', output=[ResponseOutputMessage(id='msg_68b98c223130819eb09c898a79f3fce5057b4550d4f1f4e1', content=[ResponseOutputText(annotations=[], text='Efficient', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=In: 16; Out: 3; Total: 19, user=None, store=True), sequence_number=9, type='response.completed')\n"
     ]
    }
   ],
   "source": [
    "r = await cli.responses.create(model=mods.oai, input=mk_msgs(\"openai async streaming\"), stream=True)\n",
    "async for ch in r: print(ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli = Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Coordination**\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `msg_0181qZnqWn18VFK8JtqMAgWk`\n",
       "- content: `[{'citations': None, 'text': '**Coordination**', 'type': 'text'}]`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- role: `assistant`\n",
       "- stop_reason: `end_turn`\n",
       "- stop_sequence: `None`\n",
       "- type: `message`\n",
       "- usage: `{'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 15, 'output_tokens': 7, 'server_tool_use': None, 'service_tier': 'standard'}`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Message(id='msg_0181qZnqWn18VFK8JtqMAgWk', content=[TextBlock(citations=None, text='**Coordination**', type='text')], model='claude-sonnet-4-20250514', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=In: 15; Out: 7; Cache create: 0; Cache read: 0; Total Tokens: 22; Search: 0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs(\"ant sync\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Coordination**\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `msg_0181qZnqWn18VFK8JtqMAgWk`\n",
       "- content: `[{'citations': None, 'text': '**Coordination**', 'type': 'text'}]`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- role: `assistant`\n",
       "- stop_reason: `end_turn`\n",
       "- stop_sequence: `None`\n",
       "- type: `message`\n",
       "- usage: `{'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 15, 'output_tokens': 7, 'server_tool_use': None, 'service_tier': 'standard'}`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Message(id='msg_0181qZnqWn18VFK8JtqMAgWk', content=[TextBlock(citations=None, text='**Coordination**', type='text')], model='claude-sonnet-4-20250514', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=In: 15; Out: 7; Cache create: 0; Cache read: 0; Total Tokens: 22; Search: 0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs(\"ant sync\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RawMessageStartEvent(message=Message(id='msg_01Xru7ANUjkez8Zwje9uV175', content=[], model='claude-sonnet-4-20250514', role='assistant', stop_reason=None, stop_sequence=None, type='message', usage=In: 16; Out: 1; Cache create: 0; Cache read: 0; Total Tokens: 17; Search: 0), type='message_start')\n",
      "RawContentBlockStartEvent(content_block=TextBlock(citations=None, text='', type='text'), index=0, type='content_block_start')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text='**', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text='Latency**', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockStopEvent(index=0, type='content_block_stop')\n",
      "RawMessageDeltaEvent(delta=Delta(stop_reason='end_turn', stop_sequence=None), type='message_delta', usage=MessageDeltaUsage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=16, output_tokens=8, server_tool_use=None))\n",
      "RawMessageStopEvent(type='message_stop')\n"
     ]
    }
   ],
   "source": [
    "r = cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs(\"ant sync streaming\"), stream=True)\n",
    "for ch in r: print(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RawMessageStartEvent(message=Message(id='msg_01Xru7ANUjkez8Zwje9uV175', content=[], model='claude-sonnet-4-20250514', role='assistant', stop_reason=None, stop_sequence=None, type='message', usage=In: 16; Out: 1; Cache create: 0; Cache read: 0; Total Tokens: 17; Search: 0), type='message_start')\n",
      "RawContentBlockStartEvent(content_block=TextBlock(citations=None, text='', type='text'), index=0, type='content_block_start')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text='**', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockDeltaEvent(delta=TextDelta(text='Latency**', type='text_delta'), index=0, type='content_block_delta')\n",
      "RawContentBlockStopEvent(index=0, type='content_block_stop')\n",
      "RawMessageDeltaEvent(delta=Delta(stop_reason='end_turn', stop_sequence=None), type='message_delta', usage=MessageDeltaUsage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=16, output_tokens=8, server_tool_use=None))\n",
      "RawMessageStopEvent(type='message_stop')\n"
     ]
    }
   ],
   "source": [
    "r = cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs(\"ant sync streaming\"), stream=True)\n",
    "for ch in r: print(ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test async."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import AsyncAnthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli = AsyncAnthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Concurrent**\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `msg_01WC1453hRB7KXaMhzTC6FJ8`\n",
       "- content: `[{'citations': None, 'text': '**Concurrent**', 'type': 'text'}]`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- role: `assistant`\n",
       "- stop_reason: `end_turn`\n",
       "- stop_sequence: `None`\n",
       "- type: `message`\n",
       "- usage: `{'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 15, 'output_tokens': 7, 'server_tool_use': None, 'service_tier': 'standard'}`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Message(id='msg_01WC1453hRB7KXaMhzTC6FJ8', content=[TextBlock(citations=None, text='**Concurrent**', type='text')], model='claude-sonnet-4-20250514', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=In: 15; Out: 7; Cache create: 0; Cache read: 0; Total Tokens: 22; Search: 0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs(\"ant async\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Concurrent**\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `msg_01WC1453hRB7KXaMhzTC6FJ8`\n",
       "- content: `[{'citations': None, 'text': '**Concurrent**', 'type': 'text'}]`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- role: `assistant`\n",
       "- stop_reason: `end_turn`\n",
       "- stop_sequence: `None`\n",
       "- type: `message`\n",
       "- usage: `{'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 15, 'output_tokens': 7, 'server_tool_use': None, 'service_tier': 'standard'}`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Message(id='msg_01WC1453hRB7KXaMhzTC6FJ8', content=[TextBlock(citations=None, text='**Concurrent**', type='text')], model='claude-sonnet-4-20250514', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=In: 15; Out: 7; Cache create: 0; Cache read: 0; Total Tokens: 22; Search: 0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs(\"ant async\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test async streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event: message_start\n",
      "data: {\"type\":\"message_start\",\"message\":{\"id\":\"msg_01AiTdgVvEzbkE5mbBrSC5Lp\",\"type\":\"message\",\"role\":\"assistant\",\"model\":\"claude-sonnet-4-20250514\",\"content\":[],\"stop_reason\":null,\"stop_sequence\":null,\"usage\":{\"input_tokens\":16,\"cache_creation_input_tokens\":0,\"cache_read_input_tokens\":0,\"cache_creation\":{\"ephemeral_5m_input_tokens\":0,\"ephemeral_1h_input_tokens\":0},\"output_tokens\":1,\"service_tier\":\"standard\"}}         }\n",
      "\n",
      "event: content_block_start\n",
      "data: {\"type\":\"content_block_start\",\"index\":0,\"content_block\":{\"type\":\"text\",\"text\":\"\"}    }\n",
      "\n",
      "event: content_block_delta\n",
      "data: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"text_delta\",\"text\":\"Async\"}           }\n",
      "\n",
      "event: content_block_delta\n",
      "data: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"text_delta\",\"text\":\"Iterable\"}     }\n",
      "\n",
      "event: content_block_stop\n",
      "data: {\"type\":\"content_block_stop\",\"index\":0            }\n",
      "\n",
      "event: message_delta\n",
      "data: {\"type\":\"message_delta\",\"delta\":{\"stop_reason\":\"end_turn\",\"stop_sequence\":null},\"usage\":{\"input_tokens\":16,\"cache_creation_input_tokens\":0,\"cache_read_input_tokens\":0,\"output_tokens\":6}          }\n",
      "\n",
      "event: message_stop\n",
      "data: {\"type\":\"message_stop\"       }\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = await cli.messages.create(model=mods.ant,max_tokens=1024,messages=mk_msgs(\"ant async streaming\"), stream=True)\n",
    "async for ch in r.response.aiter_bytes(): print(ch.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event: message_start\n",
      "data: {\"type\":\"message_start\",\"message\":{\"id\":\"msg_01AiTdgVvEzbkE5mbBrSC5Lp\",\"type\":\"message\",\"role\":\"assistant\",\"model\":\"claude-sonnet-4-20250514\",\"content\":[],\"stop_reason\":null,\"stop_sequence\":null,\"usage\":{\"input_tokens\":16,\"cache_creation_input_tokens\":0,\"cache_read_input_tokens\":0,\"cache_creation\":{\"ephemeral_5m_input_tokens\":0,\"ephemeral_1h_input_tokens\":0},\"output_tokens\":1,\"service_tier\":\"standard\"}}         }\n",
      "\n",
      "event: content_block_start\n",
      "data: {\"type\":\"content_block_start\",\"index\":0,\"content_block\":{\"type\":\"text\",\"text\":\"\"}    }\n",
      "\n",
      "event: content_block_delta\n",
      "data: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"text_delta\",\"text\":\"Async\"}           }\n",
      "\n",
      "event: content_block_delta\n",
      "data: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"text_delta\",\"text\":\"Iterable\"}     }\n",
      "\n",
      "event: content_block_stop\n",
      "data: {\"type\":\"content_block_stop\",\"index\":0            }\n",
      "\n",
      "event: message_delta\n",
      "data: {\"type\":\"message_delta\",\"delta\":{\"stop_reason\":\"end_turn\",\"stop_sequence\":null},\"usage\":{\"input_tokens\":16,\"cache_creation_input_tokens\":0,\"cache_read_input_tokens\":0,\"output_tokens\":6}          }\n",
      "\n",
      "event: message_stop\n",
      "data: {\"type\":\"message_stop\"       }\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = await cli.messages.create(model=mods.ant,max_tokens=1024,messages=mk_msgs(\"ant async streaming\"), stream=True)\n",
    "async for ch in r.response.aiter_bytes(): print(ch.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LiteLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the LiteLLM SDK by running sync/async calls with(out) streaming for OpenAI, Anthropic, & Gemini.\n",
    "\n",
    "We'll also double check tool calls and citations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sync Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a helper method to display a streamed response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _stream(r): \n",
    "    for ch in r: print(ch.choices[0].delta.content or \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test `claude-sonnet-x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-2d7bc3aa-c8f4-4d24-90dd-f496ea5345ca', created=1756990536, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='seamless', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=5, prompt_tokens=18, total_tokens=23, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = completion(model=mods.ant, messages=mk_msgs(\"lite: ant sync...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-6c9f6f82-613c-43d5-8f69-e0992e47a4b9', created=1756990539, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='seamless', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=5, prompt_tokens=18, total_tokens=23, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = completion(model=mods.ant, messages=mk_msgs(\"lite: ant sync...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with streaming enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "Stream**\n",
      "\n",
      "(It flows with minimal overhea\n",
      "d, like a lightweight synchronous data stream connecting\n",
      " distributed processes)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = completion(model=mods.ant, messages=mk_msgs(\"lite: ant sync stream...\"), stream=True)\n",
    "_stream(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "Stream**\n",
      "\n",
      "(It flows with minimal overhea\n",
      "d, like a lightweight synchronous data stream connecting\n",
      " distributed processes)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = completion(model=mods.ant, messages=mk_msgs(\"lite: ant sync stream...\"), stream=True)\n",
    "_stream(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test `gpt-4o`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-CC3nFvP1pBj3wuyFBKDq4E4awiP9G', created=1756990553, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_cbf1785567', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Efficiency.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=2, prompt_tokens=18, total_tokens=20, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = completion(model=mods.oai, messages=mk_msgs(\"lite: oai sync...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-CC3nFvP1pBj3wuyFBKDq4E4awiP9G', created=1756990553, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_cbf1785567', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Efficiency.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=2, prompt_tokens=18, total_tokens=20, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = completion(model=mods.oai, messages=mk_msgs(\"lite: oai sync...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with streaming enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integration\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = completion(model=mods.oai, messages=mk_msgs(\"lite: oai sync stream...\"), stream=True)\n",
    "_stream(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integration\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = completion(model=mods.oai, messages=mk_msgs(\"lite: oai sync stream...\"), stream=True)\n",
    "_stream(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test `2.0-flash`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='Yoy5aJ-NGN78nsEP9J-5gQ0', created=1756990562, model='gemini-2.0-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Speed\\n', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=2, prompt_tokens=10, total_tokens=12, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=10, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = completion(model=mods.gem, messages=mk_msgs(\"lite: gem sync...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='Yoy5aJ-NGN78nsEP9J-5gQ0', created=1756990563, model='gemini-2.0-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Speed\\n', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=2, prompt_tokens=10, total_tokens=12, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=10, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = completion(model=mods.gem, messages=mk_msgs(\"lite: gem sync...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with streaming enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real\n",
      "time\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = completion(model=mods.gem, messages=mk_msgs(\"lite: gem sync stream...\"), stream=True)\n",
    "_stream(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real\n",
      "time\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = completion(model=mods.gem, messages=mk_msgs(\"lite: gem sync stream...\"), stream=True)\n",
    "_stream(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Async Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import acompletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _astream(r):\n",
    "    async for chunk in r: print(chunk.choices[0].delta.content or \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test `claude-sonnet-x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-646ca947-93e1-4e6d-bc5a-10e7508b0f9a', created=1756990575, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='**concurrent**', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=6, prompt_tokens=18, total_tokens=24, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await acompletion(model=mods.ant, messages=mk_msgs(\"lite: ant async...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-d2bae8f2-5291-418f-bb7f-42e851a169e1', created=1756990577, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='**concurrent**', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=6, prompt_tokens=18, total_tokens=24, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await acompletion(model=mods.ant, messages=mk_msgs(\"lite: ant async...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with streaming enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "Generator**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = await acompletion(model=mods.ant, messages=mk_msgs(\"lite: ant async stream...\"), stream=True)\n",
    "await(_astream(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "Generator**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = await acompletion(model=mods.ant, messages=mk_msgs(\"lite: ant async stream...\"), stream=True)\n",
    "await(_astream(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test `gpt-4o`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-CC3nlRg0j6UCRX5XpCvk3qtilPPPC', created=1756990585, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_80956533cb', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Efficient.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=3, prompt_tokens=18, total_tokens=21, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await acompletion(model=mods.oai, messages=mk_msgs(\"lite: oai async...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-CC3nlRg0j6UCRX5XpCvk3qtilPPPC', created=1756990585, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_80956533cb', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Efficient.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=3, prompt_tokens=18, total_tokens=21, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await acompletion(model=mods.oai, messages=mk_msgs(\"lite: oai async...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with streaming enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eff\n",
      "icient\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = await acompletion(model=mods.oai, messages=mk_msgs(\"lite: oai async stream...\"), stream=True)\n",
    "await(_astream(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eff\n",
      "icient\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = await acompletion(model=mods.oai, messages=mk_msgs(\"lite: oai async stream...\"), stream=True)\n",
    "await(_astream(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test `2.0-flash`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='goy5aNKBKZfjnsEPsPWEmA0', created=1756990594, model='gemini-2.0-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Fiber\\n', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=2, prompt_tokens=10, total_tokens=12, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=10, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await acompletion(model=mods.gem, messages=mk_msgs(\"lite: gem async...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='goy5aNKBKZfjnsEPsPWEmA0', created=1756990596, model='gemini-2.0-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Fiber\\n', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=2, prompt_tokens=10, total_tokens=12, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=10, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await acompletion(model=mods.gem, messages=mk_msgs(\"lite: gem async...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with streaming enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Efficient\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = await acompletion(model=mods.gem, messages=mk_msgs(\"lite: gem async stream...\"), stream=True)\n",
    "await(_astream(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Efficient\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = await acompletion(model=mods.gem, messages=mk_msgs(\"lite: gem async stream...\"), stream=True)\n",
    "await(_astream(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tool Calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check let's confirm that tool calls work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\"type\":\"string\", \"description\":\"The city e.g. Reims\"},\n",
    "                    \"unit\": {\"type\":\"string\", \"enum\":[\"celsius\", \"fahrenheit\"]},\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-3ac7d40f-81d0-4c72-8f34-34564d1ddbc4', created=1756990637, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=0, function=Function(arguments='{\"location\": \"Reims\"}', name='get_current_weather'), id='toolu_01GVccciafRHRKcb6JoQbu38', type='function')], function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=57, prompt_tokens=427, total_tokens=484, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = completion(model=mods.ant, messages=mk_msgs(\"Is it raining in Reims?\"), tools=tools)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-e7179c73-f158-4fc1-b5cb-002ec9059b5b', created=1756990638, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=0, function=Function(arguments='{\"location\": \"Reims\"}', name='get_current_weather'), id='toolu_01GVccciafRHRKcb6JoQbu38', type='function')], function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=57, prompt_tokens=427, total_tokens=484, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = completion(model=mods.ant, messages=mk_msgs(\"Is it raining in Reims?\"), tools=tools)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
