{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from cachy.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cachy\n",
    "\n",
    "> Cache your API calls and make your notebooks fast again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often call APIs while prototyping and testing our code. A single API call (e.g. an Anthropic chat completion) can take 100's of ms to run. This can really slow down development especially if our notebook contains many API calls ðŸ˜ž."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cachy` caches API requests. It does this by saving the result of each call to a local `cachy.jsonl` file. Before calling an API (e.g. OpenAI) it will check if the request exists in `cachy.jsonl`. If it does it will return the cached result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does it work?**\n",
    "\n",
    "Under the hood popular SDK's like OpenAI, Anthropic and LiteLLM use `httpx.Client` and `httpx.AsyncClient`. \n",
    "\n",
    "`cachy` patches the `send` method of both clients and injects a simple caching mechanism:\n",
    "\n",
    "- create a cache key from the request\n",
    "- if the key exists in `cachy.jsonl` return the cached response\n",
    "- if not, call the API and save the response to `cachy.jsonl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use `cachy`\n",
    "\n",
    "- install the package: `pip install pycachy`\n",
    "- add the snippet below to the top of your notebook\n",
    "\n",
    "```python\n",
    "from cachy import enable_cachy\n",
    "\n",
    "enable_cachy()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default `cachy` will cache requests made to OpenAI, Anthropic, Gemini and DeepSeek.\n",
    "\n",
    "*Note: Gemini caching only works via the LiteLLM SDK.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "## Custom APIs\n",
    "\n",
    "If you're using the OpenAI or LiteLLM SDK for other LLM providers like Grok, Mistral you can cache these requests as shown below.\n",
    "\n",
    "```python\n",
    "from cachy import enable_cachy, doms\n",
    "enable_cachy(doms=doms+('api.x.ai', 'api.mistral.com'))\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docs can be found hosted on this GitHub [repository][repo]'s [pages][docs].\n",
    "\n",
    "[repo]: https://github.com/AnswerDotAI/cachy\n",
    "[docs]: https://AnswerDotAI.github.io/cachy/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import and enable cachy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cachy import enable_cachy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_cachy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run your api calls as normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hey! How can I help you today? ðŸ˜Š\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_68b9978ecec48196aa3e77b09ed41c6403f00c61bc19c097\n",
       "- created_at: 1756993423.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: None\n",
       "- metadata: {}\n",
       "- model: gpt-4.1-2025-04-14\n",
       "- object: response\n",
       "- output: [ResponseOutputMessage(id='msg_68b9978f9f70819684b17b0f21072a9003f00c61bc19c097', content=[ResponseOutputText(annotations=[], text='Hey! How can I help you today? ðŸ˜Š', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: []\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- conversation: None\n",
       "- max_output_tokens: None\n",
       "- max_tool_calls: None\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- prompt_cache_key: None\n",
       "- reasoning: Reasoning(effort=None, generate_summary=None, summary=None)\n",
       "- safety_identifier: None\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium')\n",
       "- top_logprobs: 0\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=9, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=11, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=20)\n",
       "- user: None\n",
       "- store: True\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_68b9978ecec48196aa3e77b09ed41c6403f00c61bc19c097', created_at=1756993423.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4.1-2025-04-14', object='response', output=[ResponseOutputMessage(id='msg_68b9978f9f70819684b17b0f21072a9003f00c61bc19c097', content=[ResponseOutputText(annotations=[], text='Hey! How can I help you today? ðŸ˜Š', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=In: 9; Out: 11; Total: 20, user=None, store=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = cli.responses.create(model=\"gpt-4.1\", input=\"Hey!\")\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the same request again it will read it from the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hey! How can I help you today? ðŸ˜Š\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_68b9978ecec48196aa3e77b09ed41c6403f00c61bc19c097\n",
       "- created_at: 1756993423.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: None\n",
       "- metadata: {}\n",
       "- model: gpt-4.1-2025-04-14\n",
       "- object: response\n",
       "- output: [ResponseOutputMessage(id='msg_68b9978f9f70819684b17b0f21072a9003f00c61bc19c097', content=[ResponseOutputText(annotations=[], text='Hey! How can I help you today? ðŸ˜Š', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: []\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- conversation: None\n",
       "- max_output_tokens: None\n",
       "- max_tool_calls: None\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- prompt_cache_key: None\n",
       "- reasoning: Reasoning(effort=None, generate_summary=None, summary=None)\n",
       "- safety_identifier: None\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium')\n",
       "- top_logprobs: 0\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=9, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=11, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=20)\n",
       "- user: None\n",
       "- store: True\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_68b9979909cc8191ba8a4347910295250fe0de503ffe089a', created_at=1756993433.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4.1-2025-04-14', object='response', output=[ResponseOutputMessage(id='msg_68b99799d0248191a7ce57b451d7414f0fe0de503ffe089a', content=[ResponseOutputText(annotations=[], text='Hey! How can I help you today? ðŸ˜Š', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=In: 9; Out: 11; Total: 20, user=None, store=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = cli.responses.create(model=\"gpt-4.1\", input=\"Hey!\")\n",
    "r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
