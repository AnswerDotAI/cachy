{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107db453",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from cachy.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6ea34a",
   "metadata": {},
   "source": [
    "# cachy\n",
    "\n",
    "> Cache your API calls with a single line of code. No mocks, no fixtures. Just faster, cleaner code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c037da9",
   "metadata": {},
   "source": [
    "We often call APIs while prototyping and testing our code. A single API call (e.g. an Anthropic chat completion) can take 100's of ms to run. This can really slow down development especially if our notebook contains many API calls ðŸ˜ž."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930c6f35",
   "metadata": {},
   "source": [
    "`cachy` caches API requests. It does this by saving the result of each call to a local `cachy.jsonl` file. Before calling an API (e.g. OpenAI) it will check if the request exists in `cachy.jsonl`. If it does it will return the cached result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be34b3c3",
   "metadata": {},
   "source": [
    "**How does it work?**\n",
    "\n",
    "Under the hood popular SDK's like OpenAI, Anthropic and LiteLLM use `httpx.Client` and `httpx.AsyncClient`. \n",
    "\n",
    "`cachy` patches the `send` method of both clients and injects a simple caching mechanism:\n",
    "\n",
    "- create a cache key from the request\n",
    "- if the key exists in `cachy.jsonl` return the cached response\n",
    "- if not, call the API and save the response to `cachy.jsonl`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6510963c",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7214acbe",
   "metadata": {},
   "source": [
    "To use `cachy`\n",
    "\n",
    "- install the package: `pip install pycachy`\n",
    "- add the snippet below to the top of your notebook\n",
    "\n",
    "```python\n",
    "from cachy import enable_cachy\n",
    "\n",
    "enable_cachy()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01595169",
   "metadata": {},
   "source": [
    "By default `cachy` will cache requests made to OpenAI, Anthropic, Gemini and DeepSeek.\n",
    "\n",
    "*Note: Gemini caching only works via the LiteLLM SDK.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad49f95",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "## Custom APIs\n",
    "\n",
    "If you're using the OpenAI or LiteLLM SDK for other LLM providers like Grok, Mistral you can cache these requests as shown below.\n",
    "\n",
    "```python\n",
    "from cachy import enable_cachy, doms\n",
    "enable_cachy(doms=doms+('api.x.ai', 'api.mistral.com'))\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b512bb1",
   "metadata": {},
   "source": [
    "## Docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dca5ff",
   "metadata": {},
   "source": [
    "Docs can be found hosted on this GitHub [repository][repo]'s [pages][docs].\n",
    "\n",
    "[repo]: https://github.com/AnswerDotAI/cachy\n",
    "[docs]: https://AnswerDotAI.github.io/cachy/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c9b035",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a4c92c",
   "metadata": {},
   "source": [
    "First import and enable cachy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6da27e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cachy import enable_cachy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186103b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_cachy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01195a75",
   "metadata": {},
   "source": [
    "Now run your api calls as normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ea245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdbf324",
   "metadata": {},
   "outputs": [],
   "source": [
    "cli = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bd3104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(id='resp_0f917a6452ee099400697b191473c48191b8e4f6e76e0add02', created_at=1769675028.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4.1-2025-04-14', object='response', output=[ResponseOutputMessage(id='msg_0f917a6452ee099400697b1914af1081919353425f8cd2d7d7', content=[ResponseOutputText(annotations=[], text='Hey! How can I help you today? ðŸ˜Š', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, completed_at=1769675028.0, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=9, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=11, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=20), user=None, billing={'payer': 'developer'}, frequency_penalty=0.0, presence_penalty=0.0, store=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = cli.responses.create(model=\"gpt-4.1\", input=\"Hey!\")\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0df5e9",
   "metadata": {},
   "source": [
    "If you run the same request again it will read it from the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fb3ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(id='resp_0f917a6452ee099400697b191473c48191b8e4f6e76e0add02', created_at=1769675028.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4.1-2025-04-14', object='response', output=[ResponseOutputMessage(id='msg_0f917a6452ee099400697b1914af1081919353425f8cd2d7d7', content=[ResponseOutputText(annotations=[], text='Hey! How can I help you today? ðŸ˜Š', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, completed_at=1769675028.0, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=9, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=11, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=20), user=None, billing={'payer': 'developer'}, frequency_penalty=0.0, presence_penalty=0.0, store=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = cli.responses.create(model=\"gpt-4.1\", input=\"Hey!\")\n",
    "r"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
