# core


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

### Introduction

We often call APIs while prototyping and testing our code. A single API
call (e.g.Â an Anthropic chat completion) can take 100â€™s of ms to run.
This can really slow down development especially if our notebook
contains many API calls ðŸ˜ž.

`cachy` caches API requests. It does this by saving the result of each
API call to a local `cachy.jsonl` file. Before calling an API
(e.g.Â OpenAI) it will check if the request already exists in
`cachy.jsonl`. If it does it will return the cached result.

**How does it work?**

Under the hood popular SDKâ€™s like OpenAI, Anthropic and LiteLLM use
`httpx.Client` and `httpx.AsyncClient`.

`cachy` patches the `send` method of both clients and injects a simple
caching mechanism:

- create a cache key from the request
- if the key exists in `cachy.jsonl` return the cached response
- if not, call the API and save the response to `cachy.jsonl`

``` python
import tempfile
from httpx import RequestNotRead
from fastcore.test import *
```

`cachy.jsonl` contains one API response per line.

Each line has the following format `{"key": key, "response": response}`

- `key`: hash of the API request
- `response`: the API response.

``` json
{
    "key": "afc2be0c", 
    "response": "{\"id\":\"msg_xxx\",\"type\":\"message\",\"role\":\"assistant\",\"model\":\"claude-sonnet-4-20250514\",\"content\":[{\"type\":\"text\",\"text\":\"Coordination.\"}],\"stop_reason\":\"end_turn\",\"stop_sequence\":null,\"usage\":{\"input_tokens\":16,\"cache_creation_input_tokens\":0,\"cache_read_input_tokens\":0,\"cache_creation\":{\"ephemeral_5m_input_tokens\":0,\"ephemeral_1h_input_tokens\":0},\"output_tokens\":6,\"service_tier\":\"standard\"}}"
}
```

### Patching `httpx`

Patching a method is very straightforward.

In our case we want to patch `httpx._client.Client.send` and
`httpx._client.AsyncClient.send`.

These methods are called when running `httpx.get`, `httpx.post`, etc.

In the example below we use `@patch` from
[fastcore](https://fastcore.fast.ai/) to print `calling an API` when
`httpx._client.Client.send` is run.

``` python
@patch
def send(self:httpx._client.Client, r, **kwargs):
    print('calling an API')
    return self._orig_send(r, **kwargs)
```

### Cache Filtering

Now, letâ€™s build up our caching logic piece-by-piece.

The first thing we need to do is ensure that our caching logic only runs
on specific urls.

For now, letâ€™s only cache API calls made to popular LLM providers like
OpenAI, Anthropic, Google and DeepSeek. We can make this fully
customizable later.

<details open class="code-fold">
<summary>Exported source</summary>

``` python
doms = ("api.openai.com", "api.anthropic.com", "generativelanguage.googleapis.com", "api.deepseek.com")
```

</details>

<details open class="code-fold">
<summary>Exported source</summary>

``` python
def _should_cache(url, doms): return any(dom in str(url) for dom in doms)
```

</details>

We could then use `_should_cache` like this.

``` python
@patch
def send(self:httpx._client.Client, r, **kwargs):
    if not _should_cache(r.url, doms): return self._orig_send(r, **kwargs)
    # insert caching logic
    ...
```

### Cache Key

The next thing we need to do is figure out if a response for the request
`r` already exists in our cache.

Recall that each line in `cachy.jsonl` has the following format
`{"key": key, "response": response}`.

Our key needs to be unique and deterministic. One way to do this is to
concatenate the request URL and content, then generate a hash from the
result.

``` python
def _key(r): return hashlib.sha256(str(r.url.copy_remove_param('key')).encode() + r.content).hexdigest()[:8]
```

When LiteLLM calls Gemini it includes the API key in a query param so
thatâ€™s why we strip the `key` param from the url.

Letâ€™s test this out.

``` python
r1 = httpx.Request('POST', 'https://api.openai.com/v1/chat/completions', content=b'some content')
r1
```

``` python
_key(r1)
```

If we run it again we should get the same key.

``` python
_key(r1)
```

Letâ€™s modify the url and confirm we get a different key.

``` python
_key(httpx.Request('POST', 'https://api.anthropic.com/v1/messages', content=b'some content'))
```

Great. Letâ€™s update our patch.

``` python
@patch
def send(self:httpx._client.Client, r, **kwargs):
    if not _should_cache(r.url, doms): return self._orig_send(r, **kwargs)
    key = _key(r)
    # if cache hit return the response
    # else run the request, write to response the cache and return it
    ...
```

### Cache Reads/Writes

Now letâ€™s add some methods that will read from and write to
`cachy.jsonl`.

<details open class="code-fold">
<summary>Exported source</summary>

``` python
def _cache(key, cfp):
    with open(cfp, "r") as f:
        line = first(f, lambda l: json.loads(l)["key"] == key)
        return json.loads(line) if line else None
```

</details>

<details open class="code-fold">
<summary>Exported source</summary>

``` python
def _write_cache(key, content, cfp, hdrs):
    with open(cfp, "a") as f: f.write(json.dumps({"key":key, "response": content, "headers":hdrs})+"\n")
```

</details>

Letâ€™s update our `patch`.

``` python
@patch
def send(self:httpx._client.Client, r, **kwargs):
    if not _should_cache(r.url, doms): return self._orig_send(r, **kwargs)
    key = key(r)
    if res := _cache(key,"cachy.jsonl"): return httpx.Response(status_code=200, content=res, request=r)
    res = self._orig_send(r, **kwargs)
    content = res.read().decode()
    _write_cache(key, content, "cachy.jsonl")
    return httpx.Response(status_code=res.status_code, content=content, request=r)
```

### Multipart Requests

`_key` will throw the following error for multipart requests (e.g.Â file
uploads).

`RequestNotRead: Attempted to access streaming request content, without having called`read()`.`

``` python
rfu = httpx.Request('POST', 'https://api.openai.com/v1/chat/completions', files={"file": ("test.txt", b"hello")})
rfu
```

``` python
test_fail(lambda: _key(rfu), RequestNotRead)
```

``` python
rfu.read(); _key(rfu);
```

Each part of a multipart request is separated by a delimiter called a
boundary with this structure `--b{RANDOM_ID}`. Hereâ€™s an example for
`rfu`.

``` txt
b'--f9ee33966b45cc8c80952bb57cc728c4\r\nContent-Disposition: form-data; name="file"; filename="test.txt"\r\nContent-Type: text/plain\r\n\r\nhello\r\n--f9ee33966b45cc8c80952bb57cc728c4--\r\n'
```

As the boundary is a random id, two identical multipart requests will
produce different boundaries. As the boundary is part of the request
content, `_key` will generate different keys leading to cache misses ðŸ˜ž.

Letâ€™s create a helper method `_content` that will extract content from
any request and remove the non-deterministic boundary.

``` python
rfu = httpx.Request('POST', 'https://api.openai.com/v1/chat/completions', files={"file": ("test.txt", b"hello")})
rfu
```

``` python
_content(rfu)
```

``` python
def _key(r): return hashlib.sha256(str(r.url.copy_remove_param('key')).encode() + _content(r)).hexdigest()[:8]
```

Letâ€™s confirm that running `_key` multiple times on the same multipart
request now returns the same key.

``` python
_key(rfu), _key(rfu)
```

### Streaming

Letâ€™s add support for streaming.

First letâ€™s include an `is_stream` bool in our hash so that a
non-streamed request will generate a different key to the same request
when streamed.

<details open class="code-fold">
<summary>Exported source</summary>

``` python
def _key(r, is_stream=False):
    "Create a unique, deterministic id from the request `r`."
    return hashlib.sha256(f"{r.url.copy_remove_param('key')}{is_stream}".encode() + _content(r)).hexdigest()[:8]
```

</details>

In the `patch` we need to `consume` the entire stream before writing it
to the cache.

``` python
@patch
def send(self:httpx._client.Client, r, **kwargs):
    is_stream = kwargs.get("stream")
    if not _should_cache(r.url, doms): return self._orig_send(r, **kwargs)
    key = _key(r, is_stream=False)
    if res := _cache(key,"cachy.jsonl"): return httpx.Response(status_code=200, content=res, request=r)
    res = self._orig_send(r, **kwargs)
    content = res.read().decode() if not is_stream else b''.join(list(res.iter_bytes())).decode()
    _write_cache(key, content, "cachy.jsonl")
    return httpx.Response(status_code=res.status_code, content=content, request=r)
```

### `enable_cachy`

To make `cachy` as user friendly as possible letâ€™s make it so that we
can apply our patch by running a single method at the top of our
notebook.

``` python
from cachy import enable_cachy

enable_cachy()
```

For this to work weâ€™ll need to wrap our patch.

``` python
def _apply_patch():    
    @patch    
    def send(self:httpx._client.Client, r, **kwargs):
        is_stream = kwargs.get("stream")
        if not _should_cache(r.url, doms): return self._orig_send(r, **kwargs)
        key = _key(r, is_stream=False)
        if res := _cache(key,"cachy.jsonl"): return httpx.Response(status_code=200, content=res, request=r)
        res = self._orig_send(r, **kwargs)
        content = res.read().decode() if not is_stream else b''.join(list(res.iter_bytes())).decode()
        _write_cache(key, content, "cachy.jsonl")
        return httpx.Response(status_code=res.status_code, content=content, request=r)
```

``` python
def enable_cachy():  
    _apply_patch()
```

Great. Now, letâ€™s make `cachy` a little more customizable by making it
possible to specify:

- the APIs (or domains) to cache
- the location of the cache file.

``` python
def enable_cachy(cache_dir=None, doms=doms):
    cfp = Path(cache_dir or find_file_parents("pyproject.toml") or ".") / "cachy.jsonl"
    cfp.touch(exist_ok=True)   
    _apply_patch(cfp, doms)
```

``` python
def _apply_patch(cfp, doms):    
    @patch
    def send(self:httpx._client.Client, r, **kwargs):
        is_stream = kwargs.get("stream")
        if not _should_cache(r.url, doms): return self._orig_send(r, **kwargs)
        key = _key(r, is_stream=False)
        if res := _cache(key,cfp): return httpx.Response(status_code=200, content=res, request=r)
        res = self._orig_send(r, **kwargs)
        content = res.read().decode() if not is_stream else b''.join(list(res.iter_bytes())).decode()
        _write_cache(key,content,cfp)
        return httpx.Response(status_code=res.status_code, content=content, request=r)
```

### Async

Some APIs such as Gemini Files API rely on response headers:

Now letâ€™s add support for `async` requests.

<details open class="code-fold">
<summary>Exported source</summary>

``` python
def _apply_async_patch(cfp, doms, hdrs):    
    @patch
    async def send(self:httpx._client.AsyncClient, r, **kwargs):
        is_stream = kwargs.get("stream")
        if not _should_cache(r.url, doms): return await self._orig_send(r, **kwargs)
        key = _key(r, is_stream=False)
        if res := _cache(key,cfp): return httpx.Response(status_code=200, content=res['response'], headers=res.get('headers'), request=r)
        res = await self._orig_send(r, **kwargs)
        content = res.read().decode() if not is_stream else b''.join([c async for c in res.aiter_bytes()]).decode()
        headers = _res_hdrs(res, hdrs)
        _write_cache(key,content,cfp,headers)
        return httpx.Response(status_code=res.status_code, content=content, headers=headers, request=r)
```

</details>

Letâ€™s rename our original patch.

<details open class="code-fold">
<summary>Exported source</summary>

``` python
def _apply_sync_patch(cfp, doms, hdrs):    
    @patch
    def send(self:httpx._client.Client, r, **kwargs):
        is_stream = kwargs.get("stream")
        if not _should_cache(r.url, doms): return self._orig_send(r, **kwargs)
        key = _key(r, is_stream=False)
        if res := _cache(key,cfp): return httpx.Response(status_code=200, content=res['response'], headers=res.get('headers'), request=r)
        res = self._orig_send(r, **kwargs)
        content = res.read().decode() if not is_stream else b''.join(list(res.iter_bytes())).decode()
        headers = _res_hdrs(res, hdrs)
        _write_cache(key,content,cfp,headers)
        return httpx.Response(status_code=res.status_code, content=content, headers=headers, request=r)
```

</details>

Finally, letâ€™s update `enable_cachy`.

------------------------------------------------------------------------

### enable_cachy

``` python

def enable_cachy(
    cache_dir:NoneType=None,
    doms:tuple=('api.openai.com', 'api.anthropic.com', 'generativelanguage.googleapis.com', 'api.deepseek.com'),
    hdrs:NoneType=None
):

```

<details open class="code-fold">
<summary>Exported source</summary>

``` python
def enable_cachy(cache_dir=None, doms=doms, hdrs=None):
    if hdrs is None: hdrs=[]
    cfp = Path(cache_dir or find_file_parents("pyproject.toml") or ".") / "cachy.jsonl"
    cfp.touch(exist_ok=True)   
    _apply_sync_patch(cfp, doms, hdrs)
    _apply_async_patch(cfp, doms, hdrs)
```

</details>

And a way to turn if off:

------------------------------------------------------------------------

### disable_cachy

``` python

def disable_cachy(
    
):

```

<details open class="code-fold">
<summary>Exported source</summary>

``` python
def disable_cachy():
    httpx._client.AsyncClient.send = httpx._client.AsyncClient._orig_send
    httpx._client.Client.send      = httpx._client.Client._orig_send
```

</details>

## Tests

Letâ€™s test `enable_cachy` on 3 SDKs (OpenAI, Anthropic, LiteLLM) for the
scenarios below:

- sync requests with(out) streaming
- async requests with(out) streaming

Add some helper functions.

``` python
class mods: ant="claude-sonnet-4-20250514"; oai="gpt-4o"; gem="gemini/gemini-2.5-flash"
```

``` python
def mk_msgs(m): return [{"role": "user", "content": f"write 1 word about {m}"}]
```

``` python
enable_cachy()
```

### OpenAI

``` python
from openai import OpenAI
```

``` python
cli = OpenAI()
```

``` python
r = cli.responses.create(model=mods.oai, input=mk_msgs("openai sync"))
r
```

``` python
r = cli.responses.create(model=mods.oai, input=mk_msgs("openai sync"))
r
```

Letâ€™s test streaming.

``` python
r = cli.responses.create(model=mods.oai, input=mk_msgs("openai sync streaming"), stream=True)
for ch in r: print(str(ch)[:60])
```

``` python
r = cli.responses.create(model=mods.oai, input=mk_msgs("openai sync streaming"), stream=True)
for ch in r: print(str(ch)[:60])
```

Letâ€™s test async.

``` python
from openai import AsyncOpenAI
```

``` python
cli = AsyncOpenAI()
```

``` python
r = await cli.responses.create(model=mods.oai, input=mk_msgs("openai async"))
r
```

``` python
r = await cli.responses.create(model=mods.oai, input=mk_msgs("openai async"))
r
```

Letâ€™s test async streaming.

``` python
r = await cli.responses.create(model=mods.oai, input=mk_msgs("openai async streaming"), stream=True)
async for ch in r: print(str(ch)[:60])
```

``` python
r = await cli.responses.create(model=mods.oai, input=mk_msgs("openai async streaming"), stream=True)
async for ch in r: print(str(ch)[:60])
```

### Anthropic

``` python
from anthropic import Anthropic
```

``` python
cli = Anthropic()
```

``` python
r = cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs("ant sync"))
r
```

``` python
r = cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs("ant sync"))
r
```

Letâ€™s test streaming.

``` python
r = cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs("ant sync streaming"), stream=True)
for ch in r: print(ch)
```

``` python
r = cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs("ant sync streaming"), stream=True)
for ch in r: print(ch)
```

Letâ€™s test async.

``` python
from anthropic import AsyncAnthropic
```

``` python
cli = AsyncAnthropic()
```

``` python
r = await cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs("ant async"))
r
```

``` python
r = await cli.messages.create(model=mods.ant, max_tokens=1024, messages=mk_msgs("ant async"))
r
```

Letâ€™s test async streaming.

``` python
r = await cli.messages.create(model=mods.ant,max_tokens=1024,messages=mk_msgs("ant async streaming"), stream=True)
async for ch in r.response.aiter_bytes(): print(ch.decode())
```

``` python
r = await cli.messages.create(model=mods.ant,max_tokens=1024,messages=mk_msgs("ant async streaming"), stream=True)
async for ch in r.response.aiter_bytes(): print(ch.decode())
```

### LiteLLM

Letâ€™s test the LiteLLM SDK by running sync/async calls with(out)
streaming for OpenAI, Anthropic, & Gemini.

Weâ€™ll also double check tool calls and citations.

#### Sync Tests

``` python
from litellm import completion
```

Letâ€™s define a helper method to display a streamed response.

``` python
def _stream(r): 
    for ch in r: print(ch.choices[0].delta.content or "")
```

##### Anthropic

Letâ€™s test `claude-sonnet-x`.

``` python
r = completion(model=mods.ant, messages=mk_msgs("lite: ant sync..."))
r
```

``` python
r = completion(model=mods.ant, messages=mk_msgs("lite: ant sync..."))
r
```

Now, with streaming enabled.

``` python
r = completion(model=mods.ant, messages=mk_msgs("lite: ant sync stream..."), stream=True)
_stream(r)
```

``` python
r = completion(model=mods.ant, messages=mk_msgs("lite: ant sync stream..."), stream=True)
_stream(r)
```

##### OpenAI

Letâ€™s test `gpt-4o`.

``` python
r = completion(model=mods.oai, messages=mk_msgs("lite: oai sync..."))
r
```

``` python
r = completion(model=mods.oai, messages=mk_msgs("lite: oai sync..."))
r
```

Now, with streaming enabled.

``` python
r = completion(model=mods.oai, messages=mk_msgs("lite: oai sync stream..."), stream=True)
_stream(r)
```

``` python
r = completion(model=mods.oai, messages=mk_msgs("lite: oai sync stream..."), stream=True)
_stream(r)
```

##### Gemini

Letâ€™s test `2.5-flash`.

``` python
import os
```

``` python
r = completion(model=mods.gem, messages=mk_msgs("lite: gem sync..."))
r
```

``` python
r = completion(model=mods.gem, messages=mk_msgs("lite: gem sync..."))
r
```

Now, with streaming enabled.

``` python
r = completion(model=mods.gem, messages=mk_msgs("lite: gem sync stream..."), stream=True)
_stream(r)
```

``` python
r = completion(model=mods.gem, messages=mk_msgs("lite: gem sync stream..."), stream=True)
_stream(r)
```

#### Async Tests

``` python
from litellm import acompletion
```

``` python
async def _astream(r):
    async for chunk in r: print(chunk.choices[0].delta.content or "")
```

##### Anthropic

Letâ€™s test `claude-sonnet-x`.

``` python
r = await acompletion(model=mods.ant, messages=mk_msgs("lite: ant async..."))
r
```

``` python
r = await acompletion(model=mods.ant, messages=mk_msgs("lite: ant async..."))
r
```

Now, with streaming enabled.

``` python
r = await acompletion(model=mods.ant, messages=mk_msgs("lite: ant async stream..."), stream=True)
await(_astream(r))
```

``` python
r = await acompletion(model=mods.ant, messages=mk_msgs("lite: ant async stream..."), stream=True)
await(_astream(r))
```

##### OpenAI

Letâ€™s test `gpt-4o`.

``` python
r = await acompletion(model=mods.oai, messages=mk_msgs("lite: oai async..."))
r
```

``` python
r = await acompletion(model=mods.oai, messages=mk_msgs("lite: oai async..."))
r
```

Now, with streaming enabled.

``` python
r = await acompletion(model=mods.oai, messages=mk_msgs("lite: oai async stream..."), stream=True)
await(_astream(r))
```

``` python
r = await acompletion(model=mods.oai, messages=mk_msgs("lite: oai async stream..."), stream=True)
await(_astream(r))
```

##### Gemini

Letâ€™s test `2.5-flash`.

``` python
r = await acompletion(model=mods.gem, messages=mk_msgs("lite: gem async..."))
r
```

``` python
r = await acompletion(model=mods.gem, messages=mk_msgs("lite: gem async..."))
r
```

Now, with streaming enabled.

``` python
r = await acompletion(model=mods.gem, messages=mk_msgs("lite: gem async stream..."), stream=True)
await(_astream(r))
```

``` python
r = await acompletion(model=mods.gem, messages=mk_msgs("lite: gem async stream..."), stream=True)
await(_astream(r))
```

#### Tool Calls

As a sanity check letâ€™s confirm that tool calls work.

``` python
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_current_weather",
            "description": "Get the current weather in a given location",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {"type":"string", "description":"The city e.g. Reims"},
                    "unit": {"type":"string", "enum":["celsius", "fahrenheit"]},
                },
                "required": ["location"],
            }
        }
    }
]
```

``` python
r = completion(model=mods.ant, messages=mk_msgs("Is it raining in Reims?"), tools=tools)
r
```

``` python
r = completion(model=mods.ant, messages=mk_msgs("Is it raining in Reims?"), tools=tools)
r
```

### Multipart Request

``` python
cli = Anthropic()
r = cli.beta.files.upload(file=("ex.txt", b"hello world", "text/plain"))
r
```

``` python
cli = Anthropic()
r = cli.beta.files.upload(file=("ex.txt", b"hello world", "text/plain"))
r
```

### Gemini Model Comparison

When LiteLLM calls Gemini it includes the model name in the url. Letâ€™s
test that we can run the same prompt with two different Gemini models.

``` python
mods.gem
```

``` python
r = completion(model=mods.gem, messages=mk_msgs("lite: gem different models..."))
r
```

``` python
r = completion(model="gemini/gemini-2.5-flash", messages=mk_msgs("lite: gem different models..."))
r
```

### Gemini File Upload

The `google-genai` SDKâ€™s `files.upload()` relies on `x-goog-upload-url`
and `x-goog-upload-status` response headers

``` python
from google import genai
```

``` python
cli = genai.Client()
```

When no `hdrs` is provided the request fails:

``` python
tfw = tempfile.NamedTemporaryFile(suffix='.txt')
```

``` python
f = tfw.__enter__()
fn = Path(f.name)
fn.write_text("test content");
```

``` python
try: gfile = cli.files.upload(file=fn)
except Exception as e: print(e)
```

Remove the failed cache entry:

``` python
cfp = Path('../cachy.jsonl')
lines = cfp.read_text().splitlines()
_ = cfp.write_text('\n'.join(lines[:-1]) + '\n' if lines[:-1] else '')
```

When caching Gemini file uploads, by default request content only
includes `mime_type` and `size_bytes`. This means different files with
the same mime type and size produce identical cache keys, causing
incorrect cache hits. The fix is to pass a file content fingerprint (a
hash of the file bytes) as the `display_name` in the upload config:
`cli.files.upload(file=fn, config={"display_name": _fingerprint(fn)})`.
This ensures the request body is unique per file content, generating
distinct cache keys.

``` python
def _fingerprint(path): return hashlib.sha256(Path(path).read_bytes()).hexdigest()[:16]
```

``` python
enable_cachy(hdrs=['x-goog-upload-url', 'x-goog-upload-status'])
```

``` python
gfile = cli.files.upload(file=fn, config={"display_name": _fingerprint(fn)})
gfile
```

``` python
tfw.__exit__(None, None, None)
```
